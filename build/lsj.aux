\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\abx@aux@sortscheme{nyt}
\abx@aux@refcontext{nyt/global/}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\select@language{english}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Preface}{ix}{chapter*.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Background}{1}{part.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {1}Why do we learn statistics?~}{3}{chapter.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:whystats}{{1}{3}{Why do we learn statistics?~}{chapter.1}{}}
\newlabel{sec:whywhywhy}{{1.1}{3}{On the psychology of statistics~\label {sec:whywhywhy}}{section.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.1}On the psychology of statistics~}{3}{section.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}The curse of belief bias}{4}{subsection.1.1.1}}
\abx@aux@cite{Evans1983}
\abx@aux@segm{0}{0}{Evans1983}
\abx@aux@backref{1}{Evans1983}{0}{5}{5}
\abx@aux@page{1}{5}
\abx@aux@cite{Bickel1975}
\abx@aux@segm{0}{0}{Bickel1975}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.2}The cautionary tale of Simpson's paradox}{6}{section.1.2}}
\abx@aux@backref{2}{Bickel1975}{0}{6}{6}
\abx@aux@page{2}{6}
\abx@aux@segm{0}{0}{Bickel1975}
\abx@aux@segm{0}{0}{Bickel1975}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The Berkeley 1973 college admissions data.}}{8}{figure.1.1}}
\abx@aux@backref{4}{Bickel1975}{0}{8}{8}
\newlabel{fig:berkeley}{{1.1}{8}{The Berkeley 1973 college admissions data}{figure.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.3}Statistics in psychology}{9}{section.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.4}Statistics in everyday life}{11}{section.1.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.5}There's more to research methods than statistics}{11}{section.1.5}}
\abx@aux@cite{Campbell1963}
\abx@aux@segm{0}{0}{Campbell1963}
\abx@aux@cite{Stevens1946}
\abx@aux@segm{0}{0}{Stevens1946}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}A brief introduction to research design}{13}{chapter.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:studydesign}{{2}{13}{A brief introduction to research design}{chapter.2}{}}
\abx@aux@backref{5}{Campbell1963}{0}{13}{13}
\abx@aux@page{5}{13}
\abx@aux@backref{6}{Stevens1946}{0}{13}{13}
\abx@aux@page{6}{13}
\newlabel{sec:measurement}{{2.1}{13}{Introduction to psychological measurement~\label {sec:measurement}}{section.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to psychological measurement~}{13}{section.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Some thoughts about psychological measurement}{13}{subsection.2.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Operationalisation: defining your measurement}{15}{subsection.2.1.2}}
\newlabel{sec:scales}{{2.2}{16}{Scales of measurement\label {sec:scales}}{section.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Scales of measurement}{16}{section.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Nominal scale}{17}{subsection.2.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Ordinal scale}{17}{subsection.2.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Interval scale}{19}{subsection.2.2.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Ratio scale}{19}{subsection.2.2.4}}
\newlabel{sec:continuousdiscrete}{{2.2.5}{19}{Continuous versus discrete variables~\label {sec:continuousdiscrete}}{subsection.2.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Continuous versus discrete variables~}{19}{subsection.2.2.5}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The relationship between the scales of measurement and the discrete/continuity distinction. Cells with a tick mark correspond to things that are possible.}}{20}{table.2.1}}
\newlabel{tab:scalescont}{{2.1}{20}{The relationship between the scales of measurement and the discrete/continuity distinction. Cells with a tick mark correspond to things that are possible}{table.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Some complexities}{20}{subsection.2.2.6}}
\newlabel{sec:reliability}{{2.3}{22}{Assessing the reliability of a measurement~\label {sec:reliability}}{section.2.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Assessing the reliability of a measurement~}{22}{section.2.3}}
\newlabel{sec:ivdv}{{2.4}{23}{The \texorpdfstring {``role''}{role} of variables: predictors and outcomes \label {sec:ivdv}}{section.2.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}The ``role'' of variables: predictors and outcomes }{23}{section.2.4}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The terminology used to distinguish between different roles that a variable can play when analysing a data set. Note that this book will tend to avoid the classical terminology in favour of the newer names.}}{24}{table.2.2}}
\newlabel{tab:ivdv}{{2.2}{24}{The terminology used to distinguish between different roles that a variable can play when analysing a data set. Note that this book will tend to avoid the classical terminology in favour of the newer names}{table.2.2}{}}
\newlabel{sec:researchdesigns}{{2.5}{24}{Experimental and non-experimental research~\label {sec:researchdesigns}}{section.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.5}Experimental and non-experimental research~}{24}{section.2.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Experimental research}{24}{subsection.2.5.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Non-experimental research}{25}{subsection.2.5.2}}
\newlabel{sec:validity}{{2.6}{26}{Assessing the validity of a study~\label {sec:validity}}{section.2.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.6}Assessing the validity of a study~}{26}{section.2.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Internal validity}{26}{subsection.2.6.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}External validity}{27}{subsection.2.6.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Construct validity}{28}{subsection.2.6.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Face validity}{28}{subsection.2.6.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Ecological validity}{29}{subsection.2.6.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.7}Confounds, artefacts and other threats to validity}{29}{section.2.7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}History effects}{30}{subsection.2.7.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Maturation effects}{31}{subsection.2.7.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Repeated testing effects}{31}{subsection.2.7.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}Selection bias}{32}{subsection.2.7.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.5}Differential attrition}{32}{subsection.2.7.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.6}Non-response bias}{33}{subsection.2.7.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.7}Regression to the mean}{33}{subsection.2.7.7}}
\abx@aux@cite{Kahneman1973}
\abx@aux@segm{0}{0}{Kahneman1973}
\abx@aux@cite{Pfungst1911}
\abx@aux@segm{0}{0}{Pfungst1911}
\abx@aux@cite{Hothersall2004}
\abx@aux@segm{0}{0}{Hothersall2004}
\abx@aux@backref{7}{Kahneman1973}{0}{34}{34}
\abx@aux@page{7}{34}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.8}Experimenter bias}{34}{subsection.2.7.8}}
\abx@aux@backref{8}{Pfungst1911}{0}{34}{34}
\abx@aux@page{8}{34}
\abx@aux@backref{9}{Hothersall2004}{0}{34}{34}
\abx@aux@page{9}{34}
\abx@aux@cite{Rosenthal1966}
\abx@aux@segm{0}{0}{Rosenthal1966}
\abx@aux@cite{Adair1984}
\abx@aux@segm{0}{0}{Adair1984}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.9}Demand effects and reactivity}{35}{subsection.2.7.9}}
\abx@aux@backref{10}{Rosenthal1966}{0}{35}{35}
\abx@aux@page{10}{35}
\abx@aux@backref{11}{Adair1984}{0}{35}{35}
\abx@aux@page{11}{35}
\abx@aux@cite{hrobjartsson2010}
\abx@aux@segm{0}{0}{hrobjartsson2010}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.10}Placebo effects}{36}{subsection.2.7.10}}
\abx@aux@backref{12}{hrobjartsson2010}{0}{36}{36}
\abx@aux@page{12}{36}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.11}Situation, measurement and sub-population effects}{36}{subsection.2.7.11}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.12}Fraud, deception and self-deception}{36}{subsection.2.7.12}}
\abx@aux@cite{Gelman2014}
\abx@aux@segm{0}{0}{Gelman2014}
\abx@aux@cite{Ioannidis2005}
\abx@aux@segm{0}{0}{Ioannidis2005}
\abx@aux@cite{Kuhberger2014}
\abx@aux@segm{0}{0}{Kuhberger2014}
\abx@aux@backref{13}{Gelman2014}{0}{38}{38}
\abx@aux@page{13}{38}
\abx@aux@backref{14}{Ioannidis2005}{0}{38}{38}
\abx@aux@page{14}{38}
\abx@aux@backref{15}{Kuhberger2014}{0}{38}{38}
\abx@aux@page{15}{38}
\abx@aux@segm{0}{0}{Campbell1963}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.8}Summary}{39}{section.2.8}}
\abx@aux@backref{16}{Campbell1963}{0}{39}{39}
\abx@aux@page{16}{39}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Describing and displaying data with JASP}{41}{part.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Getting started with JASP}{43}{chapter.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introj}{{3}{43}{Getting started with JASP}{chapter.3}{}}
\newlabel{sec:gettingjasp}{{3.1}{44}{Installing JASP \label {sec:gettingjasp}}{section.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}Installing JASP }{44}{section.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Starting up JASP}{44}{subsection.3.1.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces JASP looks like this when you start it.}}{45}{figure.3.1}}
\newlabel{fig:startingjasp}{{3.1}{45}{JASP looks like this when you start it}{figure.3.1}{}}
\newlabel{sec:analyses}{{3.2}{45}{Analyses\label {sec:analyses}}{section.3.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}Analyses}{45}{section.3.2}}
\newlabel{sec:load}{{3.3}{46}{Loading data in JASP\label {sec:load}}{section.3.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}Loading data in JASP}{46}{section.3.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Importing data from CSV files}{46}{subsection.3.3.1}}
\newlabel{sec:spreadsheet}{{3.4}{46}{The spreadsheet\label {sec:spreadsheet}}{section.3.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}The spreadsheet}{46}{section.3.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The \texttt  {booksales.csv} data file. On the left I've opened the file using a spreadsheet program, which shows that the file is basically a table. On the right the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are separated by commas.}}{47}{figure.3.2}}
\newlabel{fig:booksalescsv}{{3.2}{47}{The \filename {booksales.csv} data file. On the left I've opened the file using a spreadsheet program, which shows that the file is basically a table. On the right the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are separated by commas}{figure.3.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A dialog box on a Mac asking you to select the CSV file JASP should try to import. Mac users will recognise this immediately -- it's the usual way in which a Mac asks you to find a file. Windows users won't see this, but instead will see the usual explorer window that Windows always gives you when it wants you to select a file.}}{48}{figure.3.3}}
\newlabel{fig:fileopen}{{3.3}{48}{A dialog box on a Mac asking you to select the CSV file JASP should try to import. Mac users will recognise this immediately -- it's the usual way in which a Mac asks you to find a file. Windows users won't see this, but instead will see the usual explorer window that Windows always gives you when it wants you to select a file}{figure.3.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Variables}{48}{subsection.3.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Computed variables}{49}{subsection.3.4.2}}
\newlabel{sec:copypaste}{{3.4.3}{49}{Copy and Paste\label {sec:copypaste}}{subsection.3.4.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Copy and Paste}{49}{subsection.3.4.3}}
\newlabel{sec:coercion}{{3.5}{49}{Changing data from one measurement scale to another\label {sec:coercion}}{section.3.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.5}Changing data from one measurement scale to another}{49}{section.3.5}}
\newlabel{sec:quittingjasp}{{3.6}{50}{Quitting JASP \label {sec:quittingjasp}}{section.3.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.6}Quitting JASP }{50}{section.3.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.7}Summary}{50}{section.3.7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {4}Descriptive statistics}{51}{chapter.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:descriptives}{{4}{51}{Descriptive statistics}{chapter.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A screenshot of JASP showing the variables stored in the \texttt  {aflsmall\_margins.csv} file}}{51}{figure.4.1}}
\newlabel{fig:aflsmall}{{4.1}{51}{A screenshot of JASP showing the variables stored in the \filename {aflsmall\_margins.csv} file}{figure.4.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A histogram of the AFL 2010 winning margin data (the \texttt  {afl.margins} varable). As you might expect, the larger the winning margin the less frequently you tend to see it.}}{52}{figure.4.2}}
\newlabel{fig:histogram1}{{4.2}{52}{A histogram of the AFL 2010 winning margin data (the \texttt {afl.margins} varable). As you might expect, the larger the winning margin the less frequently you tend to see it}{figure.4.2}{}}
\newlabel{sec:centraltendency}{{4.1}{53}{Measures of central tendency~\label {sec:centraltendency}}{section.4.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}Measures of central tendency~}{53}{section.4.1}}
\newlabel{sec:mean}{{4.1.1}{53}{The mean\label {sec:mean}}{subsection.4.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}The mean}{53}{subsection.4.1.1}}
\zref@newlabel{mdf@pagelabel-1}{\default{4.1}\page{54}\abspage{67}\mdf@pagevalue{54}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Calculating the mean in JASP}{54}{subsection.4.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Default descriptives for the AFL 2010 winning margin data (the \texttt  {afl.margins} variable). }}{55}{figure.4.3}}
\newlabel{fig:descriptives_default}{{4.3}{55}{Default descriptives for the AFL 2010 winning margin data (the \texttt {afl.margins} variable)}{figure.4.3}{}}
\newlabel{sec:median}{{4.1.3}{55}{The median\label {sec:median}}{subsection.4.1.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}The median}{55}{subsection.4.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Mean or median? What's the difference?}{56}{subsection.4.1.4}}
\newlabel{sec:housingpriceexample}{{4.1.5}{56}{A real life example~\label {sec:housingpriceexample}}{subsection.4.1.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}A real life example~}{56}{subsection.4.1.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the ``centre of gravity'' of the data set. If you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation, with half of the observations smaller and half of the observations larger.}}{57}{figure.4.4}}
\newlabel{fig:meanmedian}{{4.4}{57}{An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the ``centre of gravity'' of the data set. If you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation, with half of the observations smaller and half of the observations larger}{figure.4.4}{}}
\newlabel{sec:mode}{{4.1.6}{58}{Mode\label {sec:mode}}{subsection.4.1.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Mode}{58}{subsection.4.1.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces A screenshot of JASP showing the variables stored in the \texttt  {aflsmall\_finalists.csv} file}}{59}{figure.4.5}}
\newlabel{fig:aflsmall_finalists}{{4.5}{59}{A screenshot of JASP showing the variables stored in the \filename {aflsmall\_finalists.csv} file}{figure.4.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces A screenshot of JASP showing the frequency table for the \texttt  {afl.finalists} variable }}{59}{figure.4.6}}
\newlabel{fig:aflsmall_finalists_mode}{{4.6}{59}{A screenshot of JASP showing the frequency table for the \texttt {afl.finalists} variable}{figure.4.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A screenshot of JASP showing the modal value for the \texttt  {afl.margins} variable }}{60}{figure.4.7}}
\newlabel{fig:aflsmall_margins_mode}{{4.7}{60}{A screenshot of JASP showing the modal value for the \texttt {afl.margins} variable}{figure.4.7}{}}
\newlabel{sec:var}{{4.2}{61}{Measures of variability\label {sec:var}}{section.4.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Measures of variability}{61}{section.4.2}}
\newlabel{sec:range}{{4.2.1}{61}{Range\label {sec:range}}{subsection.4.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Range}{61}{subsection.4.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Interquartile range}{61}{subsection.4.2.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces A screenshot of JASP showing the Quartiles for the \texttt  {afl.margins} variable }}{62}{figure.4.8}}
\newlabel{fig:aflsmall_margins_iqr}{{4.8}{62}{A screenshot of JASP showing the Quartiles for the \texttt {afl.margins} variable}{figure.4.8}{}}
\newlabel{sec:aad}{{4.2.3}{62}{Mean absolute deviation~\label {sec:aad}}{subsection.4.2.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Mean absolute deviation~}{62}{subsection.4.2.3}}
\zref@newlabel{mdf@pagelabel-2}{\default{4.2}\page{63}\abspage{76}\mdf@pagevalue{63}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Variance}{63}{subsection.4.2.4}}
\zref@newlabel{mdf@pagelabel-3}{\default{4.2}\page{64}\abspage{77}\mdf@pagevalue{64}}
\zref@newlabel{mdf@pagelabel-4}{\default{4.2}\page{65}\abspage{78}\mdf@pagevalue{65}}
\newlabel{sec:sd}{{4.2.5}{66}{Standard deviation\label {sec:sd}}{subsection.4.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Standard deviation}{66}{subsection.4.2.5}}
\zref@newlabel{mdf@pagelabel-5}{\default{4.2}\page{66}\abspage{79}\mdf@pagevalue{66}}
\zref@newlabel{mdf@pagelabel-6}{\default{4.2}\page{66}\abspage{79}\mdf@pagevalue{66}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces An illustration of the standard deviation from the AFL winning margins data. The shaded bars in the histogram show how much of the data fall within one standard deviation of the mean. In this case, 65.3\% of the data set lies within this range, which is pretty consistent with the ``approximately 68\% rule'' discussed in the main text.}}{67}{figure.4.9}}
\newlabel{fig:aflsd}{{4.9}{67}{An illustration of the standard deviation from the AFL winning margins data. The shaded bars in the histogram show how much of the data fall within one standard deviation of the mean. In this case, 65.3\% of the data set lies within this range, which is pretty consistent with the ``approximately 68\% rule'' discussed in the main text}{figure.4.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Which measure to use?}{67}{subsection.4.2.6}}
\newlabel{sec:skewkurt}{{4.3}{68}{Skew and kurtosis \label {sec:skewkurt}}{section.4.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.3}Skew and kurtosis }{68}{section.4.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces An illustration of skewness. On the left we have a negatively skewed data set (skewness $= -.93$), in the middle we have a data set with no skew (well, hardly any: skewness $= -.006$), and on the right we have a positively skewed data set (skewness $= .93$). }}{68}{figure.4.10}}
\newlabel{fig:skewness}{{4.10}{68}{An illustration of skewness. On the left we have a negatively skewed data set (skewness $= -.93$), in the middle we have a data set with no skew (well, hardly any: skewness $= -.006$), and on the right we have a positively skewed data set (skewness $= .93$)}{figure.4.10}{}}
\zref@newlabel{mdf@pagelabel-7}{\default{4.3}\page{69}\abspage{82}\mdf@pagevalue{69}}
\zref@newlabel{mdf@pagelabel-8}{\default{4.3}\page{69}\abspage{82}\mdf@pagevalue{69}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces An illustration of kurtosis. On the left, we have a ``platykurtic'' data set (kurtosis = $-.95$) meaning that the data set is ``too flat''. In the middle we have a ``mesokurtic'' data set (kurtosis is almost exactly 0) which means that the pointiness of the data is just about right. Finally, on the right, we have a ``leptokurtic'' data set (kurtosis $= 2.12$) indicating that the data set is ``too pointy''. Note that kurtosis is measured with respect to a normal curve (black line).}}{70}{figure.4.11}}
\newlabel{fig:kurtosis}{{4.11}{70}{An illustration of kurtosis. On the left, we have a ``platykurtic'' data set (kurtosis = $-.95$) meaning that the data set is ``too flat''. In the middle we have a ``mesokurtic'' data set (kurtosis is almost exactly 0) which means that the pointiness of the data is just about right. Finally, on the right, we have a ``leptokurtic'' data set (kurtosis $= 2.12$) indicating that the data set is ``too pointy''. Note that kurtosis is measured with respect to a normal curve (black line)}{figure.4.11}{}}
\newlabel{sec:groupdescriptives}{{4.4}{70}{Descriptive statistics separately for each group~\label {sec:groupdescriptives}}{section.4.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.4}Descriptive statistics separately for each group~}{70}{section.4.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces A screenshot of JASP showing the variables stored in the \texttt  {clinicaltrial.csv} file}}{71}{figure.4.12}}
\newlabel{fig:clinicaltrial}{{4.12}{71}{A screenshot of JASP showing the variables stored in the \filename {clinicaltrial.csv} file}{figure.4.12}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces A screenshot of JASP showing Descriptives split by therapy type}}{72}{figure.4.13}}
\newlabel{fig:clinicaltrial_grouping}{{4.13}{72}{A screenshot of JASP showing Descriptives split by therapy type}{figure.4.13}{}}
\newlabel{sec:zscore}{{4.5}{72}{Standard scores~\label {sec:zscore}}{section.4.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.5}Standard scores~}{72}{section.4.5}}
\zref@newlabel{mdf@pagelabel-9}{\default{4.5}\page{73}\abspage{86}\mdf@pagevalue{73}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.6}Summary}{74}{section.4.6}}
\abx@aux@cite{Ellman2002}
\abx@aux@segm{0}{0}{Ellman2002}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Epilogue: Good descriptive statistics are descriptive!}{75}{subsection.4.6.1}}
\abx@aux@backref{17}{Ellman2002}{0}{75}{75}
\abx@aux@page{17}{75}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {5}Drawing graphs}{77}{chapter.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:graphics}{{5}{77}{Drawing graphs}{chapter.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A stylised redrawing of John Snow's original cholera map. Each small dot represents the location of a cholera case and each large circle shows the location of a well. As the plot makes clear, the cholera outbreak is centred very closely on the Broad St pump.}}{78}{figure.5.1}}
\newlabel{fig:snowmap1}{{5.1}{78}{A stylised redrawing of John Snow's original cholera map. Each small dot represents the location of a cholera case and each large circle shows the location of a well. As the plot makes clear, the cholera outbreak is centred very closely on the Broad St pump}{figure.5.1}{}}
\newlabel{sec:hist}{{5.1}{78}{Histograms\label {sec:hist}}{section.5.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.1}Histograms}{78}{section.5.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces JASP screenshot showing the `Distribution plots' option and accompanying histogram.}}{79}{figure.5.2}}
\newlabel{fig:jasp_histogram}{{5.2}{79}{JASP screenshot showing the `Distribution plots' option and accompanying histogram}{figure.5.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces A density plot of the \texttt  {afl.margins} variable plotted in JASP}}{80}{figure.5.3}}
\newlabel{fig:histogram2}{{5.3}{80}{A density plot of the \texttt {afl.margins} variable plotted in JASP}{figure.5.3}{}}
\newlabel{sec:boxplots}{{5.2}{80}{Boxplots~\label {sec:boxplots}}{section.5.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.2}Boxplots~}{80}{section.5.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces A box plot of the \texttt  {afl.margins} variable plotted in JASP}}{81}{figure.5.4}}
\newlabel{fig:boxplot1}{{5.4}{81}{A box plot of the \texttt {afl.margins} variable plotted in JASP}{figure.5.4}{}}
\newlabel{sec:violinplots}{{5.2.1}{81}{Violin plots~\label {sec:violinplots}}{subsection.5.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Violin plots~}{81}{subsection.5.2.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces A violin plot of the \texttt  {afl.margins} variable plotted in JASP, also showing a box plot and data points}}{82}{figure.5.5}}
\newlabel{fig:boxplot2}{{5.5}{82}{A violin plot of the \texttt {afl.margins} variable plotted in JASP, also showing a box plot and data points}{figure.5.5}{}}
\newlabel{sec:multipleboxplots}{{5.2.2}{82}{Drawing multiple boxplots~\label {sec:multipleboxplots}}{subsection.5.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Drawing multiple boxplots~}{82}{subsection.5.2.2}}
\newlabel{sec:saveimage}{{5.3}{82}{Saving image files using JASP~\label {sec:saveimage}}{section.5.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.3}Saving image files using JASP~}{82}{section.5.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Multiple boxplots plotted in JASP, for the \texttt  {margin} by \texttt  {year} variables in the \texttt  {aflsmall2} data set}}{83}{figure.5.6}}
\newlabel{fig:boxplot3}{{5.6}{83}{Multiple boxplots plotted in JASP, for the \texttt {margin} by \texttt {year} variables in the \texttt {aflsmall2} data set}{figure.5.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.4}Summary}{83}{section.5.4}}
\abx@aux@cite{Wilkinson2006}
\abx@aux@segm{0}{0}{Wilkinson2006}
\abx@aux@backref{18}{Wilkinson2006}{0}{84}{84}
\abx@aux@page{18}{84}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Statistical theory}{85}{part.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {6}Introduction to probability}{93}{chapter.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:probability}{{6}{93}{Introduction to probability}{chapter.6}{}}
\newlabel{sec:probstats}{{6.1}{94}{How are probability and statistics different?~\label {sec:probstats}}{section.6.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.1}How are probability and statistics different?~}{94}{section.6.1}}
\newlabel{sec:probmeaning}{{6.2}{95}{What does probability mean?\label {sec:probmeaning}}{section.6.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.2}What does probability mean?}{95}{section.6.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}The frequentist view}{96}{subsection.6.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}The Bayesian view}{97}{subsection.6.2.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you've seen eventually settles down and converges to the true probability of 0.5. Each panel shows four different simulated experiments. In each case we pretend we flipped a coin 1000 times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we'd extended the experiment for an infinite number of coin flips they would have.}}{98}{figure.6.1}}
\newlabel{fig:frequentistprobability}{{6.1}{98}{An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you've seen eventually settles down and converges to the true probability of 0.5. Each panel shows four different simulated experiments. In each case we pretend we flipped a coin 1000 times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we'd extended the experiment for an infinite number of coin flips they would have}{figure.6.1}{}}
\abx@aux@cite{Fisher1922b}
\abx@aux@segm{0}{0}{Fisher1922b}
\abx@aux@cite{Meehl1967}
\abx@aux@segm{0}{0}{Meehl1967}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}What's the difference? And who is right?}{99}{subsection.6.2.3}}
\abx@aux@backref{19}{Fisher1922b}{0}{100}{100}
\abx@aux@page{19}{100}
\abx@aux@backref{20}{Meehl1967}{0}{100}{100}
\abx@aux@page{20}{100}
\newlabel{sec:basicprobability}{{6.3}{100}{Basic probability theory~\label {sec:basicprobability}}{section.6.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.3}Basic probability theory~}{100}{section.6.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Introducing probability distributions}{100}{subsection.6.3.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces A visual depiction of the ``trousers'' probability distribution. There are five ``elementary events'', corresponding to the five pairs of trousers that I own. Each event has some probability of occurring: this probability is a number between 0 to 1. The sum of these probabilities is 1.}}{102}{figure.6.2}}
\newlabel{fig:pantsprob}{{6.2}{102}{A visual depiction of the ``trousers'' probability distribution. There are five ``elementary events'', corresponding to the five pairs of trousers that I own. Each event has some probability of occurring: this probability is a number between 0 to 1. The sum of these probabilities is 1}{figure.6.2}{}}
\newlabel{sec:binomial}{{6.4}{102}{The binomial distribution\label {sec:binomial}}{section.6.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.4}The binomial distribution}{102}{section.6.4}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Some basic rules that probabilities must satisfy. You don't really need to know these rules in order to understand the analyses that we'll talk about later in the book, but they are important if you want to understand probability theory a bit more deeply.}}{103}{table.6.1}}
\newlabel{tab:probrules}{{6.1}{103}{Some basic rules that probabilities must satisfy. You don't really need to know these rules in order to understand the analyses that we'll talk about later in the book, but they are important if you want to understand probability theory a bit more deeply}{table.6.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Introducing the binomial}{103}{subsection.6.4.1}}
\zref@newlabel{mdf@pagelabel-10}{\default{6.4}\page{103}\abspage{116}\mdf@pagevalue{103}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Formulas for the binomial and normal distributions. We don't really use these formulas for anything in this book, but they're pretty important for more advanced work, so I thought it might be best to put them here in a table, where they can't get in the way of the text. In the equation for the binomial, $X!$ is the factorial function (i.e., multiply all whole numbers from 1 to $X$), and for the normal distribution ``exp'' refers to the exponential function. If these equations don't make a lot of sense to you, don't worry too much about them. }}{103}{table.6.2}}
\newlabel{tab:distformulas}{{6.2}{103}{Formulas for the binomial and normal distributions. We don't really use these formulas for anything in this book, but they're pretty important for more advanced work, so I thought it might be best to put them here in a table, where they can't get in the way of the text. In the equation for the binomial, $X!$ is the factorial function (i.e., multiply all whole numbers from 1 to $X$), and for the normal distribution ``exp'' refers to the exponential function. If these equations don't make a lot of sense to you, don't worry too much about them}{table.6.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The binomial distribution with size parameter of $N=20$ and an underlying success probability of $\theta = 1/6$. Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of $X$). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well.}}{104}{figure.6.3}}
\newlabel{fig:binomial1}{{6.3}{104}{The binomial distribution with size parameter of $N=20$ and an underlying success probability of $\theta = 1/6$. Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of $X$). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well}{figure.6.3}{}}
\newlabel{sec:normal}{{6.5}{105}{The normal distribution\label {sec:normal}}{section.6.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.5}The normal distribution}{105}{section.6.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Two binomial distributions, involving a scenario in which I'm flipping a fair coin, so the underlying success probability is $\theta = 1/2$. In panel (a), we assume I'm flipping the coin $N=20$ times. In panel (b) we assume that the coin is flipped $N=100$ times.}}{106}{figure.6.4}}
\newlabel{fig:binomial2}{{6.4}{106}{Two binomial distributions, involving a scenario in which I'm flipping a fair coin, so the underlying success probability is $\theta = 1/2$. In panel (a), we assume I'm flipping the coin $N=20$ times. In panel (b) we assume that the coin is flipped $N=100$ times}{figure.6.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. The $x$-axis corresponds to the value of some variable, and the $y$-axis tells us something about how likely we are to observe that value. However, notice that the $y$-axis is labelled ``Probability Density'' and not ``Probability''. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the $y$ axis behave a bit oddly: the height of the curve here isn't actually the probability of observing a particular $x$ value. On the other hand, it {\it  is} true that the heights of the curve tells you which $x$ values are more likely (the higher ones!). (see Section~\ref  {sec:density} for all the annoying details)}}{107}{figure.6.5}}
\newlabel{fig:normdist}{{6.5}{107}{The normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. The $x$-axis corresponds to the value of some variable, and the $y$-axis tells us something about how likely we are to observe that value. However, notice that the $y$-axis is labelled ``Probability Density'' and not ``Probability''. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the $y$ axis behave a bit oddly: the height of the curve here isn't actually the probability of observing a particular $x$ value. On the other hand, it {\it is} true that the heights of the curve tells you which $x$ values are more likely (the higher ones!). (see Section~\ref {sec:density} for all the annoying details)}{figure.6.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of $\mu =4$. The dashed line shows a normal distribution with a mean of $\mu =7$. In both cases, the standard deviation is $\sigma =1$. Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right.}}{108}{figure.6.6}}
\newlabel{fig:normmean}{{6.6}{108}{An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of $\mu =4$. The dashed line shows a normal distribution with a mean of $\mu =7$. In both cases, the standard deviation is $\sigma =1$. Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right}{figure.6.6}{}}
\newlabel{sec:density}{{6.5.1}{108}{Probability density~\label {sec:density} \advanced }{subsection.6.5.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Probability density~ }{108}{subsection.6.5.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of $\mu = 5$, but they have different standard deviations. The solid line plots a distribution with standard deviation $\sigma =1$, and the dashed line shows a distribution with standard deviation $\sigma = 2$. As a consequence, both distributions are ``centred'' on the same spot, but the dashed line is wider than the solid one.}}{109}{figure.6.7}}
\newlabel{fig:normsd}{{6.7}{109}{An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of $\mu = 5$, but they have different standard deviations. The solid line plots a distribution with standard deviation $\sigma =1$, and the dashed line shows a distribution with standard deviation $\sigma = 2$. As a consequence, both distributions are ``centred'' on the same spot, but the dashed line is wider than the solid one}{figure.6.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean $\mu =0$ and standard deviation $\sigma =1$. The shaded areas illustrate ``areas under the curve'' for two important cases. In panel a, we can see that there is a 68.3\% chance that an observation will fall within one standard deviation of the mean. In panel b, we see that there is a 95.4\% chance that an observation will fall within two standard deviations of the mean.}}{110}{figure.6.8}}
\newlabel{fig:sdnorm}{{6.8}{110}{The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean $\mu =0$ and standard deviation $\sigma =1$. The shaded areas illustrate ``areas under the curve'' for two important cases. In panel a, we can see that there is a 68.3\% chance that an observation will fall within one standard deviation of the mean. In panel b, we see that there is a 95.4\% chance that an observation will fall within two standard deviations of the mean}{figure.6.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Two more examples of the ``area under the curve idea''. There is a 15.9\% chance that an observation is one standard deviation below the mean or smaller (panel a), and a 34.1\% chance that the observation is somewhere between one standard deviation below the mean and the mean (panel b). Notice that if you add these two numbers together you get $15.9\% + 34.1\% = 50\%$. For normally distributed data, there is a 50\% chance that an observation falls below the mean. And of course that also implies that there is a 50\% chance that it falls above the mean.}}{110}{figure.6.9}}
\newlabel{fig:sdnorm2}{{6.9}{110}{Two more examples of the ``area under the curve idea''. There is a 15.9\% chance that an observation is one standard deviation below the mean or smaller (panel a), and a 34.1\% chance that the observation is somewhere between one standard deviation below the mean and the mean (panel b). Notice that if you add these two numbers together you get $15.9\% + 34.1\% = 50\%$. For normally distributed data, there is a 50\% chance that an observation falls below the mean. And of course that also implies that there is a 50\% chance that it falls above the mean}{figure.6.9}{}}
\newlabel{sec:otherdists}{{6.6}{111}{Other useful distributions~\label {sec:otherdists}}{section.6.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.6}Other useful distributions~}{111}{section.6.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces A $t$ distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it's not quite the same. For comparison purposes I've plotted a standard normal distribution as the dashed line.}}{112}{figure.6.10}}
\newlabel{fig:tdist}{{6.10}{112}{A $t$ distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it's not quite the same. For comparison purposes I've plotted a standard normal distribution as the dashed line}{figure.6.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces A $\chi ^2$ distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square distribution.}}{113}{figure.6.11}}
\newlabel{fig:chisqdist}{{6.11}{113}{A $\chi ^2$ distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square distribution}{figure.6.11}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces An $F$ distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they're not quite the same in general.}}{113}{figure.6.12}}
\newlabel{fig:Fdist}{{6.12}{113}{An $F$ distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they're not quite the same in general}{figure.6.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.7}Summary}{114}{section.6.7}}
\abx@aux@cite{Evans2000}
\abx@aux@segm{0}{0}{Evans2000}
\abx@aux@backref{21}{Evans2000}{0}{115}{115}
\abx@aux@page{21}{115}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {7}Estimating unknown quantities from a sample}{117}{chapter.7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:estimation}{{7}{117}{Estimating unknown quantities from a sample}{chapter.7}{}}
\newlabel{sec:srs}{{7.1}{117}{Samples, populations and sampling~\label {sec:srs}}{section.7.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.1}Samples, populations and sampling~}{117}{section.7.1}}
\newlabel{sec:pop}{{7.1.1}{118}{Defining a population~\label {sec:pop}}{subsection.7.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Defining a population~}{118}{subsection.7.1.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Simple random sampling without replacement from a finite population}}{119}{figure.7.1}}
\newlabel{fig:srs1}{{7.1}{119}{Simple random sampling without replacement from a finite population}{figure.7.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Simple random samples}{119}{subsection.7.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Biased sampling without replacement from a finite population}}{120}{figure.7.2}}
\newlabel{fig:brs}{{7.2}{120}{Biased sampling without replacement from a finite population}{figure.7.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Simple random sampling {\it  with} replacement from a finite population}}{120}{figure.7.3}}
\newlabel{fig:srs2}{{7.3}{120}{Simple random sampling {\it with} replacement from a finite population}{figure.7.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Most samples are not simple random samples}{121}{subsection.7.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}How much does it matter if you don't have a simple random sample?}{122}{subsection.7.1.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.5}Population parameters and sample statistics}{123}{subsection.7.1.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations.}}{124}{figure.7.4}}
\newlabel{fig:IQdist}{{7.4}{124}{The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations}{figure.7.4}{}}
\newlabel{sec:lawlargenumbers}{{7.2}{124}{The law of large numbers~\label {sec:lawlargenumbers}}{section.7.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.2}The law of large numbers~}{124}{section.7.2}}
\abx@aux@cite{Stigler1986}
\abx@aux@segm{0}{0}{Stigler1986}
\abx@aux@backref{22}{Stigler1986}{0}{125}{125}
\abx@aux@page{22}{125}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces A random sample drawn from a normal distribution using JASP}}{126}{figure.7.5}}
\newlabel{fig:iqsim}{{7.5}{126}{A random sample drawn from a normal distribution using JASP}{figure.7.5}{}}
\abx@aux@cite{Keynes1923}
\abx@aux@segm{0}{0}{Keynes1923}
\newlabel{sec:samplesandclt}{{7.3}{127}{Sampling distributions and the central limit theorem~\label {sec:samplesandclt}}{section.7.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.3}Sampling distributions and the central limit theorem~}{127}{section.7.3}}
\abx@aux@backref{23}{Keynes1923}{0}{127}{127}
\abx@aux@page{23}{127}
\newlabel{sec:samplingdists}{{7.3.1}{127}{Sampling distribution of the mean~\label {sec:samplingdists}}{subsection.7.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Sampling distribution of the mean~}{127}{subsection.7.3.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Using JASP to draw a random sample of 5 from a normal distribution with $\mu =100$ and $\sigma =15$.}}{128}{figure.7.6}}
\newlabel{fig:IQsample}{{7.6}{128}{Using JASP to draw a random sample of 5 from a normal distribution with $\mu =100$ and $\sigma =15$}{figure.7.6}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Ten replications of the IQ experiment, each with a sample size of $N=5$.}}{129}{table.7.1}}
\newlabel{tab:replications}{{7.1}{129}{Ten replications of the IQ experiment, each with a sample size of $N=5$}{table.7.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Sampling distributions exist for any sample statistic!}{129}{subsection.7.3.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces The sampling distribution of the mean for the ``five IQ scores experiment''. If you sample 5 people at random and calculate their {\it  average} IQ you'll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores.}}{130}{figure.7.7}}
\newlabel{fig:sampdistmean}{{7.7}{130}{The sampling distribution of the mean for the ``five IQ scores experiment''. If you sample 5 people at random and calculate their {\it average} IQ you'll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores}{figure.7.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces The sampling distribution of the {\it  maximum} for the ``five IQ scores experiment''. If you sample 5 people at random and select the one with the highest IQ score you'll probably see someone with an IQ between 100 and 140.}}{130}{figure.7.8}}
\newlabel{fig:sampdistmax}{{7.8}{130}{The sampling distribution of the {\it maximum} for the ``five IQ scores experiment''. If you sample 5 people at random and select the one with the highest IQ score you'll probably see someone with an IQ between 100 and 140}{figure.7.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces An illustration of the how sampling distribution of the mean depends on sample size. In each panel I generated 10,000 samples of IQ data and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line. In panel a, each data set contained only a single observation, so the mean of each sample is just one person's IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2 the mean of any one sample tends to be closer to the population mean than a one person's IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel c), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean.}}{131}{figure.7.9}}
\newlabel{fig:IQsamp}{{7.9}{131}{An illustration of the how sampling distribution of the mean depends on sample size. In each panel I generated 10,000 samples of IQ data and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line. In panel a, each data set contained only a single observation, so the mean of each sample is just one person's IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2 the mean of any one sample tends to be closer to the population mean than a one person's IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel c), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean}{figure.7.9}{}}
\newlabel{sec:clt}{{7.3.3}{131}{The central limit theorem~\label {sec:clt}}{subsection.7.3.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}The central limit theorem~}{131}{subsection.7.3.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution, and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8 for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations. }}{133}{figure.7.10}}
\newlabel{fig:cltdemo}{{7.10}{133}{A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution, and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8 for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations}{figure.7.10}{}}
\newlabel{sec:pointestimates}{{7.4}{134}{Estimating population parameters~\label {sec:pointestimates}}{section.7.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.4}Estimating population parameters~}{134}{section.7.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Estimating the population mean}{135}{subsection.7.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Estimating the population standard deviation}{135}{subsection.7.4.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces The sampling distribution of the sample standard deviation for a ``two IQ scores'' experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a {\it  biased} estimate of the population standard deviation.}}{137}{figure.7.11}}
\newlabel{fig:sampdistsd}{{7.11}{137}{The sampling distribution of the sample standard deviation for a ``two IQ scores'' experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a {\it biased} estimate of the population standard deviation}{figure.7.11}{}}
\zref@newlabel{mdf@pagelabel-11}{\default{7.4}\page{138}\abspage{151}\mdf@pagevalue{138}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated 10,000 simulated data sets with 1 observation each, 10,000 more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. {\it  On average}, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes.}}{139}{figure.7.12}}
\newlabel{fig:estimatorbias}{{7.12}{139}{An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated 10,000 simulated data sets with 1 observation each, 10,000 more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. {\it On average}, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes}{figure.7.12}{}}
\newlabel{sec:ci}{{7.5}{141}{Estimating a confidence interval\label {sec:ci}}{section.7.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.5}Estimating a confidence interval}{141}{section.7.5}}
\zref@newlabel{mdf@pagelabel-12}{\default{7.5}\page{142}\abspage{155}\mdf@pagevalue{142}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}A slight mistake in the formula}{142}{subsection.7.5.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Interpreting a confidence interval}{143}{subsection.7.5.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces 95\% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean and the line shows the 95\% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people.}}{144}{figure.7.13}}
\newlabel{fig:cirep}{{7.13}{144}{95\% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean and the line shows the 95\% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people}{figure.7.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Calculating confidence intervals in JASP}{145}{subsection.7.5.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.6}Summary}{145}{section.7.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {8}Hypothesis testing}{147}{chapter.8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:hypothesistesting}{{8}{147}{Hypothesis testing}{chapter.8}{}}
\newlabel{sec:hypotheses}{{8.1}{147}{A menagerie of hypotheses~\label {sec:hypotheses}}{section.8.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.1}A menagerie of hypotheses~}{147}{section.8.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Research hypotheses versus statistical hypotheses}{148}{subsection.8.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Null hypotheses and alternative hypotheses}{150}{subsection.8.1.2}}
\newlabel{sec:errortypes}{{8.2}{151}{Two types of errors~\label {sec:errortypes}}{section.8.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.2}Two types of errors~}{151}{section.8.2}}
\newlabel{sec:teststatistics}{{8.3}{153}{Test statistics and sampling distributions~\label {sec:teststatistics}}{section.8.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.3}Test statistics and sampling distributions~}{153}{section.8.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The sampling distribution for our test statistic $X$ when the null hypothesis is true. For our ESP scenario this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is $\theta = .5$, the sampling distribution says that the most likely value is 50 (out of 100) correct responses. Most of the probability mass lies between 40 and 60.}}{154}{figure.8.1}}
\newlabel{fig:samplingdist}{{8.1}{154}{The sampling distribution for our test statistic $X$ when the null hypothesis is true. For our ESP scenario this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is $\theta = .5$, the sampling distribution says that the most likely value is 50 (out of 100) correct responses. Most of the probability mass lies between 40 and 60}{figure.8.1}{}}
\newlabel{sec:decisionmaking}{{8.4}{154}{Making decisions~\label {sec:decisionmaking}}{section.8.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.4}Making decisions~}{154}{section.8.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Critical regions and critical values}{155}{subsection.8.4.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of $\alpha = .05$. The plot shows the sampling distribution of $X$ under the null hypothesis (i.e., same as Figure~\ref  {fig:samplingdist}). The grey bars correspond to those values of $X$ for which we would retain the null hypothesis. The blue (darker shaded) bars show the critical region, those values of $X$ for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both $\theta <.5$ and $\theta >.5$), the critical region covers both tails of the distribution. To ensure an $\alpha $ level of $.05$, we need to ensure that each of the two regions encompasses 2.5\% of the sampling distribution. }}{156}{figure.8.2}}
\newlabel{fig:crit2}{{8.2}{156}{The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of $\alpha = .05$. The plot shows the sampling distribution of $X$ under the null hypothesis (i.e., same as Figure~\ref {fig:samplingdist}). The grey bars correspond to those values of $X$ for which we would retain the null hypothesis. The blue (darker shaded) bars show the critical region, those values of $X$ for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both $\theta <.5$ and $\theta >.5$), the critical region covers both tails of the distribution. To ensure an $\alpha $ level of $.05$, we need to ensure that each of the two regions encompasses 2.5\% of the sampling distribution}{figure.8.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}A note on statistical ``significance''}{157}{subsection.8.4.2}}
\newlabel{sec:onesidedtests}{{8.4.3}{157}{The difference between one sided and two sided tests\label {sec:onesidedtests}}{subsection.8.4.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}The difference between one sided and two sided tests}{157}{subsection.8.4.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces The critical region for a one sided test. In this case, the alternative hypothesis is that $\theta > .5$ so we would only reject the null hypothesis for large values of $X$. As a consequence, the critical region only covers the upper tail of the sampling distribution, specifically the upper 5\% of the distribution. Contrast this to the two-sided version in Figure~\ref  {fig:crit2}. }}{158}{figure.8.3}}
\newlabel{fig:crit1}{{8.3}{158}{The critical region for a one sided test. In this case, the alternative hypothesis is that $\theta > .5$ so we would only reject the null hypothesis for large values of $X$. As a consequence, the critical region only covers the upper tail of the sampling distribution, specifically the upper 5\% of the distribution. Contrast this to the two-sided version in Figure~\ref {fig:crit2}}{figure.8.3}{}}
\newlabel{sec:pvalue}{{8.5}{158}{The \texorpdfstring {\boldm {$p$}}{} value of a test~\label {sec:pvalue}}{section.8.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.5}The \mathversion  {bold}$p$\mathversion  {normal} value of a test~}{158}{section.8.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}A softer view of decision making}{159}{subsection.8.5.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}The probability of extreme data}{159}{subsection.8.5.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}A common mistake}{160}{subsection.8.5.3}}
\newlabel{sec:writeup}{{8.6}{160}{Reporting the results of a hypothesis test~\label {sec:writeup}}{section.8.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.6}Reporting the results of a hypothesis test~}{160}{section.8.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.1}The issue}{161}{subsection.8.6.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.2}Two proposed solutions}{161}{subsection.8.6.2}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces A commonly adopted convention for reporting $p$ values: in many places it is conventional to report one of four different things (e.g., $p<.05$) as shown below. I've included the ``significance stars'' notation (i.e., a * indicates $p<.05$) because you sometimes see this notation produced by statistical software. It's also worth noting that some people will write {\it  n.s.} (not significant) rather than $p>.05$.}}{162}{table.8.1}}
\newlabel{tab:pvaltable}{{8.1}{162}{A commonly adopted convention for reporting $p$ values: in many places it is conventional to report one of four different things (e.g., $p<.05$) as shown below. I've included the ``significance stars'' notation (i.e., a * indicates $p<.05$) because you sometimes see this notation produced by statistical software. It's also worth noting that some people will write {\it n.s.} (not significant) rather than $p>.05$}{table.8.1}{}}
\newlabel{sec:runhyp}{{8.7}{163}{Running the hypothesis test in practice~\label {sec:runhyp}}{section.8.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.7}Running the hypothesis test in practice~}{163}{section.8.7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Binomial test analysis and results in JASP}}{163}{figure.8.4}}
\newlabel{fig:binomialtest}{{8.4}{163}{Binomial test analysis and results in JASP}{figure.8.4}{}}
\newlabel{sec:effectsize}{{8.8}{164}{Effect size, sample size and power \label {sec:effectsize}}{section.8.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.8}Effect size, sample size and power }{164}{section.8.8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.1}The power function}{164}{subsection.8.8.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Sampling distribution under the {\it  alternative} hypothesis for a population parameter value of $\theta = 0.55$. A reasonable proportion of the distribution lies in the rejection region.}}{164}{figure.8.5}}
\newlabel{fig:crit3}{{8.5}{164}{Sampling distribution under the {\it alternative} hypothesis for a population parameter value of $\theta = 0.55$. A reasonable proportion of the distribution lies in the rejection region}{figure.8.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Sampling distribution under the {\it  alternative} hypothesis for a population parameter value of $\theta = 0.70$. Almost all of the distribution lies in the rejection region.}}{165}{figure.8.6}}
\newlabel{fig:crit4}{{8.6}{165}{Sampling distribution under the {\it alternative} hypothesis for a population parameter value of $\theta = 0.70$. Almost all of the distribution lies in the rejection region}{figure.8.6}{}}
\abx@aux@cite{Box1976}
\abx@aux@segm{0}{0}{Box1976}
\abx@aux@cite{Cohen1988}
\abx@aux@segm{0}{0}{Cohen1988}
\abx@aux@cite{Ellis2010}
\abx@aux@segm{0}{0}{Ellis2010}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces The probability that we will reject the null hypothesis, plotted as a function of the true value of $\theta $. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of $\theta $ is very different from the value that the null hypothesis specifies (i.e., $\theta =.5$). Notice that when $\theta $ actually is equal to .5 (plotted as a black dot), the null hypothesis is in fact true and rejecting the null hypothesis in this instance would be a Type I error.}}{166}{figure.8.7}}
\newlabel{fig:powerfunction}{{8.7}{166}{The probability that we will reject the null hypothesis, plotted as a function of the true value of $\theta $. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of $\theta $ is very different from the value that the null hypothesis specifies (i.e., $\theta =.5$). Notice that when $\theta $ actually is equal to .5 (plotted as a black dot), the null hypothesis is in fact true and rejecting the null hypothesis in this instance would be a Type I error}{figure.8.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.2}Effect size}{166}{subsection.8.8.2}}
\abx@aux@backref{24}{Box1976}{0}{166}{166}
\abx@aux@page{24}{166}
\abx@aux@backref{25}{Cohen1988}{0}{166}{166}
\abx@aux@page{25}{166}
\abx@aux@backref{26}{Ellis2010}{0}{166}{166}
\abx@aux@page{26}{166}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces A crude guide to understanding the relationship between statistical significance and effect sizes. Basically, if you don't have a significant result then the effect size is pretty meaningless because you don't have any evidence that it's even real. On the other hand, if you do have a significant effect but your effect size is small then there's a pretty good chance that your result (although real) isn't all that interesting. However, this guide is very crude. It depends a lot on what exactly you're studying. Small effects can be of massive practical importance in some situations. So don't take this table too seriously. It's a rough guide at best.}}{167}{table.8.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.3}Increasing the power of your study}{168}{subsection.8.8.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces The power of our test plotted as a function of the sample size $N$. In this case, the true value of $\theta $ is 0.7 but the null hypothesis is that $\theta = 0.5$. Overall, larger $N$ means greater power. (The small zig-zags in this function occur because of some odd interactions between $\theta $, $\alpha $ and the fact that the binomial distribution is discrete, it doesn't matter for any serious purpose).}}{169}{figure.8.8}}
\newlabel{fig:powerfunctionsample}{{8.8}{169}{The power of our test plotted as a function of the sample size $N$. In this case, the true value of $\theta $ is 0.7 but the null hypothesis is that $\theta = 0.5$. Overall, larger $N$ means greater power. (The small zig-zags in this function occur because of some odd interactions between $\theta $, $\alpha $ and the fact that the binomial distribution is discrete, it doesn't matter for any serious purpose)}{figure.8.8}{}}
\abx@aux@cite{Lehmann2011}
\abx@aux@segm{0}{0}{Lehmann2011}
\newlabel{sec:nhstmess}{{8.9}{170}{Some issues to consider~\label {sec:nhstmess}}{section.8.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.9}Some issues to consider~}{170}{section.8.9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.1}Neyman versus Fisher}{170}{subsection.8.9.1}}
\abx@aux@backref{27}{Lehmann2011}{0}{170}{170}
\abx@aux@page{27}{170}
\abx@aux@cite{Gelman2006}
\abx@aux@segm{0}{0}{Gelman2006}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.2}Bayesians versus frequentists}{171}{subsection.8.9.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.3}Traps}{171}{subsection.8.9.3}}
\abx@aux@backref{28}{Gelman2006}{0}{172}{172}
\abx@aux@page{28}{172}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.10}Summary}{172}{section.8.10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Statistical tools}{175}{part.4}}
\abx@aux@cite{Pearson1900}
\abx@aux@segm{0}{0}{Pearson1900}
\abx@aux@cite{Fisher1922}
\abx@aux@segm{0}{0}{Fisher1922}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {9}Categorical data analysis}{177}{chapter.9}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:chisquare}{{9}{177}{Categorical data analysis}{chapter.9}{}}
\newlabel{sec:goftest}{{9.1}{177}{The \texorpdfstring {\boldm {$\chi ^2$}}{} (chi-square) goodness-of-fit test~\label {sec:goftest}}{section.9.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.1}The \mathversion  {bold}$\chi ^2$\mathversion  {normal} (chi-square) goodness-of-fit test~}{177}{section.9.1}}
\abx@aux@backref{29}{Pearson1900}{0}{177}{177}
\abx@aux@page{29}{177}
\abx@aux@backref{30}{Fisher1922}{0}{177}{177}
\abx@aux@page{30}{177}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}The cards data}{178}{subsection.9.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}The null hypothesis and the alternative hypothesis}{179}{subsection.9.1.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}The ``goodness-of-fit'' test statistic}{180}{subsection.9.1.3}}
\zref@newlabel{mdf@pagelabel-13}{\default{9.1}\page{181}\abspage{194}\mdf@pagevalue{181}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}The sampling distribution of the GOF statistic }{181}{subsection.9.1.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces $\chi ^2$ (chi-square) distributions with different values for the ``degrees of freedom''.}}{183}{figure.9.1}}
\newlabel{fig:manychi}{{9.1}{183}{$\chi ^2$ (chi-square) distributions with different values for the ``degrees of freedom''}{figure.9.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.5}Degrees of freedom}{183}{subsection.9.1.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.6}Testing the null hypothesis}{184}{subsection.9.1.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Illustration of how the hypothesis testing works for the $\chi ^2$ (chi-square) goodness-of-fit test.}}{185}{figure.9.2}}
\newlabel{fig:goftest}{{9.2}{185}{Illustration of how the hypothesis testing works for the $\chi ^2$ (chi-square) goodness-of-fit test}{figure.9.2}{}}
\newlabel{sec:gofTestInJASP}{{9.1.7}{185}{Doing the test in JASP\label {sec:gofTestInJASP}}{subsection.9.1.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.7}Doing the test in JASP}{185}{subsection.9.1.7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Table of critical values for the chi-square distribution}}{186}{figure.9.3}}
\newlabel{fig:chisquare.critvalues}{{9.3}{186}{Table of critical values for the chi-square distribution}{figure.9.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces A $\chi ^2$ goodness-of-fit test in JASP, with table showing both observed and expected frequencies.}}{186}{figure.9.4}}
\newlabel{fig:chisquare.analysis1}{{9.4}{186}{A $\chi ^2$ goodness-of-fit test in JASP, with table showing both observed and expected frequencies}{figure.9.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.8}Specifying a different null hypothesis}{187}{subsection.9.1.8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Changing the expected proportions in the $\chi ^2$ goodness-of-fit test in JASP}}{187}{figure.9.5}}
\newlabel{fig:chisquare.analysis2}{{9.5}{187}{Changing the expected proportions in the \texorpdfstring {$\chi ^2$}{} goodness-of-fit test in JASP}{figure.9.5}{}}
\newlabel{sec:chisqreport}{{9.1.9}{188}{How to report the results of the test~\label {sec:chisqreport}}{subsection.9.1.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.9}How to report the results of the test~}{188}{subsection.9.1.9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.10}A comment on statistical notation }{189}{subsection.9.1.10}}
\abx@aux@cite{Sokal1994}
\abx@aux@segm{0}{0}{Sokal1994}
\abx@aux@backref{31}{Sokal1994}{0}{190}{190}
\abx@aux@page{31}{190}
\newlabel{sec:chisqindependence}{{9.2}{191}{The \texorpdfstring {\boldm {$\chi ^2$}}{} test of independence (or association)~\label {sec:chisqindependence}}{section.9.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.2}The \mathversion  {bold}$\chi ^2$\mathversion  {normal} test of independence (or association)~}{191}{section.9.2}}
\abx@aux@cite{Hogg2005}
\abx@aux@segm{0}{0}{Hogg2005}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Constructing our hypothesis test}{192}{subsection.9.2.1}}
\abx@aux@backref{32}{Hogg2005}{0}{192}{192}
\abx@aux@fnpage{32}{192}
\zref@newlabel{mdf@pagelabel-14}{\default{9.2}\page{193}\abspage{206}\mdf@pagevalue{193}}
\newlabel{sec:AssocTestInJASP}{{9.2.2}{194}{Doing the test in JASP\label {sec:AssocTestInJASP}}{subsection.9.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Doing the test in JASP}{194}{subsection.9.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Postscript}{195}{subsection.9.2.3}}
\newlabel{sec:yates}{{9.3}{195}{The continuity correction~\label {sec:yates}}{section.9.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.3}The continuity correction~}{195}{section.9.3}}
\abx@aux@cite{Yates1934}
\abx@aux@segm{0}{0}{Yates1934}
\abx@aux@cite{Cramer1946}
\abx@aux@segm{0}{0}{Cramer1946}
\zref@newlabel{mdf@pagelabel-15}{\default{9.3}\page{196}\abspage{209}\mdf@pagevalue{196}}
\abx@aux@backref{33}{Yates1934}{0}{196}{196}
\abx@aux@page{33}{196}
\newlabel{sec:chisqeffectsize}{{9.4}{196}{Effect size~\label {sec:chisqeffectsize}}{section.9.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.4}Effect size~}{196}{section.9.4}}
\zref@newlabel{mdf@pagelabel-16}{\default{9.4}\page{196}\abspage{209}\mdf@pagevalue{196}}
\abx@aux@cite{Cochran1954}
\abx@aux@segm{0}{0}{Cochran1954}
\abx@aux@cite{Larntz1978}
\abx@aux@segm{0}{0}{Larntz1978}
\abx@aux@backref{34}{Cramer1946}{0}{197}{197}
\abx@aux@page{34}{197}
\newlabel{sec:chisqassumptions}{{9.5}{197}{Assumptions of the test(s)~\label {sec:chisqassumptions}}{section.9.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.5}Assumptions of the test(s)~}{197}{section.9.5}}
\abx@aux@backref{35}{Cochran1954}{0}{197}{197}
\abx@aux@page{35}{197}
\abx@aux@backref{36}{Larntz1978}{0}{197}{197}
\abx@aux@page{36}{197}
\abx@aux@cite{Agresti1996}
\abx@aux@segm{0}{0}{Agresti1996}
\abx@aux@cite{Agresti2002}
\abx@aux@segm{0}{0}{Agresti2002}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.6}Summary}{198}{section.9.6}}
\abx@aux@backref{37}{Agresti1996}{0}{198}{198}
\abx@aux@page{37}{198}
\abx@aux@backref{38}{Agresti2002}{0}{199}{199}
\abx@aux@page{38}{199}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {10}Comparing two means }{201}{chapter.10}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:ttest}{{10}{201}{Comparing two means}{chapter.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.1}The one-sample \mathversion  {bold}$z$\mathversion  {normal}-test}{201}{section.10.1}}
\newlabel{sec:onesampleztest}{{10.1}{201}{The one-sample \texorpdfstring {\boldm {$z$}}{}-test}{section.10.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}The inference problem that the test addresses}{202}{subsection.10.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Constructing the hypothesis test}{202}{subsection.10.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces The theoretical distribution (solid line) from which the psychology student grades (bars) are supposed to have been generated.}}{203}{figure.10.1}}
\newlabel{fig:zeppo}{{10.1}{203}{The theoretical distribution (solid line) from which the psychology student grades (bars) are supposed to have been generated}{figure.10.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the one sample $z$-test (the two sided version, that is). The null and alternative hypotheses both assume that the population distribution is normal, and additionally assumes that the population standard deviation is known (fixed at some value $\sigma _0$). The null hypothesis (left) is that the population mean $\mu $ is equal to some specified value $\mu _0$. The alternative hypothesis is that the population mean differs from this value, $\mu \neq \mu _0$.}}{204}{figure.10.2}}
\newlabel{fig:ztesthyp}{{10.2}{204}{Graphical illustration of the null and alternative hypotheses assumed by the one sample $z$-test (the two sided version, that is). The null and alternative hypotheses both assume that the population distribution is normal, and additionally assumes that the population standard deviation is known (fixed at some value $\sigma _0$). The null hypothesis (left) is that the population mean $\mu $ is equal to some specified value $\mu _0$. The alternative hypothesis is that the population mean differs from this value, $\mu \neq \mu _0$}{figure.10.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}A worked example, by hand}{205}{subsection.10.1.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Rejection regions for the two-sided $z$-test (panel a) and the one-sided $z$-test (panel b).}}{206}{figure.10.3}}
\newlabel{fig:ztest}{{10.3}{206}{Rejection regions for the two-sided $z$-test (panel a) and the one-sided $z$-test (panel b)}{figure.10.3}{}}
\newlabel{sec:zassumptions}{{10.1.4}{207}{Assumptions of the \texorpdfstring {$z$}{z}-test~\label {sec:zassumptions}}{subsection.10.1.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}Assumptions of the $z$-test~}{207}{subsection.10.1.4}}
\abx@aux@cite{Student1908}
\abx@aux@segm{0}{0}{Student1908}
\abx@aux@cite{Box1987}
\abx@aux@segm{0}{0}{Box1987}
\newlabel{sec:onesamplettest}{{10.2}{208}{The one-sample \texorpdfstring {\boldm {$t$}}{}-test~\label {sec:onesamplettest}}{section.10.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.2}The one-sample \mathversion  {bold}$t$\mathversion  {normal}-test~}{208}{section.10.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Introducing the $t$-test}{208}{subsection.10.2.1}}
\abx@aux@backref{39}{Student1908}{0}{208}{208}
\abx@aux@page{39}{208}
\abx@aux@backref{40}{Box1987}{0}{208}{208}
\abx@aux@page{40}{208}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the (two sided) one sample $t$-test. Note the similarity to the $z$-test (Figure~\ref  {fig:ztesthyp}). The null hypothesis is that the population mean $\mu $ is equal to some specified value $\mu _0$, and the alternative hypothesis is that it is not. Like the $z$-test, we assume that the data are normally distributed, but we do not assume that the population standard deviation $\sigma $ is known in advance.}}{209}{figure.10.4}}
\newlabel{fig:ttesthyp_onesample}{{10.4}{209}{Graphical illustration of the null and alternative hypotheses assumed by the (two sided) one sample \texorpdfstring {$t$}{}-test. Note the similarity to the \texorpdfstring {$z$}{}-test (Figure~\ref {fig:ztesthyp}). The null hypothesis is that the population mean \texorpdfstring {$\mu $}{} is equal to some specified value \texorpdfstring {$\mu _0$}{}, and the alternative hypothesis is that it is not. Like the \texorpdfstring {$z$}{}-test, we assume that the data are normally distributed, but we do not assume that the population standard deviation \texorpdfstring {$\sigma $}{} is known in advance}{figure.10.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Doing the test in JASP}{209}{subsection.10.2.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces The $t$ distribution with 2 degrees of freedom (left) and 10 degrees of freedom (right), with a standard normal distribution (i.e., mean 0 and std dev 1) plotted as dotted lines for comparison purposes. Notice that the $t$ distribution has heavier tails (leptokurtic: higher kurtosis) than the normal distribution; this effect is quite exaggerated when the degrees of freedom are very small, but negligible for larger values. In other words, for large $df$ the $t$ distribution is essentially identical to a normal distribution.}}{210}{figure.10.5}}
\newlabel{fig:ttestdist}{{10.5}{210}{The $t$ distribution with 2 degrees of freedom (left) and 10 degrees of freedom (right), with a standard normal distribution (i.e., mean 0 and std dev 1) plotted as dotted lines for comparison purposes. Notice that the $t$ distribution has heavier tails (leptokurtic: higher kurtosis) than the normal distribution; this effect is quite exaggerated when the degrees of freedom are very small, but negligible for larger values. In other words, for large $df$ the $t$ distribution is essentially identical to a normal distribution}{figure.10.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces JASP does the one-sample t-test.}}{211}{figure.10.6}}
\newlabel{fig:ttest_one}{{10.6}{211}{JASP does the one-sample t-test}{figure.10.6}{}}
\newlabel{sec:ttestoneassumptions}{{10.2.3}{211}{Assumptions of the one sample \texorpdfstring {\boldm {$t$}}{}-test~\label {sec:ttestoneassumptions}}{subsection.10.2.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Assumptions of the one sample \mathversion  {bold}$t$\mathversion  {normal}-test~}{211}{subsection.10.2.3}}
\newlabel{sec:studentttest}{{10.3}{212}{The independent samples \texorpdfstring {\boldm {$t$}}{}-test (Student test)~\label {sec:studentttest}}{section.10.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.3}The independent samples \mathversion  {bold}$t$\mathversion  {normal}-test (Student test)~}{212}{section.10.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}The data}{212}{subsection.10.3.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Histograms showing the distribution of grades for students in Anastasia's (panel a) and in Bernadette's (panel b) classes. Visually, these suggest that students in Anastasia's class may be getting slightly better grades on average, though they also seem a bit more variable.}}{213}{figure.10.7}}
\newlabel{fig:harpohist}{{10.7}{213}{Histograms showing the distribution of grades for students in Anastasia's (panel a) and in Bernadette's (panel b) classes. Visually, these suggest that students in Anastasia's class may be getting slightly better grades on average, though they also seem a bit more variable}{figure.10.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Introducing the test}{213}{subsection.10.3.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces The plots show the mean grade for students in Anastasia\IeC {\textquoteright }s and Bernadette\IeC {\textquoteright }s tutorials. Error bars depict 95\% confidence intervals around the mean. Visually, it does look like there's a real difference between the groups, though it's hard to say for sure.}}{214}{figure.10.8}}
\newlabel{fig:ttestci}{{10.8}{214}{The plots show the mean grade for students in Anastasias and Bernadettes tutorials. Error bars depict 95\% confidence intervals around the mean. Visually, it does look like there's a real difference between the groups, though it's hard to say for sure}{figure.10.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the Student $t$-test. The null hypothesis assumes that both groups have the same mean $\mu $, whereas the alternative assumes that they have different means $\mu _1$ and $\mu _2$. Notice that it is assumed that the population distributions are normal, and that, although the alternative hypothesis allows the group to have different means, it assumes they have the same standard deviation.}}{215}{figure.10.9}}
\newlabel{fig:ttesthyp}{{10.9}{215}{Graphical illustration of the null and alternative hypotheses assumed by the Student $t$-test. The null hypothesis assumes that both groups have the same mean $\mu $, whereas the alternative assumes that they have different means $\mu _1$ and $\mu _2$. Notice that it is assumed that the population distributions are normal, and that, although the alternative hypothesis allows the group to have different means, it assumes they have the same standard deviation}{figure.10.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}A ``pooled estimate'' of the standard deviation}{215}{subsection.10.3.3}}
\zref@newlabel{mdf@pagelabel-17}{\default{10.3}\page{216}\abspage{229}\mdf@pagevalue{216}}
\zref@newlabel{mdf@pagelabel-18}{\default{10.3}\page{217}\abspage{230}\mdf@pagevalue{217}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.4}Completing the test}{217}{subsection.10.3.4}}
\zref@newlabel{mdf@pagelabel-19}{\default{10.3}\page{217}\abspage{230}\mdf@pagevalue{217}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.5}Doing the test in JASP}{218}{subsection.10.3.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces Independent $t$-test in JASP, with options checked for useful results}}{218}{figure.10.10}}
\newlabel{fig:ttest_ind}{{10.10}{218}{Independent $t$-test in JASP, with options checked for useful results}{figure.10.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.6}Positive and negative $t$ values}{219}{subsection.10.3.6}}
\newlabel{sec:studentassumptions}{{10.3.7}{220}{Assumptions of the test~\label {sec:studentassumptions}}{subsection.10.3.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.7}Assumptions of the test~}{220}{subsection.10.3.7}}
\abx@aux@cite{Welch1947}
\abx@aux@segm{0}{0}{Welch1947}
\newlabel{sec:welchttest}{{10.4}{221}{The independent samples \texorpdfstring {\boldm {$t$}}{}-test (Welch test)~\label {sec:welchttest}}{section.10.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.4}The independent samples \mathversion  {bold}$t$\mathversion  {normal}-test (Welch test)~}{221}{section.10.4}}
\abx@aux@backref{41}{Welch1947}{0}{221}{221}
\abx@aux@page{41}{221}
\zref@newlabel{mdf@pagelabel-20}{\default{10.4}\page{221}\abspage{234}\mdf@pagevalue{221}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the Welch $t$-test. Like the Student test (Figure~\ref  {fig:ttesthyp}) we assume that both samples are drawn from a normal population; but the alternative hypothesis no longer requires the two populations to have equal variance.}}{222}{figure.10.11}}
\newlabel{fig:ttesthyp2}{{10.11}{222}{Graphical illustration of the null and alternative hypotheses assumed by the Welch $t$-test. Like the Student test (Figure~\ref {fig:ttesthyp}) we assume that both samples are drawn from a normal population; but the alternative hypothesis no longer requires the two populations to have equal variance}{figure.10.11}{}}
\zref@newlabel{mdf@pagelabel-21}{\default{10.4}\page{222}\abspage{235}\mdf@pagevalue{222}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.1}Doing the Welch test in JASP}{223}{subsection.10.4.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.12}{\ignorespaces Results showing the Welch test alongside the default Student's t-test in JASP}}{223}{figure.10.12}}
\newlabel{fig:ttest_welch}{{10.12}{223}{Results showing the Welch test alongside the default Student's t-test in JASP}{figure.10.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.2}Assumptions of the test}{223}{subsection.10.4.2}}
\newlabel{sec:pairedsamplesttest}{{10.5}{224}{The paired-samples \texorpdfstring {\boldm {$t$}}{}-test~\label {sec:pairedsamplesttest}}{section.10.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.5}The paired-samples \mathversion  {bold}$t$\mathversion  {normal}-test~}{224}{section.10.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}The data}{224}{subsection.10.5.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.13}{\ignorespaces Descriptives for the two grade\_test variables in the \texttt  {chico} data set}}{225}{figure.10.13}}
\newlabel{fig:ttest_paired1}{{10.13}{225}{Descriptives for the two grade\_test variables in the \texttt {chico} data set}{figure.10.13}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.14}{\ignorespaces Mean grade for test 1 and test 2, with associated 95\% confidence intervals (panel a). Scatterplot showing the individual grades for test 1 and test 2 (panel b). Histogram showing the improvement made by each student in Dr Chico's class (panel c). In panel c, notice that almost the entire distribution is above zero: the vast majority of students did improve their performance from the first test to the second one}}{226}{figure.10.14}}
\newlabel{fig:pairedt}{{10.14}{226}{Mean grade for test 1 and test 2, with associated 95\% confidence intervals (panel a). Scatterplot showing the individual grades for test 1 and test 2 (panel b). Histogram showing the improvement made by each student in Dr Chico's class (panel c). In panel c, notice that almost the entire distribution is above zero: the vast majority of students did improve their performance from the first test to the second one}{figure.10.14}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.2}What is the paired samples $t$-test?}{226}{subsection.10.5.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.15}{\ignorespaces Using R code to compute an \texttt  {improvement} score in JASP.}}{227}{figure.10.15}}
\newlabel{fig:improvement}{{10.15}{227}{Using R code to compute an \texttt {improvement} score in JASP}{figure.10.15}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.3}Doing the test in JASP}{227}{subsection.10.5.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.16}{\ignorespaces Results showing a one sample $t$-test on paired difference scores}}{228}{figure.10.16}}
\newlabel{fig:ttest_paired2}{{10.16}{228}{Results showing a one sample $t$-test on paired difference scores}{figure.10.16}{}}
\newlabel{sec:onesidedttest}{{10.6}{228}{One sided tests~\label {sec:onesidedttest}}{section.10.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.6}One sided tests~}{228}{section.10.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.17}{\ignorespaces JASP results showing a `One Sample T-Test' where the actual hypothesis is one sided, i.e. that the true mean is greater than 67.5\%}}{229}{figure.10.17}}
\newlabel{fig:ttest_onesided1}{{10.17}{229}{JASP results showing a `One Sample T-Test' where the actual hypothesis is one sided, i.e. that the true mean is greater than 67.5\%}{figure.10.17}{}}
\abx@aux@segm{0}{0}{Cohen1988}
\abx@aux@cite{McGrath2006}
\abx@aux@segm{0}{0}{McGrath2006}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.18}{\ignorespaces JASP results showing an `Independent Samples T-Test' where the actual hypothesis is one sided, i.e. that Anastasia's students had higher grades than Bernadette's}}{230}{figure.10.18}}
\newlabel{fig:ttest_onesided2}{{10.18}{230}{JASP results showing an `Independent Samples T-Test' where the actual hypothesis is one sided, i.e. that Anastasia's students had higher grades than Bernadette's}{figure.10.18}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.19}{\ignorespaces JASP results showing a `Paired Samples T-Test' where the actual hypothesis is one sided, i.e. that \texttt  {grade\_test2} (`Measure 1') $>$ \texttt  {grade\_test1} (`Measure 2')}}{230}{figure.10.19}}
\newlabel{fig:ttest_onesided3}{{10.19}{230}{JASP results showing a `Paired Samples T-Test' where the actual hypothesis is one sided, i.e. that \texttt {grade\_test2} (`Measure 1') $>$ \texttt {grade\_test1} (`Measure 2')}{figure.10.19}{}}
\newlabel{sec:cohensd}{{10.7}{230}{Effect size~\label {sec:cohensd}}{section.10.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.7}Effect size~}{230}{section.10.7}}
\abx@aux@backref{42}{Cohen1988}{0}{230}{230}
\abx@aux@page{42}{230}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces A (very) rough guide to interpreting Cohen's $d$. My personal recommendation is to not use these blindly. The $d$ statistic has a natural interpretation in and of itself. It re-describes the difference in means as the number of standard deviations that separates those means. So it's generally a good idea to think about what that means in practical terms. In some contexts a ``small'' effect could be of big practical importance. In other situations a ``large'' effect may not be all that interesting.}}{231}{table.10.1}}
\newlabel{tab:cohensdinterpretation}{{10.1}{231}{A (very) rough guide to interpreting Cohen's $d$. My personal recommendation is to not use these blindly. The $d$ statistic has a natural interpretation in and of itself. It re-describes the difference in means as the number of standard deviations that separates those means. So it's generally a good idea to think about what that means in practical terms. In some contexts a ``small'' effect could be of big practical importance. In other situations a ``large'' effect may not be all that interesting}{table.10.1}{}}
\abx@aux@backref{43}{McGrath2006}{0}{231}{231}
\abx@aux@page{43}{231}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.1}Cohen's $d$ from one sample}{231}{subsection.10.7.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.2}Cohen's $d$ from a Student's $t$ test}{231}{subsection.10.7.2}}
\abx@aux@cite{Hedges1981}
\abx@aux@segm{0}{0}{Hedges1981}
\abx@aux@cite{Hedges1985}
\abx@aux@segm{0}{0}{Hedges1985}
\abx@aux@backref{44}{Hedges1981}{0}{232}{232}
\abx@aux@page{44}{232}
\abx@aux@backref{45}{Hedges1985}{0}{232}{232}
\abx@aux@page{45}{232}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.3}Cohen's $d$ from a paired-samples test}{232}{subsection.10.7.3}}
\newlabel{sec:shapiro}{{10.8}{233}{Checking the normality of a sample\label {sec:shapiro}}{section.10.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.8}Checking the normality of a sample}{233}{section.10.8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.8.1}QQ plots}{233}{subsection.10.8.1}}
\abx@aux@cite{Shapiro1965}
\abx@aux@segm{0}{0}{Shapiro1965}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.20}{\ignorespaces Histogram (panel a) and normal QQ plot (panel b) of \texttt  {normal.data}, a normally distributed sample with 100 observations. The Shapiro-Wilk statistic associated with these data is $W = .99$, indicating that no significant departures from normality were detected ($p = .73$).}}{234}{figure.10.20}}
\newlabel{fig:qq1}{{10.20}{234}{Histogram (panel a) and normal QQ plot (panel b) of \texttt {normal.data}, a normally distributed sample with 100 observations. The Shapiro-Wilk statistic associated with these data is $W = .99$, indicating that no significant departures from normality were detected ($p = .73$)}{figure.10.20}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.8.2}Shapiro-Wilk tests}{234}{subsection.10.8.2}}
\abx@aux@backref{46}{Shapiro1965}{0}{234}{234}
\abx@aux@page{46}{234}
\zref@newlabel{mdf@pagelabel-22}{\default{10.8}\page{234}\abspage{247}\mdf@pagevalue{234}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.21}{\ignorespaces In the top row, a histogram (panel a) and normal QQ plot (panel b) of the 100 observations in a \texttt  {skewed.data} set. The skewness of the data here is 1.94, and is reflected in a QQ plot that curves upwards. As a consequence, the Shapiro-Wilk statistic is $W=.80$, reflecting a significant departure from normality ($p<.001$). The bottom row shows the same plots for a heavy tailed data set, again consisting of 100 observations. In this case the heavy tails in the data produce a high kurtosis (2.80), and cause the QQ plot to flatten in the middle, and curve away sharply on either side. The resulting Shapiro-Wilk statistic is $W = .93$, again reflecting significant non-normality ($p < .001$).}}{235}{figure.10.21}}
\newlabel{fig:qq2}{{10.21}{235}{In the top row, a histogram (panel a) and normal QQ plot (panel b) of the 100 observations in a \texttt {skewed.data} set. The skewness of the data here is 1.94, and is reflected in a QQ plot that curves upwards. As a consequence, the Shapiro-Wilk statistic is $W=.80$, reflecting a significant departure from normality ($p<.001$). The bottom row shows the same plots for a heavy tailed data set, again consisting of 100 observations. In this case the heavy tails in the data produce a high kurtosis (2.80), and cause the QQ plot to flatten in the middle, and curve away sharply on either side. The resulting Shapiro-Wilk statistic is $W = .93$, again reflecting significant non-normality ($p < .001$)}{figure.10.21}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.8.3}Example}{236}{subsection.10.8.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.22}{\ignorespaces Sampling distribution of the Shapiro-Wilk $W$ statistic, under the null hypothesis that the data are normally distributed, for samples of size 10, 20 and 50. Note that {\it  small} values of $W$ indicate departure from normality.}}{237}{figure.10.22}}
\newlabel{fig:swdist}{{10.22}{237}{Sampling distribution of the Shapiro-Wilk $W$ statistic, under the null hypothesis that the data are normally distributed, for samples of size 10, 20 and 50. Note that {\it small} values of $W$ indicate departure from normality}{figure.10.22}{}}
\newlabel{sec:wilcox}{{10.9}{238}{Testing non-normal data with Wilcoxon tests\label {sec:wilcox}}{section.10.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.9}Testing non-normal data with Wilcoxon tests}{238}{section.10.9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.9.1}Two sample Mann-Whitney U test}{238}{subsection.10.9.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.9.2}One sample Wilcoxon test}{239}{subsection.10.9.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.23}{\ignorespaces jamovi screen showing results for one sample and paired sample Wilcoxon non-parametric tests}}{240}{figure.10.23}}
\newlabel{fig:ttest_nonparametric}{{10.23}{240}{jamovi screen showing results for one sample and paired sample Wilcoxon non-parametric tests}{figure.10.23}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.10}Summary}{240}{section.10.10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {11}References}{243}{chapter.11}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\abx@aux@page{47}{243}
\abx@aux@page{48}{243}
\abx@aux@page{49}{243}
\abx@aux@page{50}{243}
\abx@aux@page{51}{243}
\abx@aux@page{52}{243}
\abx@aux@page{53}{243}
\abx@aux@page{54}{243}
\abx@aux@page{55}{243}
\abx@aux@page{56}{243}
\abx@aux@page{57}{243}
\abx@aux@page{58}{243}
\abx@aux@page{59}{243}
\abx@aux@page{60}{243}
\abx@aux@page{61}{243}
\abx@aux@page{62}{243}
\abx@aux@page{63}{243}
\abx@aux@page{64}{244}
\abx@aux@page{65}{244}
\abx@aux@page{66}{244}
\abx@aux@page{67}{244}
\abx@aux@page{68}{244}
\abx@aux@page{69}{244}
\abx@aux@page{70}{244}
\abx@aux@page{71}{244}
\abx@aux@page{72}{244}
\abx@aux@page{73}{244}
\abx@aux@page{74}{244}
\abx@aux@page{75}{244}
\abx@aux@page{76}{244}
\abx@aux@page{77}{244}
\abx@aux@page{78}{244}
\abx@aux@page{79}{244}
\abx@aux@page{80}{244}
\abx@aux@page{81}{244}
\abx@aux@page{82}{244}
\abx@aux@page{83}{244}
\abx@aux@page{84}{244}
\abx@aux@page{85}{244}
\abx@aux@page{86}{245}
\abx@aux@page{87}{245}
\abx@aux@page{88}{245}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{Adair1984}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Agresti1996}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Agresti2002}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Bickel1975}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Box1976}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Box1987}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Campbell1963}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Cochran1954}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Cohen1988}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Cramer1946}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Ellis2010}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Ellman2002}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Evans1983}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Evans2000}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Fisher1922}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Fisher1922b}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Gelman2006}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Gelman2014}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hedges1981}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hedges1985}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hogg2005}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hothersall2004}{nyt/global/}
\abx@aux@defaultrefcontext{0}{hrobjartsson2010}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Ioannidis2005}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Kahneman1973}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Keynes1923}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Kuhberger2014}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Larntz1978}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Lehmann2011}{nyt/global/}
\abx@aux@defaultrefcontext{0}{McGrath2006}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Meehl1967}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Pearson1900}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Pfungst1911}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Rosenthal1966}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Shapiro1965}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Sokal1994}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Stevens1946}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Stigler1986}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Student1908}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Welch1947}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Wilkinson2006}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Yates1934}{nyt/global/}
