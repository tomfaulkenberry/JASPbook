\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\abx@aux@sortscheme{nyt}
\abx@aux@refcontext{nyt/global/}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\select@language{english}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Preface}{ix}{chapter*.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Background}{1}{part.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {1}Why do we learn statistics?~}{3}{chapter.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:whystats}{{1}{3}{Why do we learn statistics?~}{chapter.1}{}}
\newlabel{sec:whywhywhy}{{1.1}{3}{On the psychology of statistics~\label {sec:whywhywhy}}{section.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.1}On the psychology of statistics~}{3}{section.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}The curse of belief bias}{4}{subsection.1.1.1}}
\abx@aux@cite{Evans1983}
\abx@aux@segm{0}{0}{Evans1983}
\abx@aux@backref{1}{Evans1983}{0}{5}{5}
\abx@aux@page{1}{5}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.2}The cautionary tale of Simpson's paradox}{6}{section.1.2}}
\abx@aux@cite{Bickel1975}
\abx@aux@segm{0}{0}{Bickel1975}
\abx@aux@backref{2}{Bickel1975}{0}{7}{7}
\abx@aux@page{2}{7}
\abx@aux@segm{0}{0}{Bickel1975}
\abx@aux@segm{0}{0}{Bickel1975}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The Berkeley 1973 college admissions data.}}{8}{figure.1.1}}
\abx@aux@backref{4}{Bickel1975}{0}{8}{8}
\newlabel{fig:berkeley}{{1.1}{8}{The Berkeley 1973 college admissions data}{figure.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.3}Statistics in psychology}{9}{section.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.4}Statistics in everyday life}{11}{section.1.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.5}There's more to research methods than statistics}{11}{section.1.5}}
\abx@aux@cite{Campbell1963}
\abx@aux@segm{0}{0}{Campbell1963}
\abx@aux@cite{Stevens1946}
\abx@aux@segm{0}{0}{Stevens1946}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}A brief introduction to research design}{13}{chapter.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:studydesign}{{2}{13}{A brief introduction to research design}{chapter.2}{}}
\abx@aux@backref{5}{Campbell1963}{0}{13}{13}
\abx@aux@page{5}{13}
\abx@aux@backref{6}{Stevens1946}{0}{13}{13}
\abx@aux@page{6}{13}
\newlabel{sec:measurement}{{2.1}{13}{Introduction to psychological measurement~\label {sec:measurement}}{section.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to psychological measurement~}{13}{section.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Some thoughts about psychological measurement}{14}{subsection.2.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Operationalisation: defining your measurement}{15}{subsection.2.1.2}}
\newlabel{sec:scales}{{2.2}{17}{Scales of measurement\label {sec:scales}}{section.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Scales of measurement}{17}{section.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Nominal scale}{17}{subsection.2.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Ordinal scale}{18}{subsection.2.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Interval scale}{19}{subsection.2.2.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Ratio scale}{20}{subsection.2.2.4}}
\newlabel{sec:continuousdiscrete}{{2.2.5}{20}{Continuous versus discrete variables~\label {sec:continuousdiscrete}}{subsection.2.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Continuous versus discrete variables~}{20}{subsection.2.2.5}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The relationship between the scales of measurement and the discrete/continuity distinction. Cells with a tick mark correspond to things that are possible.}}{21}{table.2.1}}
\newlabel{tab:scalescont}{{2.1}{21}{The relationship between the scales of measurement and the discrete/continuity distinction. Cells with a tick mark correspond to things that are possible}{table.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Some complexities}{21}{subsection.2.2.6}}
\newlabel{sec:reliability}{{2.3}{22}{Assessing the reliability of a measurement~\label {sec:reliability}}{section.2.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Assessing the reliability of a measurement~}{22}{section.2.3}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The terminology used to distinguish between different roles that a variable can play when analysing a data set. Note that this book will tend to avoid the classical terminology in favour of the newer names.}}{24}{table.2.2}}
\newlabel{tab:ivdv}{{2.2}{24}{The terminology used to distinguish between different roles that a variable can play when analysing a data set. Note that this book will tend to avoid the classical terminology in favour of the newer names}{table.2.2}{}}
\newlabel{sec:ivdv}{{2.4}{24}{The \texorpdfstring {``role''}{role} of variables: predictors and outcomes \label {sec:ivdv}}{section.2.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}The ``role'' of variables: predictors and outcomes }{24}{section.2.4}}
\newlabel{sec:researchdesigns}{{2.5}{25}{Experimental and non-experimental research~\label {sec:researchdesigns}}{section.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.5}Experimental and non-experimental research~}{25}{section.2.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Experimental research}{25}{subsection.2.5.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Non-experimental research}{26}{subsection.2.5.2}}
\newlabel{sec:validity}{{2.6}{27}{Assessing the validity of a study~\label {sec:validity}}{section.2.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.6}Assessing the validity of a study~}{27}{section.2.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Internal validity}{27}{subsection.2.6.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}External validity}{28}{subsection.2.6.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Construct validity}{29}{subsection.2.6.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Face validity}{29}{subsection.2.6.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Ecological validity}{30}{subsection.2.6.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.7}Confounds, artefacts and other threats to validity}{30}{section.2.7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}History effects}{31}{subsection.2.7.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Maturation effects}{32}{subsection.2.7.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Repeated testing effects}{33}{subsection.2.7.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}Selection bias}{33}{subsection.2.7.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.5}Differential attrition}{33}{subsection.2.7.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.6}Non-response bias}{34}{subsection.2.7.6}}
\abx@aux@cite{Kahneman1973}
\abx@aux@segm{0}{0}{Kahneman1973}
\abx@aux@cite{Pfungst1911}
\abx@aux@segm{0}{0}{Pfungst1911}
\abx@aux@cite{Hothersall2004}
\abx@aux@segm{0}{0}{Hothersall2004}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.7}Regression to the mean}{35}{subsection.2.7.7}}
\abx@aux@backref{7}{Kahneman1973}{0}{35}{35}
\abx@aux@page{7}{35}
\abx@aux@cite{Rosenthal1966}
\abx@aux@segm{0}{0}{Rosenthal1966}
\abx@aux@cite{Adair1984}
\abx@aux@segm{0}{0}{Adair1984}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.8}Experimenter bias}{36}{subsection.2.7.8}}
\abx@aux@backref{8}{Pfungst1911}{0}{36}{36}
\abx@aux@page{8}{36}
\abx@aux@backref{9}{Hothersall2004}{0}{36}{36}
\abx@aux@page{9}{36}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.9}Demand effects and reactivity}{36}{subsection.2.7.9}}
\abx@aux@backref{10}{Rosenthal1966}{0}{36}{36}
\abx@aux@page{10}{36}
\abx@aux@cite{hrobjartsson2010}
\abx@aux@segm{0}{0}{hrobjartsson2010}
\abx@aux@backref{11}{Adair1984}{0}{37}{37}
\abx@aux@page{11}{37}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.10}Placebo effects}{37}{subsection.2.7.10}}
\abx@aux@backref{12}{hrobjartsson2010}{0}{37}{37}
\abx@aux@page{12}{37}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.11}Situation, measurement and sub-population effects}{37}{subsection.2.7.11}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.12}Fraud, deception and self-deception}{38}{subsection.2.7.12}}
\abx@aux@cite{Gelman2014}
\abx@aux@segm{0}{0}{Gelman2014}
\abx@aux@backref{13}{Gelman2014}{0}{39}{39}
\abx@aux@page{13}{39}
\abx@aux@cite{Ioannidis2005}
\abx@aux@segm{0}{0}{Ioannidis2005}
\abx@aux@cite{Kuhberger2014}
\abx@aux@segm{0}{0}{Kuhberger2014}
\abx@aux@backref{14}{Ioannidis2005}{0}{40}{40}
\abx@aux@page{14}{40}
\abx@aux@backref{15}{Kuhberger2014}{0}{40}{40}
\abx@aux@page{15}{40}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.8}Summary}{40}{section.2.8}}
\abx@aux@segm{0}{0}{Campbell1963}
\abx@aux@backref{16}{Campbell1963}{0}{41}{41}
\abx@aux@page{16}{41}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Describing and displaying data with JASP}{43}{part.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Getting started with JASP}{45}{chapter.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introj}{{3}{45}{Getting started with JASP}{chapter.3}{}}
\newlabel{sec:gettingjasp}{{3.1}{46}{Installing JASP \label {sec:gettingjasp}}{section.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}Installing JASP }{46}{section.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Starting up JASP}{46}{subsection.3.1.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces JASP looks like this when you start it.}}{47}{figure.3.1}}
\newlabel{fig:startingjasp}{{3.1}{47}{JASP looks like this when you start it}{figure.3.1}{}}
\newlabel{sec:analyses}{{3.2}{47}{Analyses\label {sec:analyses}}{section.3.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}Analyses}{47}{section.3.2}}
\newlabel{sec:load}{{3.3}{48}{Loading data in JASP\label {sec:load}}{section.3.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}Loading data in JASP}{48}{section.3.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Importing data from CSV files}{48}{subsection.3.3.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The \texttt  {booksales.csv} data file. On the left I've opened the file using a spreadsheet program, which shows that the file is basically a table. On the right the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are separated by commas.}}{49}{figure.3.2}}
\newlabel{fig:booksalescsv}{{3.2}{49}{The \filename {booksales.csv} data file. On the left I've opened the file using a spreadsheet program, which shows that the file is basically a table. On the right the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are separated by commas}{figure.3.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A dialog box on a Mac asking you to select the CSV file JASP should try to import. Mac users will recognise this immediately -- it's the usual way in which a Mac asks you to find a file. Windows users won't see this, but instead will see the usual explorer window that Windows always gives you when it wants you to select a file.}}{50}{figure.3.3}}
\newlabel{fig:fileopen}{{3.3}{50}{A dialog box on a Mac asking you to select the CSV file JASP should try to import. Mac users will recognise this immediately -- it's the usual way in which a Mac asks you to find a file. Windows users won't see this, but instead will see the usual explorer window that Windows always gives you when it wants you to select a file}{figure.3.3}{}}
\newlabel{sec:spreadsheet}{{3.4}{50}{The spreadsheet\label {sec:spreadsheet}}{section.3.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}The spreadsheet}{50}{section.3.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Variables}{51}{subsection.3.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Computed variables}{51}{subsection.3.4.2}}
\newlabel{sec:copypaste}{{3.4.3}{51}{Copy and Paste\label {sec:copypaste}}{subsection.3.4.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Copy and Paste}{51}{subsection.3.4.3}}
\newlabel{sec:coercion}{{3.5}{52}{Changing data from one measurement scale to another\label {sec:coercion}}{section.3.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.5}Changing data from one measurement scale to another}{52}{section.3.5}}
\newlabel{sec:quittingjasp}{{3.6}{52}{Quitting JASP \label {sec:quittingjasp}}{section.3.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.6}Quitting JASP }{52}{section.3.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.7}Summary}{53}{section.3.7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {4}Descriptive statistics}{55}{chapter.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:descriptives}{{4}{55}{Descriptive statistics}{chapter.4}{}}
\newlabel{sec:centraltendency}{{4.1}{55}{Measures of central tendency~\label {sec:centraltendency}}{section.4.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}Measures of central tendency~}{55}{section.4.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A screenshot of JASP showing the variables stored in the \texttt  {aflsmall\_margins.csv} file}}{56}{figure.4.1}}
\newlabel{fig:aflsmall}{{4.1}{56}{A screenshot of JASP showing the variables stored in the \filename {aflsmall\_margins.csv} file}{figure.4.1}{}}
\newlabel{sec:mean}{{4.1.1}{56}{The mean\label {sec:mean}}{subsection.4.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}The mean}{56}{subsection.4.1.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A histogram of the AFL 2010 winning margin data (the \texttt  {afl.margins} varable). As you might expect, the larger the winning margin the less frequently you tend to see it.}}{57}{figure.4.2}}
\newlabel{fig:histogram1}{{4.2}{57}{A histogram of the AFL 2010 winning margin data (the \texttt {afl.margins} varable). As you might expect, the larger the winning margin the less frequently you tend to see it}{figure.4.2}{}}
\zref@newlabel{mdf@pagelabel-1}{\default{4.1}\page{58}\abspage{73}\mdf@pagevalue{58}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Calculating the mean in JASP}{59}{subsection.4.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Default descriptives for the AFL 2010 winning margin data (the \texttt  {afl.margins} variable). }}{59}{figure.4.3}}
\newlabel{fig:descriptives_default}{{4.3}{59}{Default descriptives for the AFL 2010 winning margin data (the \texttt {afl.margins} variable)}{figure.4.3}{}}
\newlabel{sec:median}{{4.1.3}{59}{The median\label {sec:median}}{subsection.4.1.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}The median}{59}{subsection.4.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Mean or median? What's the difference?}{60}{subsection.4.1.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the ``centre of gravity'' of the data set. If you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation, with half of the observations smaller and half of the observations larger.}}{61}{figure.4.4}}
\newlabel{fig:meanmedian}{{4.4}{61}{An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the ``centre of gravity'' of the data set. If you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation, with half of the observations smaller and half of the observations larger}{figure.4.4}{}}
\newlabel{sec:housingpriceexample}{{4.1.5}{61}{A real life example~\label {sec:housingpriceexample}}{subsection.4.1.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}A real life example~}{61}{subsection.4.1.5}}
\newlabel{sec:mode}{{4.1.6}{63}{Mode\label {sec:mode}}{subsection.4.1.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Mode}{63}{subsection.4.1.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces A screenshot of JASP showing the variables stored in the \texttt  {aflsmall\_finalists.csv} file}}{63}{figure.4.5}}
\newlabel{fig:aflsmall_finalists}{{4.5}{63}{A screenshot of JASP showing the variables stored in the \filename {aflsmall\_finalists.csv} file}{figure.4.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces A screenshot of JASP showing the frequency table for the \texttt  {afl.finalists} variable }}{64}{figure.4.6}}
\newlabel{fig:aflsmall_finalists_mode}{{4.6}{64}{A screenshot of JASP showing the frequency table for the \texttt {afl.finalists} variable}{figure.4.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A screenshot of JASP showing the modal value for the \texttt  {afl.margins} variable }}{65}{figure.4.7}}
\newlabel{fig:aflsmall_margins_mode}{{4.7}{65}{A screenshot of JASP showing the modal value for the \texttt {afl.margins} variable}{figure.4.7}{}}
\newlabel{sec:var}{{4.2}{66}{Measures of variability\label {sec:var}}{section.4.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Measures of variability}{66}{section.4.2}}
\newlabel{sec:range}{{4.2.1}{66}{Range\label {sec:range}}{subsection.4.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Range}{66}{subsection.4.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Interquartile range}{66}{subsection.4.2.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces A screenshot of JASP showing the Quartiles for the \texttt  {afl.margins} variable }}{67}{figure.4.8}}
\newlabel{fig:aflsmall_margins_iqr}{{4.8}{67}{A screenshot of JASP showing the Quartiles for the \texttt {afl.margins} variable}{figure.4.8}{}}
\newlabel{sec:aad}{{4.2.3}{67}{Mean absolute deviation~\label {sec:aad}}{subsection.4.2.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Mean absolute deviation~}{67}{subsection.4.2.3}}
\zref@newlabel{mdf@pagelabel-2}{\default{4.2}\page{68}\abspage{83}\mdf@pagevalue{68}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Variance}{69}{subsection.4.2.4}}
\zref@newlabel{mdf@pagelabel-3}{\default{4.2}\page{69}\abspage{84}\mdf@pagevalue{69}}
\zref@newlabel{mdf@pagelabel-4}{\default{4.2}\page{70}\abspage{85}\mdf@pagevalue{70}}
\newlabel{sec:sd}{{4.2.5}{71}{Standard deviation\label {sec:sd}}{subsection.4.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Standard deviation}{71}{subsection.4.2.5}}
\zref@newlabel{mdf@pagelabel-5}{\default{4.2}\page{71}\abspage{86}\mdf@pagevalue{71}}
\zref@newlabel{mdf@pagelabel-6}{\default{4.2}\page{71}\abspage{86}\mdf@pagevalue{71}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces An illustration of the standard deviation from the AFL winning margins data. The shaded bars in the histogram show how much of the data fall within one standard deviation of the mean. In this case, 65.3\% of the data set lies within this range, which is pretty consistent with the ``approximately 68\% rule'' discussed in the main text.}}{72}{figure.4.9}}
\newlabel{fig:aflsd}{{4.9}{72}{An illustration of the standard deviation from the AFL winning margins data. The shaded bars in the histogram show how much of the data fall within one standard deviation of the mean. In this case, 65.3\% of the data set lies within this range, which is pretty consistent with the ``approximately 68\% rule'' discussed in the main text}{figure.4.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Which measure to use?}{72}{subsection.4.2.6}}
\newlabel{sec:skewkurt}{{4.3}{73}{Skew and kurtosis \label {sec:skewkurt}}{section.4.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.3}Skew and kurtosis }{73}{section.4.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces An illustration of skewness. On the left we have a negatively skewed data set (skewness $= -.93$), in the middle we have a data set with no skew (well, hardly any: skewness $= -.006$), and on the right we have a positively skewed data set (skewness $= .93$). }}{74}{figure.4.10}}
\newlabel{fig:skewness}{{4.10}{74}{An illustration of skewness. On the left we have a negatively skewed data set (skewness $= -.93$), in the middle we have a data set with no skew (well, hardly any: skewness $= -.006$), and on the right we have a positively skewed data set (skewness $= .93$)}{figure.4.10}{}}
\zref@newlabel{mdf@pagelabel-7}{\default{4.3}\page{74}\abspage{89}\mdf@pagevalue{74}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces An illustration of kurtosis. On the left, we have a ``platykurtic'' data set (kurtosis = $-.95$) meaning that the data set is ``too flat''. In the middle we have a ``mesokurtic'' data set (kurtosis is almost exactly 0) which means that the pointiness of the data is just about right. Finally, on the right, we have a ``leptokurtic'' data set (kurtosis $= 2.12$) indicating that the data set is ``too pointy''. Note that kurtosis is measured with respect to a normal curve (black line).}}{75}{figure.4.11}}
\newlabel{fig:kurtosis}{{4.11}{75}{An illustration of kurtosis. On the left, we have a ``platykurtic'' data set (kurtosis = $-.95$) meaning that the data set is ``too flat''. In the middle we have a ``mesokurtic'' data set (kurtosis is almost exactly 0) which means that the pointiness of the data is just about right. Finally, on the right, we have a ``leptokurtic'' data set (kurtosis $= 2.12$) indicating that the data set is ``too pointy''. Note that kurtosis is measured with respect to a normal curve (black line)}{figure.4.11}{}}
\zref@newlabel{mdf@pagelabel-8}{\default{4.3}\page{75}\abspage{90}\mdf@pagevalue{75}}
\newlabel{sec:groupdescriptives}{{4.4}{76}{Descriptive statistics separately for each group~\label {sec:groupdescriptives}}{section.4.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.4}Descriptive statistics separately for each group~}{76}{section.4.4}}
\newlabel{sec:zscore}{{4.5}{76}{Standard scores~\label {sec:zscore}}{section.4.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.5}Standard scores~}{76}{section.4.5}}
\zref@newlabel{mdf@pagelabel-9}{\default{4.5}\page{77}\abspage{92}\mdf@pagevalue{77}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.6}Summary}{78}{section.4.6}}
\abx@aux@cite{Ellman2002}
\abx@aux@segm{0}{0}{Ellman2002}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Epilogue: Good descriptive statistics are descriptive!}{79}{subsection.4.6.1}}
\abx@aux@backref{17}{Ellman2002}{0}{79}{79}
\abx@aux@page{17}{79}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces A screenshot of JASP showing the variables stored in the \texttt  {clinicaltrial.csv} file}}{81}{figure.4.12}}
\newlabel{fig:clinicaltrial}{{4.12}{81}{A screenshot of JASP showing the variables stored in the \filename {clinicaltrial.csv} file}{figure.4.12}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces A screenshot of JASP showing Descriptives split by therapy type}}{82}{figure.4.13}}
\newlabel{fig:clinicaltrial_grouping}{{4.13}{82}{A screenshot of JASP showing Descriptives split by therapy type}{figure.4.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {5}Drawing graphs}{83}{chapter.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:graphics}{{5}{83}{Drawing graphs}{chapter.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A stylised redrawing of John Snow's original cholera map. Each small dot represents the location of a cholera case and each large circle shows the location of a well. As the plot makes clear, the cholera outbreak is centred very closely on the Broad St pump.}}{84}{figure.5.1}}
\newlabel{fig:snowmap1}{{5.1}{84}{A stylised redrawing of John Snow's original cholera map. Each small dot represents the location of a cholera case and each large circle shows the location of a well. As the plot makes clear, the cholera outbreak is centred very closely on the Broad St pump}{figure.5.1}{}}
\newlabel{sec:hist}{{5.1}{84}{Histograms\label {sec:hist}}{section.5.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.1}Histograms}{84}{section.5.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces JASP screenshot showing the `Distribution plots' option and accompanying histogram.}}{85}{figure.5.2}}
\newlabel{fig:jasp_histogram}{{5.2}{85}{JASP screenshot showing the `Distribution plots' option and accompanying histogram}{figure.5.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces A density plot of the \texttt  {afl.margins} variable plotted in JASP}}{86}{figure.5.3}}
\newlabel{fig:histogram2}{{5.3}{86}{A density plot of the \texttt {afl.margins} variable plotted in JASP}{figure.5.3}{}}
\newlabel{sec:boxplots}{{5.2}{86}{Boxplots~\label {sec:boxplots}}{section.5.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.2}Boxplots~}{86}{section.5.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces A box plot of the \texttt  {afl.margins} variable plotted in JASP}}{87}{figure.5.4}}
\newlabel{fig:boxplot1}{{5.4}{87}{A box plot of the \texttt {afl.margins} variable plotted in JASP}{figure.5.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces A violin plot of the \texttt  {afl.margins} variable plotted in JASP, also showing a box plot and data points}}{88}{figure.5.5}}
\newlabel{fig:boxplot2}{{5.5}{88}{A violin plot of the \texttt {afl.margins} variable plotted in JASP, also showing a box plot and data points}{figure.5.5}{}}
\newlabel{sec:violinplots}{{5.2.1}{88}{Violin plots~\label {sec:violinplots}}{subsection.5.2.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Violin plots~}{88}{subsection.5.2.1}}
\newlabel{sec:multipleboxplots}{{5.2.2}{88}{Drawing multiple boxplots~\label {sec:multipleboxplots}}{subsection.5.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Drawing multiple boxplots~}{88}{subsection.5.2.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Multiple boxplots plotted in JASP, for the \texttt  {margin} by \texttt  {year} variables in the \texttt  {aflsmall2} data set}}{89}{figure.5.6}}
\newlabel{fig:boxplot3}{{5.6}{89}{Multiple boxplots plotted in JASP, for the \texttt {margin} by \texttt {year} variables in the \texttt {aflsmall2} data set}{figure.5.6}{}}
\newlabel{sec:saveimage}{{5.3}{89}{Saving image files using JASP~\label {sec:saveimage}}{section.5.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.3}Saving image files using JASP~}{89}{section.5.3}}
\abx@aux@cite{Wilkinson2006}
\abx@aux@segm{0}{0}{Wilkinson2006}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.4}Summary}{90}{section.5.4}}
\abx@aux@backref{18}{Wilkinson2006}{0}{90}{90}
\abx@aux@page{18}{90}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Statistical theory}{91}{part.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {6}Introduction to probability}{99}{chapter.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:probability}{{6}{99}{Introduction to probability}{chapter.6}{}}
\newlabel{sec:probstats}{{6.1}{100}{How are probability and statistics different?~\label {sec:probstats}}{section.6.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.1}How are probability and statistics different?~}{100}{section.6.1}}
\newlabel{sec:probmeaning}{{6.2}{101}{What does probability mean?\label {sec:probmeaning}}{section.6.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.2}What does probability mean?}{101}{section.6.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}The frequentist view}{102}{subsection.6.2.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you've seen eventually settles down and converges to the true probability of 0.5. Each panel shows four different simulated experiments. In each case we pretend we flipped a coin 1000 times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we'd extended the experiment for an infinite number of coin flips they would have.}}{104}{figure.6.1}}
\newlabel{fig:frequentistprobability}{{6.1}{104}{An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you've seen eventually settles down and converges to the true probability of 0.5. Each panel shows four different simulated experiments. In each case we pretend we flipped a coin 1000 times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we'd extended the experiment for an infinite number of coin flips they would have}{figure.6.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}The Bayesian view}{105}{subsection.6.2.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}What's the difference? And who is right?}{105}{subsection.6.2.3}}
\abx@aux@cite{Fisher1922b}
\abx@aux@segm{0}{0}{Fisher1922b}
\abx@aux@cite{Meehl1967}
\abx@aux@segm{0}{0}{Meehl1967}
\abx@aux@backref{19}{Fisher1922b}{0}{106}{106}
\abx@aux@page{19}{106}
\abx@aux@backref{20}{Meehl1967}{0}{106}{106}
\abx@aux@page{20}{106}
\newlabel{sec:basicprobability}{{6.3}{106}{Basic probability theory~\label {sec:basicprobability}}{section.6.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.3}Basic probability theory~}{106}{section.6.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Introducing probability distributions}{106}{subsection.6.3.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces A visual depiction of the ``trousers'' probability distribution. There are five ``elementary events'', corresponding to the five pairs of trousers that I own. Each event has some probability of occurring: this probability is a number between 0 to 1. The sum of these probabilities is 1.}}{108}{figure.6.2}}
\newlabel{fig:pantsprob}{{6.2}{108}{A visual depiction of the ``trousers'' probability distribution. There are five ``elementary events'', corresponding to the five pairs of trousers that I own. Each event has some probability of occurring: this probability is a number between 0 to 1. The sum of these probabilities is 1}{figure.6.2}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Some basic rules that probabilities must satisfy. You don't really need to know these rules in order to understand the analyses that we'll talk about later in the book, but they are important if you want to understand probability theory a bit more deeply.}}{109}{table.6.1}}
\newlabel{tab:probrules}{{6.1}{109}{Some basic rules that probabilities must satisfy. You don't really need to know these rules in order to understand the analyses that we'll talk about later in the book, but they are important if you want to understand probability theory a bit more deeply}{table.6.1}{}}
\newlabel{sec:binomial}{{6.4}{109}{The binomial distribution\label {sec:binomial}}{section.6.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.4}The binomial distribution}{109}{section.6.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Introducing the binomial}{109}{subsection.6.4.1}}
\zref@newlabel{mdf@pagelabel-10}{\default{6.4}\page{110}\abspage{125}\mdf@pagevalue{110}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Formulas for the binomial and normal distributions. We don't really use these formulas for anything in this book, but they're pretty important for more advanced work, so I thought it might be best to put them here in a table, where they can't get in the way of the text. In the equation for the binomial, $X!$ is the factorial function (i.e., multiply all whole numbers from 1 to $X$), and for the normal distribution ``exp'' refers to the exponential function. If these equations don't make a lot of sense to you, don't worry too much about them. }}{110}{table.6.2}}
\newlabel{tab:distformulas}{{6.2}{110}{Formulas for the binomial and normal distributions. We don't really use these formulas for anything in this book, but they're pretty important for more advanced work, so I thought it might be best to put them here in a table, where they can't get in the way of the text. In the equation for the binomial, $X!$ is the factorial function (i.e., multiply all whole numbers from 1 to $X$), and for the normal distribution ``exp'' refers to the exponential function. If these equations don't make a lot of sense to you, don't worry too much about them}{table.6.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The binomial distribution with size parameter of $N=20$ and an underlying success probability of $\theta = 1/6$. Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of $X$). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well.}}{111}{figure.6.3}}
\newlabel{fig:binomial1}{{6.3}{111}{The binomial distribution with size parameter of $N=20$ and an underlying success probability of $\theta = 1/6$. Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of $X$). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well}{figure.6.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Two binomial distributions, involving a scenario in which I'm flipping a fair coin, so the underlying success probability is $\theta = 1/2$. In panel (a), we assume I'm flipping the coin $N=20$ times. In panel (b) we assume that the coin is flipped $N=100$ times.}}{112}{figure.6.4}}
\newlabel{fig:binomial2}{{6.4}{112}{Two binomial distributions, involving a scenario in which I'm flipping a fair coin, so the underlying success probability is $\theta = 1/2$. In panel (a), we assume I'm flipping the coin $N=20$ times. In panel (b) we assume that the coin is flipped $N=100$ times}{figure.6.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. The $x$-axis corresponds to the value of some variable, and the $y$-axis tells us something about how likely we are to observe that value. However, notice that the $y$-axis is labelled ``Probability Density'' and not ``Probability''. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the $y$ axis behave a bit oddly: the height of the curve here isn't actually the probability of observing a particular $x$ value. On the other hand, it {\it  is} true that the heights of the curve tells you which $x$ values are more likely (the higher ones!). (see Section~\ref  {sec:density} for all the annoying details)}}{113}{figure.6.5}}
\newlabel{fig:normdist}{{6.5}{113}{The normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. The $x$-axis corresponds to the value of some variable, and the $y$-axis tells us something about how likely we are to observe that value. However, notice that the $y$-axis is labelled ``Probability Density'' and not ``Probability''. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the $y$ axis behave a bit oddly: the height of the curve here isn't actually the probability of observing a particular $x$ value. On the other hand, it {\it is} true that the heights of the curve tells you which $x$ values are more likely (the higher ones!). (see Section~\ref {sec:density} for all the annoying details)}{figure.6.5}{}}
\newlabel{sec:normal}{{6.5}{113}{The normal distribution\label {sec:normal}}{section.6.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.5}The normal distribution}{113}{section.6.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of $\mu =4$. The dashed line shows a normal distribution with a mean of $\mu =7$. In both cases, the standard deviation is $\sigma =1$. Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right.}}{114}{figure.6.6}}
\newlabel{fig:normmean}{{6.6}{114}{An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of $\mu =4$. The dashed line shows a normal distribution with a mean of $\mu =7$. In both cases, the standard deviation is $\sigma =1$. Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right}{figure.6.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of $\mu = 5$, but they have different standard deviations. The solid line plots a distribution with standard deviation $\sigma =1$, and the dashed line shows a distribution with standard deviation $\sigma = 2$. As a consequence, both distributions are ``centred'' on the same spot, but the dashed line is wider than the solid one.}}{115}{figure.6.7}}
\newlabel{fig:normsd}{{6.7}{115}{An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of $\mu = 5$, but they have different standard deviations. The solid line plots a distribution with standard deviation $\sigma =1$, and the dashed line shows a distribution with standard deviation $\sigma = 2$. As a consequence, both distributions are ``centred'' on the same spot, but the dashed line is wider than the solid one}{figure.6.7}{}}
\newlabel{sec:density}{{6.5.1}{116}{Probability density~\label {sec:density} \advanced }{subsection.6.5.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Probability density~ }{116}{subsection.6.5.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean $\mu =0$ and standard deviation $\sigma =1$. The shaded areas illustrate ``areas under the curve'' for two important cases. In panel a, we can see that there is a 68.3\% chance that an observation will fall within one standard deviation of the mean. In panel b, we see that there is a 95.4\% chance that an observation will fall within two standard deviations of the mean.}}{117}{figure.6.8}}
\newlabel{fig:sdnorm}{{6.8}{117}{The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean $\mu =0$ and standard deviation $\sigma =1$. The shaded areas illustrate ``areas under the curve'' for two important cases. In panel a, we can see that there is a 68.3\% chance that an observation will fall within one standard deviation of the mean. In panel b, we see that there is a 95.4\% chance that an observation will fall within two standard deviations of the mean}{figure.6.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Two more examples of the ``area under the curve idea''. There is a 15.9\% chance that an observation is one standard deviation below the mean or smaller (panel a), and a 34.1\% chance that the observation is somewhere between one standard deviation below the mean and the mean (panel b). Notice that if you add these two numbers together you get $15.9\% + 34.1\% = 50\%$. For normally distributed data, there is a 50\% chance that an observation falls below the mean. And of course that also implies that there is a 50\% chance that it falls above the mean.}}{117}{figure.6.9}}
\newlabel{fig:sdnorm2}{{6.9}{117}{Two more examples of the ``area under the curve idea''. There is a 15.9\% chance that an observation is one standard deviation below the mean or smaller (panel a), and a 34.1\% chance that the observation is somewhere between one standard deviation below the mean and the mean (panel b). Notice that if you add these two numbers together you get $15.9\% + 34.1\% = 50\%$. For normally distributed data, there is a 50\% chance that an observation falls below the mean. And of course that also implies that there is a 50\% chance that it falls above the mean}{figure.6.9}{}}
\newlabel{sec:otherdists}{{6.6}{118}{Other useful distributions~\label {sec:otherdists}}{section.6.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.6}Other useful distributions~}{118}{section.6.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces A $t$ distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it's not quite the same. For comparison purposes I've plotted a standard normal distribution as the dashed line.}}{119}{figure.6.10}}
\newlabel{fig:tdist}{{6.10}{119}{A $t$ distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it's not quite the same. For comparison purposes I've plotted a standard normal distribution as the dashed line}{figure.6.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces A $\chi ^2$ distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square distribution.}}{120}{figure.6.11}}
\newlabel{fig:chisqdist}{{6.11}{120}{A $\chi ^2$ distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square distribution}{figure.6.11}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces An $F$ distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they're not quite the same in general.}}{121}{figure.6.12}}
\newlabel{fig:Fdist}{{6.12}{121}{An $F$ distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they're not quite the same in general}{figure.6.12}{}}
\abx@aux@cite{Evans2000}
\abx@aux@segm{0}{0}{Evans2000}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.7}Summary}{122}{section.6.7}}
\abx@aux@backref{21}{Evans2000}{0}{122}{122}
\abx@aux@page{21}{122}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {7}Estimating unknown quantities from a sample}{123}{chapter.7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:estimation}{{7}{123}{Estimating unknown quantities from a sample}{chapter.7}{}}
\newlabel{sec:srs}{{7.1}{123}{Samples, populations and sampling~\label {sec:srs}}{section.7.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.1}Samples, populations and sampling~}{123}{section.7.1}}
\newlabel{sec:pop}{{7.1.1}{124}{Defining a population~\label {sec:pop}}{subsection.7.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Defining a population~}{124}{subsection.7.1.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Simple random sampling without replacement from a finite population}}{125}{figure.7.1}}
\newlabel{fig:srs1}{{7.1}{125}{Simple random sampling without replacement from a finite population}{figure.7.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Simple random samples}{125}{subsection.7.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Biased sampling without replacement from a finite population}}{126}{figure.7.2}}
\newlabel{fig:brs}{{7.2}{126}{Biased sampling without replacement from a finite population}{figure.7.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Simple random sampling {\it  with} replacement from a finite population}}{127}{figure.7.3}}
\newlabel{fig:srs2}{{7.3}{127}{Simple random sampling {\it with} replacement from a finite population}{figure.7.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Most samples are not simple random samples}{127}{subsection.7.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}How much does it matter if you don't have a simple random sample?}{128}{subsection.7.1.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.5}Population parameters and sample statistics}{129}{subsection.7.1.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations.}}{130}{figure.7.4}}
\newlabel{fig:IQdist}{{7.4}{130}{The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations}{figure.7.4}{}}
\abx@aux@cite{Stigler1986}
\abx@aux@segm{0}{0}{Stigler1986}
\newlabel{sec:lawlargenumbers}{{7.2}{131}{The law of large numbers~\label {sec:lawlargenumbers}}{section.7.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.2}The law of large numbers~}{131}{section.7.2}}
\abx@aux@backref{22}{Stigler1986}{0}{131}{131}
\abx@aux@page{22}{131}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces A random sample drawn from a normal distribution using JASP}}{132}{figure.7.5}}
\newlabel{fig:iqsim}{{7.5}{132}{A random sample drawn from a normal distribution using JASP}{figure.7.5}{}}
\abx@aux@cite{Keynes1923}
\abx@aux@segm{0}{0}{Keynes1923}
\newlabel{sec:samplesandclt}{{7.3}{133}{Sampling distributions and the central limit theorem~\label {sec:samplesandclt}}{section.7.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.3}Sampling distributions and the central limit theorem~}{133}{section.7.3}}
\abx@aux@backref{23}{Keynes1923}{0}{133}{133}
\abx@aux@page{23}{133}
\newlabel{sec:samplingdists}{{7.3.1}{133}{Sampling distribution of the mean~\label {sec:samplingdists}}{subsection.7.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Sampling distribution of the mean~}{133}{subsection.7.3.1}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Ten replications of the IQ experiment, each with a sample size of $N=5$.}}{134}{table.7.1}}
\newlabel{tab:replications}{{7.1}{134}{Ten replications of the IQ experiment, each with a sample size of $N=5$}{table.7.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Using JASP to draw a random sample of 5 from a normal distribution with $\mu =100$ and $\sigma =15$.}}{135}{figure.7.6}}
\newlabel{fig:IQsample}{{7.6}{135}{Using JASP to draw a random sample of 5 from a normal distribution with $\mu =100$ and $\sigma =15$}{figure.7.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces The sampling distribution of the mean for the ``five IQ scores experiment''. If you sample 5 people at random and calculate their {\it  average} IQ you'll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores.}}{136}{figure.7.7}}
\newlabel{fig:sampdistmean}{{7.7}{136}{The sampling distribution of the mean for the ``five IQ scores experiment''. If you sample 5 people at random and calculate their {\it average} IQ you'll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores}{figure.7.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Sampling distributions exist for any sample statistic!}{136}{subsection.7.3.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces The sampling distribution of the {\it  maximum} for the ``five IQ scores experiment''. If you sample 5 people at random and select the one with the highest IQ score you'll probably see someone with an IQ between 100 and 140.}}{137}{figure.7.8}}
\newlabel{fig:sampdistmax}{{7.8}{137}{The sampling distribution of the {\it maximum} for the ``five IQ scores experiment''. If you sample 5 people at random and select the one with the highest IQ score you'll probably see someone with an IQ between 100 and 140}{figure.7.8}{}}
\newlabel{sec:clt}{{7.3.3}{137}{The central limit theorem~\label {sec:clt}}{subsection.7.3.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}The central limit theorem~}{137}{subsection.7.3.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces An illustration of the how sampling distribution of the mean depends on sample size. In each panel I generated 10,000 samples of IQ data and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line. In panel a, each data set contained only a single observation, so the mean of each sample is just one person's IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2 the mean of any one sample tends to be closer to the population mean than a one person's IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel c), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean.}}{138}{figure.7.9}}
\newlabel{fig:IQsamp}{{7.9}{138}{An illustration of the how sampling distribution of the mean depends on sample size. In each panel I generated 10,000 samples of IQ data and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line. In panel a, each data set contained only a single observation, so the mean of each sample is just one person's IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2 the mean of any one sample tends to be closer to the population mean than a one person's IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel c), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean}{figure.7.9}{}}
\newlabel{sec:pointestimates}{{7.4}{139}{Estimating population parameters~\label {sec:pointestimates}}{section.7.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.4}Estimating population parameters~}{139}{section.7.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution, and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8 for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations. }}{140}{figure.7.10}}
\newlabel{fig:cltdemo}{{7.10}{140}{A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution, and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8 for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations}{figure.7.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Estimating the population mean}{141}{subsection.7.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Estimating the population standard deviation}{142}{subsection.7.4.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces The sampling distribution of the sample standard deviation for a ``two IQ scores'' experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a {\it  biased} estimate of the population standard deviation.}}{144}{figure.7.11}}
\newlabel{fig:sampdistsd}{{7.11}{144}{The sampling distribution of the sample standard deviation for a ``two IQ scores'' experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a {\it biased} estimate of the population standard deviation}{figure.7.11}{}}
\zref@newlabel{mdf@pagelabel-11}{\default{7.4}\page{145}\abspage{160}\mdf@pagevalue{145}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated 10,000 simulated data sets with 1 observation each, 10,000 more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. {\it  On average}, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes.}}{146}{figure.7.12}}
\newlabel{fig:estimatorbias}{{7.12}{146}{An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated 10,000 simulated data sets with 1 observation each, 10,000 more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. {\it On average}, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes}{figure.7.12}{}}
\newlabel{sec:ci}{{7.5}{147}{Estimating a confidence interval\label {sec:ci}}{section.7.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.5}Estimating a confidence interval}{147}{section.7.5}}
\zref@newlabel{mdf@pagelabel-12}{\default{7.5}\page{148}\abspage{163}\mdf@pagevalue{148}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}A slight mistake in the formula}{149}{subsection.7.5.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Interpreting a confidence interval}{149}{subsection.7.5.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Calculating confidence intervals in JASP}{150}{subsection.7.5.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces 95\% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean and the line shows the 95\% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people.}}{151}{figure.7.13}}
\newlabel{fig:cirep}{{7.13}{151}{95\% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean and the line shows the 95\% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people}{figure.7.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7.6}Summary}{152}{section.7.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {8}Hypothesis testing}{153}{chapter.8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:hypothesistesting}{{8}{153}{Hypothesis testing}{chapter.8}{}}
\newlabel{sec:hypotheses}{{8.1}{154}{A menagerie of hypotheses~\label {sec:hypotheses}}{section.8.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.1}A menagerie of hypotheses~}{154}{section.8.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Research hypotheses versus statistical hypotheses}{154}{subsection.8.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Null hypotheses and alternative hypotheses}{157}{subsection.8.1.2}}
\newlabel{sec:errortypes}{{8.2}{157}{Two types of errors~\label {sec:errortypes}}{section.8.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.2}Two types of errors~}{157}{section.8.2}}
\newlabel{sec:teststatistics}{{8.3}{159}{Test statistics and sampling distributions~\label {sec:teststatistics}}{section.8.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.3}Test statistics and sampling distributions~}{159}{section.8.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The sampling distribution for our test statistic $X$ when the null hypothesis is true. For our ESP scenario this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is $\theta = .5$, the sampling distribution says that the most likely value is 50 (out of 100) correct responses. Most of the probability mass lies between 40 and 60.}}{160}{figure.8.1}}
\newlabel{fig:samplingdist}{{8.1}{160}{The sampling distribution for our test statistic $X$ when the null hypothesis is true. For our ESP scenario this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is $\theta = .5$, the sampling distribution says that the most likely value is 50 (out of 100) correct responses. Most of the probability mass lies between 40 and 60}{figure.8.1}{}}
\newlabel{sec:decisionmaking}{{8.4}{161}{Making decisions~\label {sec:decisionmaking}}{section.8.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.4}Making decisions~}{161}{section.8.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Critical regions and critical values}{161}{subsection.8.4.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of $\alpha = .05$. The plot shows the sampling distribution of $X$ under the null hypothesis (i.e., same as Figure~\ref  {fig:samplingdist}). The grey bars correspond to those values of $X$ for which we would retain the null hypothesis. The blue (darker shaded) bars show the critical region, those values of $X$ for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both $\theta <.5$ and $\theta >.5$), the critical region covers both tails of the distribution. To ensure an $\alpha $ level of $.05$, we need to ensure that each of the two regions encompasses 2.5\% of the sampling distribution. }}{162}{figure.8.2}}
\newlabel{fig:crit2}{{8.2}{162}{The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of $\alpha = .05$. The plot shows the sampling distribution of $X$ under the null hypothesis (i.e., same as Figure~\ref {fig:samplingdist}). The grey bars correspond to those values of $X$ for which we would retain the null hypothesis. The blue (darker shaded) bars show the critical region, those values of $X$ for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both $\theta <.5$ and $\theta >.5$), the critical region covers both tails of the distribution. To ensure an $\alpha $ level of $.05$, we need to ensure that each of the two regions encompasses 2.5\% of the sampling distribution}{figure.8.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}A note on statistical ``significance''}{163}{subsection.8.4.2}}
\newlabel{sec:onesidedtests}{{8.4.3}{163}{The difference between one sided and two sided tests\label {sec:onesidedtests}}{subsection.8.4.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}The difference between one sided and two sided tests}{163}{subsection.8.4.3}}
\newlabel{sec:pvalue}{{8.5}{164}{The \texorpdfstring {\boldm {$p$}}{} value of a test~\label {sec:pvalue}}{section.8.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.5}The \mathversion  {bold}$p$\mathversion  {normal} value of a test~}{164}{section.8.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}A softer view of decision making}{164}{subsection.8.5.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces The critical region for a one sided test. In this case, the alternative hypothesis is that $\theta > .5$ so we would only reject the null hypothesis for large values of $X$. As a consequence, the critical region only covers the upper tail of the sampling distribution, specifically the upper 5\% of the distribution. Contrast this to the two-sided version in Figure~\ref  {fig:crit2}. }}{165}{figure.8.3}}
\newlabel{fig:crit1}{{8.3}{165}{The critical region for a one sided test. In this case, the alternative hypothesis is that $\theta > .5$ so we would only reject the null hypothesis for large values of $X$. As a consequence, the critical region only covers the upper tail of the sampling distribution, specifically the upper 5\% of the distribution. Contrast this to the two-sided version in Figure~\ref {fig:crit2}}{figure.8.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}The probability of extreme data}{166}{subsection.8.5.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}A common mistake}{166}{subsection.8.5.3}}
\newlabel{sec:writeup}{{8.6}{167}{Reporting the results of a hypothesis test~\label {sec:writeup}}{section.8.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.6}Reporting the results of a hypothesis test~}{167}{section.8.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.1}The issue}{167}{subsection.8.6.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.6.2}Two proposed solutions}{168}{subsection.8.6.2}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces A commonly adopted convention for reporting $p$ values: in many places it is conventional to report one of four different things (e.g., $p<.05$) as shown below. I've included the ``significance stars'' notation (i.e., a * indicates $p<.05$) because you sometimes see this notation produced by statistical software. It's also worth noting that some people will write {\it  n.s.} (not significant) rather than $p>.05$.}}{169}{table.8.1}}
\newlabel{tab:pvaltable}{{8.1}{169}{A commonly adopted convention for reporting $p$ values: in many places it is conventional to report one of four different things (e.g., $p<.05$) as shown below. I've included the ``significance stars'' notation (i.e., a * indicates $p<.05$) because you sometimes see this notation produced by statistical software. It's also worth noting that some people will write {\it n.s.} (not significant) rather than $p>.05$}{table.8.1}{}}
\newlabel{sec:runhyp}{{8.7}{169}{Running the hypothesis test in practice~\label {sec:runhyp}}{section.8.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.7}Running the hypothesis test in practice~}{169}{section.8.7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Binomial test analysis and results in JASP}}{170}{figure.8.4}}
\newlabel{fig:binomialtest}{{8.4}{170}{Binomial test analysis and results in JASP}{figure.8.4}{}}
\newlabel{sec:effectsize}{{8.8}{170}{Effect size, sample size and power \label {sec:effectsize}}{section.8.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.8}Effect size, sample size and power }{170}{section.8.8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.1}The power function}{170}{subsection.8.8.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Sampling distribution under the {\it  alternative} hypothesis for a population parameter value of $\theta = 0.55$. A reasonable proportion of the distribution lies in the rejection region.}}{171}{figure.8.5}}
\newlabel{fig:crit3}{{8.5}{171}{Sampling distribution under the {\it alternative} hypothesis for a population parameter value of $\theta = 0.55$. A reasonable proportion of the distribution lies in the rejection region}{figure.8.5}{}}
\abx@aux@cite{Box1976}
\abx@aux@segm{0}{0}{Box1976}
\abx@aux@cite{Cohen1988}
\abx@aux@segm{0}{0}{Cohen1988}
\abx@aux@cite{Ellis2010}
\abx@aux@segm{0}{0}{Ellis2010}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Sampling distribution under the {\it  alternative} hypothesis for a population parameter value of $\theta = 0.70$. Almost all of the distribution lies in the rejection region.}}{172}{figure.8.6}}
\newlabel{fig:crit4}{{8.6}{172}{Sampling distribution under the {\it alternative} hypothesis for a population parameter value of $\theta = 0.70$. Almost all of the distribution lies in the rejection region}{figure.8.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.2}Effect size}{172}{subsection.8.8.2}}
\abx@aux@backref{24}{Box1976}{0}{172}{172}
\abx@aux@page{24}{172}
\abx@aux@backref{25}{Cohen1988}{0}{172}{172}
\abx@aux@page{25}{172}
\abx@aux@backref{26}{Ellis2010}{0}{172}{172}
\abx@aux@page{26}{172}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces The probability that we will reject the null hypothesis, plotted as a function of the true value of $\theta $. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of $\theta $ is very different from the value that the null hypothesis specifies (i.e., $\theta =.5$). Notice that when $\theta $ actually is equal to .5 (plotted as a black dot), the null hypothesis is in fact true and rejecting the null hypothesis in this instance would be a Type I error.}}{173}{figure.8.7}}
\newlabel{fig:powerfunction}{{8.7}{173}{The probability that we will reject the null hypothesis, plotted as a function of the true value of $\theta $. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of $\theta $ is very different from the value that the null hypothesis specifies (i.e., $\theta =.5$). Notice that when $\theta $ actually is equal to .5 (plotted as a black dot), the null hypothesis is in fact true and rejecting the null hypothesis in this instance would be a Type I error}{figure.8.7}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces A crude guide to understanding the relationship between statistical significance and effect sizes. Basically, if you don't have a significant result then the effect size is pretty meaningless because you don't have any evidence that it's even real. On the other hand, if you do have a significant effect but your effect size is small then there's a pretty good chance that your result (although real) isn't all that interesting. However, this guide is very crude. It depends a lot on what exactly you're studying. Small effects can be of massive practical importance in some situations. So don't take this table too seriously. It's a rough guide at best.}}{174}{table.8.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.3}Increasing the power of your study}{174}{subsection.8.8.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces The power of our test plotted as a function of the sample size $N$. In this case, the true value of $\theta $ is 0.7 but the null hypothesis is that $\theta = 0.5$. Overall, larger $N$ means greater power. (The small zig-zags in this function occur because of some odd interactions between $\theta $, $\alpha $ and the fact that the binomial distribution is discrete, it doesn't matter for any serious purpose).}}{176}{figure.8.8}}
\newlabel{fig:powerfunctionsample}{{8.8}{176}{The power of our test plotted as a function of the sample size $N$. In this case, the true value of $\theta $ is 0.7 but the null hypothesis is that $\theta = 0.5$. Overall, larger $N$ means greater power. (The small zig-zags in this function occur because of some odd interactions between $\theta $, $\alpha $ and the fact that the binomial distribution is discrete, it doesn't matter for any serious purpose)}{figure.8.8}{}}
\abx@aux@cite{Lehmann2011}
\abx@aux@segm{0}{0}{Lehmann2011}
\newlabel{sec:nhstmess}{{8.9}{177}{Some issues to consider~\label {sec:nhstmess}}{section.8.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.9}Some issues to consider~}{177}{section.8.9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.1}Neyman versus Fisher}{177}{subsection.8.9.1}}
\abx@aux@backref{27}{Lehmann2011}{0}{177}{177}
\abx@aux@page{27}{177}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.2}Bayesians versus frequentists}{178}{subsection.8.9.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.3}Traps}{178}{subsection.8.9.3}}
\abx@aux@cite{Gelman2006}
\abx@aux@segm{0}{0}{Gelman2006}
\abx@aux@backref{28}{Gelman2006}{0}{179}{179}
\abx@aux@page{28}{179}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8.10}Summary}{179}{section.8.10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Statistical tools}{181}{part.4}}
\abx@aux@cite{Pearson1900}
\abx@aux@segm{0}{0}{Pearson1900}
\abx@aux@cite{Fisher1922}
\abx@aux@segm{0}{0}{Fisher1922}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {9}Categorical data analysis}{183}{chapter.9}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:chisquare}{{9}{183}{Categorical data analysis}{chapter.9}{}}
\newlabel{sec:goftest}{{9.1}{183}{The \texorpdfstring {\boldm {$\chi ^2$}}{} (chi-square) goodness-of-fit test~\label {sec:goftest}}{section.9.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.1}The \mathversion  {bold}$\chi ^2$\mathversion  {normal} (chi-square) goodness-of-fit test~}{183}{section.9.1}}
\abx@aux@backref{29}{Pearson1900}{0}{183}{183}
\abx@aux@page{29}{183}
\abx@aux@backref{30}{Fisher1922}{0}{183}{183}
\abx@aux@page{30}{183}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}The cards data}{184}{subsection.9.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}The null hypothesis and the alternative hypothesis}{185}{subsection.9.1.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}The ``goodness-of-fit'' test statistic}{186}{subsection.9.1.3}}
\zref@newlabel{mdf@pagelabel-13}{\default{9.1}\page{187}\abspage{202}\mdf@pagevalue{187}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.4}The sampling distribution of the GOF statistic }{188}{subsection.9.1.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.5}Degrees of freedom}{189}{subsection.9.1.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces $\chi ^2$ (chi-square) distributions with different values for the ``degrees of freedom''.}}{189}{figure.9.1}}
\newlabel{fig:manychi}{{9.1}{189}{$\chi ^2$ (chi-square) distributions with different values for the ``degrees of freedom''}{figure.9.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.6}Testing the null hypothesis}{190}{subsection.9.1.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Illustration of how the hypothesis testing works for the $\chi ^2$ (chi-square) goodness-of-fit test.}}{191}{figure.9.2}}
\newlabel{fig:goftest}{{9.2}{191}{Illustration of how the hypothesis testing works for the $\chi ^2$ (chi-square) goodness-of-fit test}{figure.9.2}{}}
\newlabel{sec:gofTestInJASP}{{9.1.7}{191}{Doing the test in JASP\label {sec:gofTestInJASP}}{subsection.9.1.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.7}Doing the test in JASP}{191}{subsection.9.1.7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Table of critical values for the chi-square distribution}}{192}{figure.9.3}}
\newlabel{fig:chisquare.critvalues}{{9.3}{192}{Table of critical values for the chi-square distribution}{figure.9.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.8}Specifying a different null hypothesis}{192}{subsection.9.1.8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces A $\chi ^2$ goodness-of-fit test in JASP, with table showing both observed and expected frequencies.}}{193}{figure.9.4}}
\newlabel{fig:chisquare.analysis1}{{9.4}{193}{A $\chi ^2$ goodness-of-fit test in JASP, with table showing both observed and expected frequencies}{figure.9.4}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Changing the expected proportions in the $\chi ^2$ goodness-of-fit test in JASP}}{194}{figure.9.5}}
\newlabel{fig:chisquare.analysis2}{{9.5}{194}{Changing the expected proportions in the \texorpdfstring {$\chi ^2$}{} goodness-of-fit test in JASP}{figure.9.5}{}}
\newlabel{sec:chisqreport}{{9.1.9}{194}{How to report the results of the test~\label {sec:chisqreport}}{subsection.9.1.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.9}How to report the results of the test~}{194}{subsection.9.1.9}}
\abx@aux@cite{Sokal1994}
\abx@aux@segm{0}{0}{Sokal1994}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.10}A comment on statistical notation }{196}{subsection.9.1.10}}
\abx@aux@backref{31}{Sokal1994}{0}{196}{196}
\abx@aux@page{31}{196}
\newlabel{sec:chisqindependence}{{9.2}{198}{The \texorpdfstring {\boldm {$\chi ^2$}}{} test of independence (or association)~\label {sec:chisqindependence}}{section.9.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.2}The \mathversion  {bold}$\chi ^2$\mathversion  {normal} test of independence (or association)~}{198}{section.9.2}}
\abx@aux@cite{Hogg2005}
\abx@aux@segm{0}{0}{Hogg2005}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Constructing our hypothesis test}{199}{subsection.9.2.1}}
\abx@aux@backref{32}{Hogg2005}{0}{199}{199}
\abx@aux@fnpage{32}{199}
\zref@newlabel{mdf@pagelabel-14}{\default{9.2}\page{200}\abspage{215}\mdf@pagevalue{200}}
\newlabel{sec:AssocTestInJASP}{{9.2.2}{201}{Doing the test in JASP\label {sec:AssocTestInJASP}}{subsection.9.2.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Doing the test in JASP}{201}{subsection.9.2.2}}
\abx@aux@cite{Yates1934}
\abx@aux@segm{0}{0}{Yates1934}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Postscript}{203}{subsection.9.2.3}}
\newlabel{sec:yates}{{9.3}{203}{The continuity correction~\label {sec:yates}}{section.9.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.3}The continuity correction~}{203}{section.9.3}}
\zref@newlabel{mdf@pagelabel-15}{\default{9.3}\page{203}\abspage{218}\mdf@pagevalue{203}}
\abx@aux@backref{33}{Yates1934}{0}{203}{203}
\abx@aux@page{33}{203}
\newlabel{sec:chisqeffectsize}{{9.4}{203}{Effect size~\label {sec:chisqeffectsize}}{section.9.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.4}Effect size~}{203}{section.9.4}}
\abx@aux@cite{Cramer1946}
\abx@aux@segm{0}{0}{Cramer1946}
\abx@aux@cite{Cochran1954}
\abx@aux@segm{0}{0}{Cochran1954}
\abx@aux@cite{Larntz1978}
\abx@aux@segm{0}{0}{Larntz1978}
\zref@newlabel{mdf@pagelabel-16}{\default{9.4}\page{204}\abspage{219}\mdf@pagevalue{204}}
\abx@aux@backref{34}{Cramer1946}{0}{204}{204}
\abx@aux@page{34}{204}
\newlabel{sec:chisqassumptions}{{9.5}{204}{Assumptions of the test(s)~\label {sec:chisqassumptions}}{section.9.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.5}Assumptions of the test(s)~}{204}{section.9.5}}
\abx@aux@backref{35}{Cochran1954}{0}{205}{205}
\abx@aux@page{35}{205}
\abx@aux@backref{36}{Larntz1978}{0}{205}{205}
\abx@aux@page{36}{205}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9.6}Summary}{205}{section.9.6}}
\abx@aux@cite{Agresti1996}
\abx@aux@segm{0}{0}{Agresti1996}
\abx@aux@cite{Agresti2002}
\abx@aux@segm{0}{0}{Agresti2002}
\abx@aux@backref{37}{Agresti1996}{0}{206}{206}
\abx@aux@page{37}{206}
\abx@aux@backref{38}{Agresti2002}{0}{206}{206}
\abx@aux@page{38}{206}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {10}Comparing two means }{207}{chapter.10}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:ttest}{{10}{207}{Comparing two means}{chapter.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.1}The one-sample \mathversion  {bold}$z$\mathversion  {normal}-test}{208}{section.10.1}}
\newlabel{sec:onesampleztest}{{10.1}{208}{The one-sample \texorpdfstring {\boldm {$z$}}{}-test}{section.10.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}The inference problem that the test addresses}{208}{subsection.10.1.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces The theoretical distribution (solid line) from which the psychology student grades (bars) are supposed to have been generated.}}{209}{figure.10.1}}
\newlabel{fig:zeppo}{{10.1}{209}{The theoretical distribution (solid line) from which the psychology student grades (bars) are supposed to have been generated}{figure.10.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Constructing the hypothesis test}{209}{subsection.10.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the one sample $z$-test (the two sided version, that is). The null and alternative hypotheses both assume that the population distribution is normal, and additionally assumes that the population standard deviation is known (fixed at some value $\sigma _0$). The null hypothesis (left) is that the population mean $\mu $ is equal to some specified value $\mu _0$. The alternative hypothesis is that the population mean differs from this value, $\mu \neq \mu _0$.}}{210}{figure.10.2}}
\newlabel{fig:ztesthyp}{{10.2}{210}{Graphical illustration of the null and alternative hypotheses assumed by the one sample $z$-test (the two sided version, that is). The null and alternative hypotheses both assume that the population distribution is normal, and additionally assumes that the population standard deviation is known (fixed at some value $\sigma _0$). The null hypothesis (left) is that the population mean $\mu $ is equal to some specified value $\mu _0$. The alternative hypothesis is that the population mean differs from this value, $\mu \neq \mu _0$}{figure.10.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Rejection regions for the two-sided $z$-test (panel a) and the one-sided $z$-test (panel b).}}{212}{figure.10.3}}
\newlabel{fig:ztest}{{10.3}{212}{Rejection regions for the two-sided $z$-test (panel a) and the one-sided $z$-test (panel b)}{figure.10.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}A worked example, by hand}{212}{subsection.10.1.3}}
\newlabel{sec:zassumptions}{{10.1.4}{213}{Assumptions of the \texorpdfstring {$z$}{z}-test~\label {sec:zassumptions}}{subsection.10.1.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}Assumptions of the $z$-test~}{213}{subsection.10.1.4}}
\newlabel{sec:onesamplettest}{{10.2}{214}{The one-sample \texorpdfstring {\boldm {$t$}}{}-test~\label {sec:onesamplettest}}{section.10.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.2}The one-sample \mathversion  {bold}$t$\mathversion  {normal}-test~}{214}{section.10.2}}
\abx@aux@cite{Student1908}
\abx@aux@segm{0}{0}{Student1908}
\abx@aux@cite{Box1987}
\abx@aux@segm{0}{0}{Box1987}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the (two sided) one sample $t$-test. Note the similarity to the $z$-test (Figure~\ref  {fig:ztesthyp}). The null hypothesis is that the population mean $\mu $ is equal to some specified value $\mu _0$, and the alternative hypothesis is that it is not. Like the $z$-test, we assume that the data are normally distributed, but we do not assume that the population standard deviation $\sigma $ is known in advance.}}{215}{figure.10.4}}
\newlabel{fig:ttesthyp_onesample}{{10.4}{215}{Graphical illustration of the null and alternative hypotheses assumed by the (two sided) one sample \texorpdfstring {$t$}{}-test. Note the similarity to the \texorpdfstring {$z$}{}-test (Figure~\ref {fig:ztesthyp}). The null hypothesis is that the population mean \texorpdfstring {$\mu $}{} is equal to some specified value \texorpdfstring {$\mu _0$}{}, and the alternative hypothesis is that it is not. Like the \texorpdfstring {$z$}{}-test, we assume that the data are normally distributed, but we do not assume that the population standard deviation \texorpdfstring {$\sigma $}{} is known in advance}{figure.10.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Introducing the $t$-test}{215}{subsection.10.2.1}}
\abx@aux@backref{39}{Student1908}{0}{215}{215}
\abx@aux@page{39}{215}
\abx@aux@backref{40}{Box1987}{0}{215}{215}
\abx@aux@page{40}{215}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces The $t$ distribution with 2 degrees of freedom (left) and 10 degrees of freedom (right), with a standard normal distribution (i.e., mean 0 and std dev 1) plotted as dotted lines for comparison purposes. Notice that the $t$ distribution has heavier tails (leptokurtic: higher kurtosis) than the normal distribution; this effect is quite exaggerated when the degrees of freedom are very small, but negligible for larger values. In other words, for large $df$ the $t$ distribution is essentially identical to a normal distribution.}}{216}{figure.10.5}}
\newlabel{fig:ttestdist}{{10.5}{216}{The $t$ distribution with 2 degrees of freedom (left) and 10 degrees of freedom (right), with a standard normal distribution (i.e., mean 0 and std dev 1) plotted as dotted lines for comparison purposes. Notice that the $t$ distribution has heavier tails (leptokurtic: higher kurtosis) than the normal distribution; this effect is quite exaggerated when the degrees of freedom are very small, but negligible for larger values. In other words, for large $df$ the $t$ distribution is essentially identical to a normal distribution}{figure.10.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Doing the test in JASP}{216}{subsection.10.2.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces JASP does the one-sample t-test.}}{217}{figure.10.6}}
\newlabel{fig:ttest_one}{{10.6}{217}{JASP does the one-sample t-test}{figure.10.6}{}}
\newlabel{sec:ttestoneassumptions}{{10.2.3}{218}{Assumptions of the one sample \texorpdfstring {\boldm {$t$}}{}-test~\label {sec:ttestoneassumptions}}{subsection.10.2.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Assumptions of the one sample \mathversion  {bold}$t$\mathversion  {normal}-test~}{218}{subsection.10.2.3}}
\newlabel{sec:studentttest}{{10.3}{219}{The independent samples \texorpdfstring {\boldm {$t$}}{}-test (Student test)~\label {sec:studentttest}}{section.10.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.3}The independent samples \mathversion  {bold}$t$\mathversion  {normal}-test (Student test)~}{219}{section.10.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}The data}{219}{subsection.10.3.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Introducing the test}{219}{subsection.10.3.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Histograms showing the distribution of grades for students in Anastasia's (panel a) and in Bernadette's (panel b) classes. Visually, these suggest that students in Anastasia's class may be getting slightly better grades on average, though they also seem a bit more variable.}}{220}{figure.10.7}}
\newlabel{fig:harpohist}{{10.7}{220}{Histograms showing the distribution of grades for students in Anastasia's (panel a) and in Bernadette's (panel b) classes. Visually, these suggest that students in Anastasia's class may be getting slightly better grades on average, though they also seem a bit more variable}{figure.10.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces The plots show the mean grade for students in Anastasia\IeC {\textquoteright }s and Bernadette\IeC {\textquoteright }s tutorials. Error bars depict 95\% confidence intervals around the mean. Visually, it does look like there's a real difference between the groups, though it's hard to say for sure.}}{221}{figure.10.8}}
\newlabel{fig:ttestci}{{10.8}{221}{The plots show the mean grade for students in Anastasias and Bernadettes tutorials. Error bars depict 95\% confidence intervals around the mean. Visually, it does look like there's a real difference between the groups, though it's hard to say for sure}{figure.10.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the Student $t$-test. The null hypothesis assumes that both groups have the same mean $\mu $, whereas the alternative assumes that they have different means $\mu _1$ and $\mu _2$. Notice that it is assumed that the population distributions are normal, and that, although the alternative hypothesis allows the group to have different means, it assumes they have the same standard deviation.}}{223}{figure.10.9}}
\newlabel{fig:ttesthyp}{{10.9}{223}{Graphical illustration of the null and alternative hypotheses assumed by the Student $t$-test. The null hypothesis assumes that both groups have the same mean $\mu $, whereas the alternative assumes that they have different means $\mu _1$ and $\mu _2$. Notice that it is assumed that the population distributions are normal, and that, although the alternative hypothesis allows the group to have different means, it assumes they have the same standard deviation}{figure.10.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}A ``pooled estimate'' of the standard deviation}{223}{subsection.10.3.3}}
\zref@newlabel{mdf@pagelabel-17}{\default{10.3}\page{224}\abspage{239}\mdf@pagevalue{224}}
\zref@newlabel{mdf@pagelabel-18}{\default{10.3}\page{225}\abspage{240}\mdf@pagevalue{225}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.4}Completing the test}{225}{subsection.10.3.4}}
\zref@newlabel{mdf@pagelabel-19}{\default{10.3}\page{225}\abspage{240}\mdf@pagevalue{225}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.5}Doing the test in JASP}{226}{subsection.10.3.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces Independent $t$-test in JASP, with options checked for useful results}}{226}{figure.10.10}}
\newlabel{fig:ttest_ind}{{10.10}{226}{Independent $t$-test in JASP, with options checked for useful results}{figure.10.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.6}Positive and negative $t$ values}{227}{subsection.10.3.6}}
\newlabel{sec:studentassumptions}{{10.3.7}{228}{Assumptions of the test~\label {sec:studentassumptions}}{subsection.10.3.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.7}Assumptions of the test~}{228}{subsection.10.3.7}}
\abx@aux@cite{Welch1947}
\abx@aux@segm{0}{0}{Welch1947}
\newlabel{sec:welchttest}{{10.4}{229}{The independent samples \texorpdfstring {\boldm {$t$}}{}-test (Welch test)~\label {sec:welchttest}}{section.10.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.4}The independent samples \mathversion  {bold}$t$\mathversion  {normal}-test (Welch test)~}{229}{section.10.4}}
\abx@aux@backref{41}{Welch1947}{0}{229}{229}
\abx@aux@page{41}{229}
\zref@newlabel{mdf@pagelabel-20}{\default{10.4}\page{229}\abspage{244}\mdf@pagevalue{229}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the Welch $t$-test. Like the Student test (Figure~\ref  {fig:ttesthyp}) we assume that both samples are drawn from a normal population; but the alternative hypothesis no longer requires the two populations to have equal variance.}}{230}{figure.10.11}}
\newlabel{fig:ttesthyp2}{{10.11}{230}{Graphical illustration of the null and alternative hypotheses assumed by the Welch $t$-test. Like the Student test (Figure~\ref {fig:ttesthyp}) we assume that both samples are drawn from a normal population; but the alternative hypothesis no longer requires the two populations to have equal variance}{figure.10.11}{}}
\zref@newlabel{mdf@pagelabel-21}{\default{10.4}\page{230}\abspage{245}\mdf@pagevalue{230}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.1}Doing the Welch test in JASP}{231}{subsection.10.4.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.12}{\ignorespaces Results showing the Welch test alongside the default Student's t-test in JASP}}{231}{figure.10.12}}
\newlabel{fig:ttest_welch}{{10.12}{231}{Results showing the Welch test alongside the default Student's t-test in JASP}{figure.10.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.2}Assumptions of the test}{231}{subsection.10.4.2}}
\newlabel{sec:pairedsamplesttest}{{10.5}{232}{The paired-samples \texorpdfstring {\boldm {$t$}}{}-test~\label {sec:pairedsamplesttest}}{section.10.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.5}The paired-samples \mathversion  {bold}$t$\mathversion  {normal}-test~}{232}{section.10.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}The data}{232}{subsection.10.5.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.13}{\ignorespaces Descriptives for the two grade\_test variables in the \texttt  {chico} data set}}{233}{figure.10.13}}
\newlabel{fig:ttest_paired1}{{10.13}{233}{Descriptives for the two grade\_test variables in the \texttt {chico} data set}{figure.10.13}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.14}{\ignorespaces Mean grade for test 1 and test 2, with associated 95\% confidence intervals (panel a). Scatterplot showing the individual grades for test 1 and test 2 (panel b). Histogram showing the improvement made by each student in Dr Chico's class (panel c). In panel c, notice that almost the entire distribution is above zero: the vast majority of students did improve their performance from the first test to the second one}}{234}{figure.10.14}}
\newlabel{fig:pairedt}{{10.14}{234}{Mean grade for test 1 and test 2, with associated 95\% confidence intervals (panel a). Scatterplot showing the individual grades for test 1 and test 2 (panel b). Histogram showing the improvement made by each student in Dr Chico's class (panel c). In panel c, notice that almost the entire distribution is above zero: the vast majority of students did improve their performance from the first test to the second one}{figure.10.14}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.2}What is the paired samples $t$-test?}{234}{subsection.10.5.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.15}{\ignorespaces Using R code to compute an \texttt  {improvement} score in JASP.}}{235}{figure.10.15}}
\newlabel{fig:improvement}{{10.15}{235}{Using R code to compute an \texttt {improvement} score in JASP}{figure.10.15}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.3}Doing the test in JASP}{236}{subsection.10.5.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.16}{\ignorespaces Results showing a one sample $t$-test on paired difference scores}}{236}{figure.10.16}}
\newlabel{fig:ttest_paired2}{{10.16}{236}{Results showing a one sample $t$-test on paired difference scores}{figure.10.16}{}}
\newlabel{sec:onesidedttest}{{10.6}{237}{One sided tests~\label {sec:onesidedttest}}{section.10.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.6}One sided tests~}{237}{section.10.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.17}{\ignorespaces JASP results showing a `One Sample T-Test' where the actual hypothesis is one sided, i.e. that the true mean is greater than 67.5\%}}{237}{figure.10.17}}
\newlabel{fig:ttest_onesided1}{{10.17}{237}{JASP results showing a `One Sample T-Test' where the actual hypothesis is one sided, i.e. that the true mean is greater than 67.5\%}{figure.10.17}{}}
\abx@aux@segm{0}{0}{Cohen1988}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.18}{\ignorespaces JASP results showing an `Independent Samples T-Test' where the actual hypothesis is one sided, i.e. that Anastasia's students had higher grades than Bernadette's}}{238}{figure.10.18}}
\newlabel{fig:ttest_onesided2}{{10.18}{238}{JASP results showing an `Independent Samples T-Test' where the actual hypothesis is one sided, i.e. that Anastasia's students had higher grades than Bernadette's}{figure.10.18}{}}
\newlabel{sec:cohensd}{{10.7}{238}{Effect size~\label {sec:cohensd}}{section.10.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.7}Effect size~}{238}{section.10.7}}
\abx@aux@backref{42}{Cohen1988}{0}{238}{238}
\abx@aux@page{42}{238}
\abx@aux@cite{McGrath2006}
\abx@aux@segm{0}{0}{McGrath2006}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.19}{\ignorespaces JASP results showing a `Paired Samples T-Test' where the actual hypothesis is one sided, i.e. that \texttt  {grade\_test2} (`Measure 1') $>$ \texttt  {grade\_test1} (`Measure 2')}}{239}{figure.10.19}}
\newlabel{fig:ttest_onesided3}{{10.19}{239}{JASP results showing a `Paired Samples T-Test' where the actual hypothesis is one sided, i.e. that \texttt {grade\_test2} (`Measure 1') $>$ \texttt {grade\_test1} (`Measure 2')}{figure.10.19}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces A (very) rough guide to interpreting Cohen's $d$. My personal recommendation is to not use these blindly. The $d$ statistic has a natural interpretation in and of itself. It re-describes the difference in means as the number of standard deviations that separates those means. So it's generally a good idea to think about what that means in practical terms. In some contexts a ``small'' effect could be of big practical importance. In other situations a ``large'' effect may not be all that interesting.}}{239}{table.10.1}}
\newlabel{tab:cohensdinterpretation}{{10.1}{239}{A (very) rough guide to interpreting Cohen's $d$. My personal recommendation is to not use these blindly. The $d$ statistic has a natural interpretation in and of itself. It re-describes the difference in means as the number of standard deviations that separates those means. So it's generally a good idea to think about what that means in practical terms. In some contexts a ``small'' effect could be of big practical importance. In other situations a ``large'' effect may not be all that interesting}{table.10.1}{}}
\abx@aux@backref{43}{McGrath2006}{0}{239}{239}
\abx@aux@page{43}{239}
\abx@aux@cite{Hedges1981}
\abx@aux@segm{0}{0}{Hedges1981}
\abx@aux@cite{Hedges1985}
\abx@aux@segm{0}{0}{Hedges1985}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.1}Cohen's $d$ from one sample}{240}{subsection.10.7.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.2}Cohen's $d$ from a Student's $t$ test}{240}{subsection.10.7.2}}
\abx@aux@backref{44}{Hedges1981}{0}{240}{240}
\abx@aux@page{44}{240}
\abx@aux@backref{45}{Hedges1985}{0}{241}{241}
\abx@aux@page{45}{241}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.7.3}Cohen's $d$ from a paired-samples test}{241}{subsection.10.7.3}}
\newlabel{sec:shapiro}{{10.8}{241}{Checking the normality of a sample\label {sec:shapiro}}{section.10.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.8}Checking the normality of a sample}{241}{section.10.8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.8.1}QQ plots}{242}{subsection.10.8.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.20}{\ignorespaces Histogram (panel a) and normal QQ plot (panel b) of \texttt  {normal.data}, a normally distributed sample with 100 observations. The Shapiro-Wilk statistic associated with these data is $W = .99$, indicating that no significant departures from normality were detected ($p = .73$).}}{242}{figure.10.20}}
\newlabel{fig:qq1}{{10.20}{242}{Histogram (panel a) and normal QQ plot (panel b) of \texttt {normal.data}, a normally distributed sample with 100 observations. The Shapiro-Wilk statistic associated with these data is $W = .99$, indicating that no significant departures from normality were detected ($p = .73$)}{figure.10.20}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.21}{\ignorespaces In the top row, a histogram (panel a) and normal QQ plot (panel b) of the 100 observations in a \texttt  {skewed.data} set. The skewness of the data here is 1.94, and is reflected in a QQ plot that curves upwards. As a consequence, the Shapiro-Wilk statistic is $W=.80$, reflecting a significant departure from normality ($p<.001$). The bottom row shows the same plots for a heavy tailed data set, again consisting of 100 observations. In this case the heavy tails in the data produce a high kurtosis (2.80), and cause the QQ plot to flatten in the middle, and curve away sharply on either side. The resulting Shapiro-Wilk statistic is $W = .93$, again reflecting significant non-normality ($p < .001$).}}{243}{figure.10.21}}
\newlabel{fig:qq2}{{10.21}{243}{In the top row, a histogram (panel a) and normal QQ plot (panel b) of the 100 observations in a \texttt {skewed.data} set. The skewness of the data here is 1.94, and is reflected in a QQ plot that curves upwards. As a consequence, the Shapiro-Wilk statistic is $W=.80$, reflecting a significant departure from normality ($p<.001$). The bottom row shows the same plots for a heavy tailed data set, again consisting of 100 observations. In this case the heavy tails in the data produce a high kurtosis (2.80), and cause the QQ plot to flatten in the middle, and curve away sharply on either side. The resulting Shapiro-Wilk statistic is $W = .93$, again reflecting significant non-normality ($p < .001$)}{figure.10.21}{}}
\abx@aux@cite{Shapiro1965}
\abx@aux@segm{0}{0}{Shapiro1965}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.8.2}Shapiro-Wilk tests}{244}{subsection.10.8.2}}
\abx@aux@backref{46}{Shapiro1965}{0}{244}{244}
\abx@aux@page{46}{244}
\zref@newlabel{mdf@pagelabel-22}{\default{10.8}\page{244}\abspage{259}\mdf@pagevalue{244}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.22}{\ignorespaces Sampling distribution of the Shapiro-Wilk $W$ statistic, under the null hypothesis that the data are normally distributed, for samples of size 10, 20 and 50. Note that {\it  small} values of $W$ indicate departure from normality.}}{245}{figure.10.22}}
\newlabel{fig:swdist}{{10.22}{245}{Sampling distribution of the Shapiro-Wilk $W$ statistic, under the null hypothesis that the data are normally distributed, for samples of size 10, 20 and 50. Note that {\it small} values of $W$ indicate departure from normality}{figure.10.22}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.8.3}Example}{246}{subsection.10.8.3}}
\newlabel{sec:wilcox}{{10.9}{246}{Testing non-normal data with Wilcoxon tests\label {sec:wilcox}}{section.10.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.9}Testing non-normal data with Wilcoxon tests}{246}{section.10.9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.9.1}Two sample Mann-Whitney U test}{247}{subsection.10.9.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {10.9.2}One sample Wilcoxon test}{248}{subsection.10.9.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {10.10}Summary}{248}{section.10.10}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10.23}{\ignorespaces JASP screen showing results for one sample and paired sample Wilcoxon non-parametric tests}}{249}{figure.10.23}}
\newlabel{fig:ttest_nonparametric}{{10.23}{249}{JASP screen showing results for one sample and paired sample Wilcoxon non-parametric tests}{figure.10.23}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {11}Correlation and linear regression}{251}{chapter.11}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:regression}{{11}{251}{Correlation and linear regression}{chapter.11}{}}
\newlabel{sec:correl}{{11.1}{251}{Correlations\label {sec:correl}}{section.11.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.1}Correlations}{251}{section.11.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.1}The data}{251}{subsection.11.1.1}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {11.1}{\ignorespaces Descriptive statistics for the parenthood data.}}{251}{table.11.1}}
\newlabel{tab:parenthood}{{11.1}{251}{Descriptive statistics for the parenthood data}{table.11.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Histograms for the three interesting variables in the \texttt  {parenthood} data set.}}{252}{figure.11.1}}
\newlabel{fig:parenthood}{{11.1}{252}{Histograms for the three interesting variables in the \texttt {parenthood} data set}{figure.11.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.2}The strength and direction of a relationship}{252}{subsection.11.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces Scatterplots showing the relationship between \texttt  {dan.sleep} and \texttt  {dan.grump} (left) and the relationship between \texttt  {baby.sleep} and \texttt  {dan.grump} (right).}}{253}{figure.11.2}}
\newlabel{fig:scatterparent}{{11.2}{253}{Scatterplots showing the relationship between \texttt {dan.sleep} and \texttt {dan.grump} (left) and the relationship between \texttt {baby.sleep} and \texttt {dan.grump} (right)}{figure.11.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.3}The correlation coefficient}{253}{subsection.11.1.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Scatterplots showing the relationship between \texttt  {baby.sleep} and \texttt  {dan.grump} (left), as compared to the relationship between \texttt  {baby.sleep} and \texttt  {dan.sleep} (right).}}{254}{figure.11.3}}
\newlabel{fig:scatterparent2}{{11.3}{254}{Scatterplots showing the relationship between \texttt {baby.sleep} and \texttt {dan.grump} (left), as compared to the relationship between \texttt {baby.sleep} and \texttt {dan.sleep} (right)}{figure.11.3}{}}
\zref@newlabel{mdf@pagelabel-23}{\default{11.1}\page{254}\abspage{269}\mdf@pagevalue{254}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Illustration of the effect of varying the strength and direction of a correlation. In the left hand column, the correlations are 0, .33, .66 and 1. In the right hand column, the correlations are 0, -.33, -.66 and -1.}}{255}{figure.11.4}}
\newlabel{fig:corr}{{11.4}{255}{Illustration of the effect of varying the strength and direction of a correlation. In the left hand column, the correlations are 0, .33, .66 and 1. In the right hand column, the correlations are 0, -.33, -.66 and -1}{figure.11.4}{}}
\zref@newlabel{mdf@pagelabel-24}{\default{11.1}\page{256}\abspage{271}\mdf@pagevalue{256}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.4}Calculating correlations in JASP}{256}{subsection.11.1.4}}
\newlabel{sec:interpretingcorrelations}{{11.1.5}{256}{Interpreting a correlation~\label {sec:interpretingcorrelations}}{subsection.11.1.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.5}Interpreting a correlation~}{256}{subsection.11.1.5}}
\abx@aux@cite{Anscombe1973}
\abx@aux@segm{0}{0}{Anscombe1973}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces A JASP screenshot showing correlations between variables in the \texttt  {parenthood.csv} file}}{257}{figure.11.5}}
\newlabel{fig:correlations}{{11.5}{257}{A JASP screenshot showing correlations between variables in the \filename {parenthood.csv} file}{figure.11.5}{}}
\abx@aux@backref{47}{Anscombe1973}{0}{257}{257}
\abx@aux@page{47}{257}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {11.2}{\ignorespaces A rough guide to interpreting correlations. Note that I say a {\it  rough} guide. There aren't hard and fast rules for what counts as strong or weak relationships. It depends on the context.}}{258}{table.11.2}}
\newlabel{tab:interpretingcorrelations}{{11.2}{258}{A rough guide to interpreting correlations. Note that I say a {\it rough} guide. There aren't hard and fast rules for what counts as strong or weak relationships. It depends on the context}{table.11.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.6}Spearman's rank correlations}{258}{subsection.11.1.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces Anscombe's quartet. All four of these data sets have a Pearson correlation of $r = .816$, but they are qualitatively different from one another.}}{259}{figure.11.6}}
\newlabel{fig:anscombe}{{11.6}{259}{Anscombe's quartet. All four of these data sets have a Pearson correlation of $r = .816$, but they are qualitatively different from one another}{figure.11.6}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces The relationship between hours worked and grade received for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of $r = .91$. However, the interesting thing to note here is that there's actually a perfect monotonic relationship between the two variables. In this toy example, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of $\rho = 1$. With such a small data set, however, it's an open question as to which version better describes the actual relationship involved. }}{260}{figure.11.7}}
\newlabel{fig:rankcorrpic}{{11.7}{260}{The relationship between hours worked and grade received for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of $r = .91$. However, the interesting thing to note here is that there's actually a perfect monotonic relationship between the two variables. In this toy example, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of $\rho = 1$. With such a small data set, however, it's an open question as to which version better describes the actual relationship involved}{figure.11.7}{}}
\newlabel{sec:scatterplots}{{11.2}{261}{Scatterplots\label {sec:scatterplots}}{section.11.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.2}Scatterplots}{261}{section.11.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.8}{\ignorespaces Scatterplot via the `Correlation Matrix' method in JASP}}{262}{figure.11.8}}
\newlabel{fig:scatterplot1}{{11.8}{262}{Scatterplot via the `Correlation Matrix' method in JASP}{figure.11.8}{}}
\newlabel{sec:introregression}{{11.3}{262}{What is a linear regression model?~\label {sec:introregression}}{section.11.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.3}What is a linear regression model?~}{262}{section.11.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.9}{\ignorespaces Panel a shows the sleep-grumpiness scatterplot from Figure~\ref  {fig:scatterplot1} with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, panel b shows the same data, but with a very poor choice of regression line drawn over the top.}}{263}{figure.11.9}}
\newlabel{fig:regression1}{{11.9}{263}{Panel a shows the sleep-grumpiness scatterplot from Figure~\ref {fig:scatterplot1} with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, panel b shows the same data, but with a very poor choice of regression line drawn over the top}{figure.11.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.10}{\ignorespaces A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data.}}{265}{figure.11.10}}
\newlabel{fig:regression3}{{11.10}{265}{A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data}{figure.11.10}{}}
\newlabel{sec:regressionestimation}{{11.4}{265}{Estimating a linear regression model~\label {sec:regressionestimation}}{section.11.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.4}Estimating a linear regression model~}{265}{section.11.4}}
\newlabel{sec:lm}{{11.4.1}{266}{Linear regression in JASP~\label {sec:lm}}{subsection.11.4.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Linear regression in JASP~}{266}{subsection.11.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}Interpreting the estimated model}{266}{subsection.11.4.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.11}{\ignorespaces A JASP screenshot showing a simple linear regression analysis.}}{267}{figure.11.11}}
\newlabel{fig:reg1}{{11.11}{267}{A JASP screenshot showing a simple linear regression analysis}{figure.11.11}{}}
\newlabel{sec:multipleregression}{{11.5}{267}{Multiple linear regression~\label {sec:multipleregression}}{section.11.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.5}Multiple linear regression~}{267}{section.11.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.1}Doing it in JASP}{268}{subsection.11.5.1}}
\zref@newlabel{mdf@pagelabel-25}{\default{11.5}\page{268}\abspage{283}\mdf@pagevalue{268}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.2}Formula for the general case}{268}{subsection.11.5.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.12}{\ignorespaces A 3D visualisation of a multiple regression model. There are two predictors in the model, \texttt  {dan.sleep} and \texttt  {baby.sleep} and the outcome variable is \texttt  {dan.grump}. Together, these three variables form a 3D space. Each observation (dot) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients what we're trying to do is find a plane that is as close to all the blue dots as possible.}}{269}{figure.11.12}}
\newlabel{fig:multipleregression}{{11.12}{269}{A 3D visualisation of a multiple regression model. There are two predictors in the model, \texttt {dan.sleep} and \texttt {baby.sleep} and the outcome variable is \texttt {dan.grump}. Together, these three variables form a 3D space. Each observation (dot) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients what we're trying to do is find a plane that is as close to all the blue dots as possible}{figure.11.12}{}}
\newlabel{sec:r2}{{11.6}{270}{Quantifying the fit of the regression model~\label {sec:r2}}{section.11.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.6}Quantifying the fit of the regression model~}{270}{section.11.6}}
\newlabel{sec:rsquared}{{11.6.1}{270}{The \texorpdfstring {$R^2$}{R-squared} value~\label {sec:rsquared}}{subsection.11.6.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.1}The $R^2$ value~}{270}{subsection.11.6.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.13}{\ignorespaces A JASP screenshot showing a residual computation in R.}}{271}{figure.11.13}}
\newlabel{fig:resid}{{11.13}{271}{A JASP screenshot showing a residual computation in R}{figure.11.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.2}The relationship between regression and correlation}{272}{subsection.11.6.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.3}The adjusted \mathversion  {bold}$R^2$\mathversion  {normal} value}{273}{subsection.11.6.3}}
\zref@newlabel{mdf@pagelabel-26}{\default{11.6}\page{273}\abspage{288}\mdf@pagevalue{273}}
\newlabel{sec:regressiontests}{{11.7}{273}{Hypothesis tests for regression models~\label {sec:regressiontests}}{section.11.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.7}Hypothesis tests for regression models~}{273}{section.11.7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.1}Testing the model as a whole}{274}{subsection.11.7.1}}
\zref@newlabel{mdf@pagelabel-27}{\default{11.7}\page{274}\abspage{289}\mdf@pagevalue{274}}
\zref@newlabel{mdf@pagelabel-28}{\default{11.7}\page{275}\abspage{290}\mdf@pagevalue{275}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.2}Tests for individual coefficients}{275}{subsection.11.7.2}}
\newlabel{sec:regressionsummary}{{11.7.3}{276}{Running the hypothesis tests in JASP~\label {sec:regressionsummary}}{subsection.11.7.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.3}Running the hypothesis tests in JASP~}{276}{subsection.11.7.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.14}{\ignorespaces A JASP screenshot showing a multiple linear regression analysis, including relevant hypothesis tests.}}{277}{figure.11.14}}
\newlabel{fig:reg2}{{11.14}{277}{A JASP screenshot showing a multiple linear regression analysis, including relevant hypothesis tests}{figure.11.14}{}}
\newlabel{sec:regressioncoefs}{{11.8}{277}{Regarding regression coefficients~\label {sec:regressioncoefs}}{section.11.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.8}Regarding regression coefficients~}{277}{section.11.8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.1}Confidence intervals for the coefficients}{278}{subsection.11.8.1}}
\zref@newlabel{mdf@pagelabel-29}{\default{11.8}\page{278}\abspage{293}\mdf@pagevalue{278}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.2}Calculating standardised regression coefficients}{278}{subsection.11.8.2}}
\zref@newlabel{mdf@pagelabel-30}{\default{11.8}\page{279}\abspage{294}\mdf@pagevalue{279}}
\newlabel{sec:regressionassumptions}{{11.9}{279}{Assumptions of regression~\label {sec:regressionassumptions}}{section.11.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.9}Assumptions of regression~}{279}{section.11.9}}
\abx@aux@cite{Fox2011}
\abx@aux@segm{0}{0}{Fox2011}
\abx@aux@segm{0}{0}{Fox2011}
\newlabel{sec:regressiondiagnostics}{{11.10}{280}{Model checking~\label {sec:regressiondiagnostics}}{section.11.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.10}Model checking~}{280}{section.11.10}}
\abx@aux@backref{48}{Fox2011}{0}{281}{281}
\abx@aux@page{48}{281}
\abx@aux@backref{49}{Fox2011}{0}{281}{281}
\abx@aux@page{49}{281}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.1}Three kinds of residuals}{281}{subsection.11.10.1}}
\zref@newlabel{mdf@pagelabel-31}{\default{11.10}\page{281}\abspage{296}\mdf@pagevalue{281}}
\zref@newlabel{mdf@pagelabel-32}{\default{11.10}\page{282}\abspage{297}\mdf@pagevalue{282}}
\zref@newlabel{mdf@pagelabel-33}{\default{11.10}\page{282}\abspage{297}\mdf@pagevalue{282}}
\newlabel{sec:regressionoutliers}{{11.10.2}{282}{Three kinds of anomalous data~\label {sec:regressionoutliers}}{subsection.11.10.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.2}Three kinds of anomalous data~}{282}{subsection.11.10.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.15}{\ignorespaces An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line.}}{283}{figure.11.15}}
\newlabel{fig:outlier}{{11.15}{283}{An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line}{figure.11.15}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.16}{\ignorespaces An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations. The observation falls very close to the regression line and does not distort it.}}{284}{figure.11.16}}
\newlabel{fig:leverage}{{11.16}{284}{An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations. The observation falls very close to the regression line and does not distort it}{figure.11.16}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.17}{\ignorespaces An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis).}}{285}{figure.11.17}}
\newlabel{fig:influence}{{11.17}{285}{An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis)}{figure.11.17}{}}
\zref@newlabel{mdf@pagelabel-34}{\default{11.10}\page{286}\abspage{301}\mdf@pagevalue{286}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.18}{\ignorespaces JASP output showing Cook's distance for each case/row of data}}{286}{figure.11.18}}
\newlabel{fig:reg4}{{11.18}{286}{JASP output showing Cook's distance for each case/row of data}{figure.11.18}{}}
\newlabel{sec:regressionnormality}{{11.10.3}{287}{Checking the normality of the residuals~\label {sec:regressionnormality}}{subsection.11.10.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.3}Checking the normality of the residuals~}{287}{subsection.11.10.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.19}{\ignorespaces Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals, produced in JASP. }}{287}{figure.11.19}}
\newlabel{fig:reg5}{{11.19}{287}{Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals, produced in JASP}{figure.11.19}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11.20}{\ignorespaces Residuals plots produced in JASP}}{288}{figure.11.20}}
\newlabel{fig:reg6}{{11.20}{288}{Residuals plots produced in JASP}{figure.11.20}{}}
\newlabel{sec:modelselreg}{{11.11}{288}{Model selection\label {sec:modelselreg}}{section.11.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.11}Model selection}{288}{section.11.11}}
\abx@aux@cite{Akaike1974}
\abx@aux@segm{0}{0}{Akaike1974}
\abx@aux@backref{50}{Akaike1974}{0}{289}{289}
\abx@aux@page{50}{289}
\zref@newlabel{mdf@pagelabel-35}{\default{11.11}\page{289}\abspage{304}\mdf@pagevalue{289}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {11.12}Summary}{290}{section.11.12}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {12}Comparing several means (one-way ANOVA)}{293}{chapter.12}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:anova}{{12}{293}{Comparing several means (one-way ANOVA)}{chapter.12}{}}
\newlabel{sec:anxifree}{{12.1}{293}{An illustrative data set~\label {sec:anxifree}}{section.12.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.1}An illustrative data set~}{293}{section.12.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces JASP screenshot showing descriptives and box plots for mood gain, split by drug administered}}{294}{figure.12.1}}
\newlabel{fig:anova1}{{12.1}{294}{JASP screenshot showing descriptives and box plots for mood gain, split by drug administered}{figure.12.1}{}}
\newlabel{sec:anovaintro}{{12.2}{295}{How ANOVA works \label {sec:anovaintro}}{section.12.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.2}How ANOVA works }{295}{section.12.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Two formulas for the variance of $Y$}{296}{subsection.12.2.1}}
\zref@newlabel{mdf@pagelabel-36}{\default{12.2}\page{296}\abspage{311}\mdf@pagevalue{296}}
\zref@newlabel{mdf@pagelabel-37}{\default{12.2}\page{297}\abspage{312}\mdf@pagevalue{297}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}From variances to sums of squares}{297}{subsection.12.2.2}}
\zref@newlabel{mdf@pagelabel-38}{\default{12.2}\page{297}\abspage{312}\mdf@pagevalue{297}}
\zref@newlabel{mdf@pagelabel-39}{\default{12.2}\page{297}\abspage{312}\mdf@pagevalue{297}}
\zref@newlabel{mdf@pagelabel-40}{\default{12.2}\page{298}\abspage{313}\mdf@pagevalue{298}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces Graphical illustration of ``between groups'' variation (panel a) and ``within groups'' variation (panel b). On the left the arrows show the differences in the group means. On the right the arrows highlight the variability within each group.}}{299}{figure.12.2}}
\newlabel{fig:anovavar}{{12.2}{299}{Graphical illustration of ``between groups'' variation (panel a) and ``within groups'' variation (panel b). On the left the arrows show the differences in the group means. On the right the arrows highlight the variability within each group}{figure.12.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.3}From sums of squares to the $F$-test}{299}{subsection.12.2.3}}
\abx@aux@cite{Hays1994}
\abx@aux@segm{0}{0}{Hays1994}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {12.1}{\ignorespaces All of the key quantities involved in an ANOVA organised into a ``standard'' ANOVA table. The formulas for all quantities (except the $p$-value which has a very ugly formula and would be nightmarishly hard to calculate without a computer) are shown.}}{300}{table.12.1}}
\newlabel{tab:anovatable}{{12.1}{300}{All of the key quantities involved in an ANOVA organised into a ``standard'' ANOVA table. The formulas for all quantities (except the $p$-value which has a very ugly formula and would be nightmarishly hard to calculate without a computer) are shown}{table.12.1}{}}
\zref@newlabel{mdf@pagelabel-41}{\default{12.2}\page{300}\abspage{315}\mdf@pagevalue{300}}
\newlabel{sec:anovamodel}{{12.2.4}{300}{The model for the data and the meaning of \texorpdfstring {$F$}{F} \advanced \label {sec:anovamodel}}{subsection.12.2.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.4}The model for the data and the meaning of $F$ }{300}{subsection.12.2.4}}
\zref@newlabel{mdf@pagelabel-42}{\default{12.2}\page{301}\abspage{316}\mdf@pagevalue{301}}
\abx@aux@backref{51}{Hays1994}{0}{301}{301}
\abx@aux@page{51}{301}
\zref@newlabel{mdf@pagelabel-43}{\default{12.2}\page{302}\abspage{317}\mdf@pagevalue{302}}
\newlabel{sec:anovacalc}{{12.2.5}{302}{A worked example \label {sec:anovacalc}}{subsection.12.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.5}A worked example }{302}{subsection.12.2.5}}
\newlabel{sec:introduceaov}{{12.3}{305}{Running an ANOVA in JASP \label {sec:introduceaov}}{section.12.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.3}Running an ANOVA in JASP }{305}{section.12.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.1}Using JASP to specify your ANOVA}{305}{subsection.12.3.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces JASP results table for ANOVA of mood gain by drug administered}}{306}{figure.12.3}}
\newlabel{fig:anova2}{{12.3}{306}{JASP results table for ANOVA of mood gain by drug administered}{figure.12.3}{}}
\newlabel{sec:etasquared}{{12.4}{306}{Effect size\label {sec:etasquared}}{section.12.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.4}Effect size}{306}{section.12.4}}
\newlabel{sec:posthoc}{{12.5}{307}{Multiple comparisons and post hoc tests~\label {sec:posthoc}}{section.12.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.5}Multiple comparisons and post hoc tests~}{307}{section.12.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.1}Running ``pairwise'' $t$-tests}{308}{subsection.12.5.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces Pairwise $t$-tests as post hoc comparisons in JASP}}{308}{figure.12.4}}
\newlabel{fig:anova3}{{12.4}{308}{Pairwise $t$-tests as post hoc comparisons in JASP}{figure.12.4}{}}
\abx@aux@cite{Shaffer1995}
\abx@aux@segm{0}{0}{Shaffer1995}
\abx@aux@cite{Hsu1996}
\abx@aux@segm{0}{0}{Hsu1996}
\abx@aux@cite{Dunn1961}
\abx@aux@segm{0}{0}{Dunn1961}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.2}Corrections for multiple testing}{309}{subsection.12.5.2}}
\abx@aux@backref{52}{Shaffer1995}{0}{309}{309}
\abx@aux@page{52}{309}
\abx@aux@backref{53}{Hsu1996}{0}{309}{309}
\abx@aux@page{53}{309}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.3}Bonferroni corrections}{309}{subsection.12.5.3}}
\abx@aux@backref{54}{Dunn1961}{0}{309}{309}
\abx@aux@page{54}{309}
\abx@aux@segm{0}{0}{Dunn1961}
\abx@aux@cite{Holm1979}
\abx@aux@segm{0}{0}{Holm1979}
\abx@aux@backref{55}{Dunn1961}{0}{310}{310}
\abx@aux@page{55}{310}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.4}Holm corrections}{310}{subsection.12.5.4}}
\abx@aux@backref{56}{Holm1979}{0}{310}{310}
\abx@aux@page{56}{310}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.5}Writing up the post hoc test}{311}{subsection.12.5.5}}
\newlabel{sec:anovaassumptions}{{12.6}{311}{Assumptions of one-way ANOVA \label {sec:anovaassumptions}}{section.12.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.6}Assumptions of one-way ANOVA }{311}{section.12.6}}
\zref@newlabel{mdf@pagelabel-44}{\default{12.6}\page{311}\abspage{326}\mdf@pagevalue{311}}
\abx@aux@cite{Box1953}
\abx@aux@segm{0}{0}{Box1953}
\abx@aux@cite{Levene1960}
\abx@aux@segm{0}{0}{Levene1960}
\abx@aux@cite{BrownForsythe1974}
\abx@aux@segm{0}{0}{BrownForsythe1974}
\zref@newlabel{mdf@pagelabel-45}{\default{12.6}\page{312}\abspage{327}\mdf@pagevalue{312}}
\newlabel{sec:levene}{{12.6.1}{312}{Checking the homogeneity of variance assumption~\label {sec:levene}}{subsection.12.6.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.1}Checking the homogeneity of variance assumption~}{312}{subsection.12.6.1}}
\abx@aux@backref{57}{Box1953}{0}{312}{312}
\abx@aux@page{57}{312}
\abx@aux@backref{58}{Levene1960}{0}{312}{312}
\abx@aux@page{58}{312}
\abx@aux@backref{59}{BrownForsythe1974}{0}{312}{312}
\abx@aux@page{59}{312}
\abx@aux@cite{Welch1951}
\abx@aux@segm{0}{0}{Welch1951}
\zref@newlabel{mdf@pagelabel-46}{\default{12.6}\page{313}\abspage{328}\mdf@pagevalue{313}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.2}Running the Levene test in JASP}{313}{subsection.12.6.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.5}{\ignorespaces Levene test output for one-way ANOVA in JASP}}{314}{figure.12.5}}
\newlabel{fig:anova4}{{12.5}{314}{Levene test output for one-way ANOVA in JASP}{figure.12.5}{}}
\newlabel{sec:welchoneway}{{12.6.3}{314}{Removing the homogeneity of variance assumption~\label {sec:welchoneway}}{subsection.12.6.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.3}Removing the homogeneity of variance assumption~}{314}{subsection.12.6.3}}
\abx@aux@backref{60}{Welch1951}{0}{314}{314}
\abx@aux@page{60}{314}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.6}{\ignorespaces Welch's homogeneity correction as part of the One Way ANOVA analysis in JASP }}{314}{figure.12.6}}
\newlabel{fig:anova4a}{{12.6}{314}{Welch's homogeneity correction as part of the One Way ANOVA analysis in JASP}{figure.12.6}{}}
\abx@aux@cite{KruskalWallis1952}
\abx@aux@segm{0}{0}{KruskalWallis1952}
\newlabel{sec:anovanormality}{{12.6.4}{315}{Checking the normality assumption~\label {sec:anovanormality}}{subsection.12.6.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.6.4}Checking the normality assumption~}{315}{subsection.12.6.4}}
\newlabel{sec:kruskalwallis}{{12.7}{315}{Removing the normality assumption~\label {sec:kruskalwallis}}{section.12.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.7}Removing the normality assumption~}{315}{section.12.7}}
\abx@aux@backref{61}{KruskalWallis1952}{0}{315}{315}
\abx@aux@page{61}{315}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.7.1}The logic behind the Kruskal-Wallis test}{315}{subsection.12.7.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.7}{\ignorespaces QQ plot produced from JASP}}{316}{figure.12.7}}
\newlabel{fig:anova5}{{12.7}{316}{QQ plot produced from JASP}{figure.12.7}{}}
\zref@newlabel{mdf@pagelabel-47}{\default{12.7}\page{317}\abspage{332}\mdf@pagevalue{317}}
\zref@newlabel{mdf@pagelabel-48}{\default{12.7}\page{318}\abspage{333}\mdf@pagevalue{318}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.7.2}Additional details}{318}{subsection.12.7.2}}
\zref@newlabel{mdf@pagelabel-49}{\default{12.7}\page{319}\abspage{334}\mdf@pagevalue{319}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.7.3}How to run the Kruskal-Wallis test in JASP}{319}{subsection.12.7.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.8}{\ignorespaces Kruskall-Wallis one-way non-parametric ANOVA in JASP}}{319}{figure.12.8}}
\newlabel{fig:anova6}{{12.8}{319}{Kruskall-Wallis one-way non-parametric ANOVA in JASP}{figure.12.8}{}}
\newlabel{sec:RManova}{{12.8}{319}{Repeated measures one-way ANOVA~\label {sec:RManova}}{section.12.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.8}Repeated measures one-way ANOVA~}{319}{section.12.8}}
\abx@aux@cite{Geschwind1972}
\abx@aux@segm{0}{0}{Geschwind1972}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {12.8.1}Repeated measures ANOVA in JASP}{320}{subsection.12.8.1}}
\abx@aux@backref{62}{Geschwind1972}{0}{320}{320}
\abx@aux@page{62}{320}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {12.2}{\ignorespaces Number of attempts successfully completed on three experimental tasks.}}{321}{table.12.2}}
\newlabel{tab:RManova}{{12.2}{321}{Number of attempts successfully completed on three experimental tasks}{table.12.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.9}{\ignorespaces Repeated measures ANOVA in JASP}}{322}{figure.12.9}}
\newlabel{fig:RManova1}{{12.9}{322}{Repeated measures ANOVA in JASP}{figure.12.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.10}{\ignorespaces One-way repeated measures ANOVA output: Mauchly\IeC {\textquoteright }s Test of Sphericity}}{322}{figure.12.10}}
\newlabel{fig:RManova2}{{12.10}{322}{One-way repeated measures ANOVA output: Mauchlys Test of Sphericity}{figure.12.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.11}{\ignorespaces One-way repeated measures ANOVA output: Tests of Within-Subjects Effects}}{323}{figure.12.11}}
\newlabel{fig:RManova3}{{12.11}{323}{One-way repeated measures ANOVA output: Tests of Within-Subjects Effects}{figure.12.11}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.12}{\ignorespaces Repeated measures ANOVA output: Descriptive statistics via the `Marginal Means' dialogue.}}{323}{figure.12.12}}
\newlabel{fig:RManova5}{{12.12}{323}{Repeated measures ANOVA output: Descriptive statistics via the `Marginal Means' dialogue}{figure.12.12}{}}
\newlabel{sec:Friedman}{{12.9}{324}{The Friedman non-parametric repeated measures ANOVA test~\label {sec:Friedman}}{section.12.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.9}The Friedman non-parametric repeated measures ANOVA test~}{324}{section.12.9}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12.13}{\ignorespaces The Friedman nonparametric repeated measures ANOVA in JASP}}{324}{figure.12.13}}
\newlabel{fig:RManova6}{{12.13}{324}{The Friedman nonparametric repeated measures ANOVA in JASP}{figure.12.13}{}}
\abx@aux@cite{Sahai2000}
\abx@aux@segm{0}{0}{Sahai2000}
\newlabel{sec:anovaandt}{{12.10}{325}{On the relationship between ANOVA and the Student \texorpdfstring {\boldm {$t$}}{}-test~\label {sec:anovaandt}}{section.12.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.10}On the relationship between ANOVA and the Student \mathversion  {bold}$t$\mathversion  {normal}-test~}{325}{section.12.10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {12.11}Summary}{325}{section.12.11}}
\abx@aux@backref{63}{Sahai2000}{0}{326}{326}
\abx@aux@page{63}{326}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {13}Factorial ANOVA }{327}{chapter.13}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:anova2}{{13}{327}{Factorial ANOVA}{chapter.13}{}}
\newlabel{sec:factorialanovasimple}{{13.1}{327}{Factorial ANOVA 1: balanced designs, no interactions\label {sec:factorialanovasimple}}{section.13.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.1}Factorial ANOVA 1: balanced designs, no interactions}{327}{section.13.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces JASP contingency table of \texttt  {drug} by \texttt  {therapy}}}{328}{figure.13.1}}
\newlabel{fig:factorialanova1}{{13.1}{328}{JASP contingency table of \texttt {drug} by \texttt {therapy}}{figure.13.1}{}}
\newlabel{sec:factanovahyp}{{13.1.1}{328}{What hypotheses are we testing?\label {sec:factanovahyp}}{subsection.13.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.1}What hypotheses are we testing?}{328}{subsection.13.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.2}Running the analysis in JASP}{331}{subsection.13.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.2}{\ignorespaces JASP one way ANOVA of \texttt  {mood.gain} by \texttt  {drug}}}{331}{figure.13.2}}
\newlabel{fig:factorialanova2}{{13.2}{331}{JASP one way ANOVA of \texttt {mood.gain} by \texttt {drug}}{figure.13.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.3}{\ignorespaces JASP two way ANOVA of \texttt  {mood.gain} by \texttt  {drug} and \texttt  {therapy}}}{332}{figure.13.3}}
\newlabel{fig:factorialanova3}{{13.3}{332}{JASP two way ANOVA of \texttt {mood.gain} by \texttt {drug} and \texttt {therapy}}{figure.13.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.3}How are the sum of squares calculated?}{333}{subsection.13.1.3}}
\zref@newlabel{mdf@pagelabel-50}{\default{13.1}\page{334}\abspage{349}\mdf@pagevalue{334}}
\zref@newlabel{mdf@pagelabel-51}{\default{13.1}\page{335}\abspage{350}\mdf@pagevalue{335}}
\zref@newlabel{mdf@pagelabel-52}{\default{13.1}\page{336}\abspage{351}\mdf@pagevalue{336}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.4}What are our degrees of freedom?}{336}{subsection.13.1.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.5}Factorial ANOVA versus one-way ANOVAs}{337}{subsection.13.1.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.6}What kinds of outcomes does this analysis capture?}{337}{subsection.13.1.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.4}{\ignorespaces The four different outcomes for a $2 \times 2$ ANOVA when no interactions are present. In panel (a) we see a main effect of Factor A and no effect of Factor B. Panel (b) shows a main effect of Factor B but no effect of Factor A. Panel (c) shows main effects of both Factor A and Factor B. Finally, panel (d) shows no effect of either factor.}}{338}{figure.13.4}}
\newlabel{fig:maineffects}{{13.4}{338}{The four different outcomes for a $2 \times 2$ ANOVA when no interactions are present. In panel (a) we see a main effect of Factor A and no effect of Factor B. Panel (b) shows a main effect of Factor B but no effect of Factor A. Panel (c) shows main effects of both Factor A and Factor B. Finally, panel (d) shows no effect of either factor}{figure.13.4}{}}
\newlabel{sec:interactions}{{13.2}{339}{Factorial ANOVA 2: balanced designs, interactions allowed\label {sec:interactions}}{section.13.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.2}Factorial ANOVA 2: balanced designs, interactions allowed}{339}{section.13.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}What exactly \relax $\@@underline {\hbox {is}}\mathsurround \z@ $\relax  an interaction effect?}{339}{subsection.13.2.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.5}{\ignorespaces Qualitatively different interactions for a $2 \times 2$ ANOVA}}{340}{figure.13.5}}
\newlabel{fig:interaction}{{13.5}{340}{Qualitatively different interactions for a $2 \times 2$ ANOVA}{figure.13.5}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.6}{\ignorespaces JASP screen showing a `Descriptives Plot' in ANOVA using the clinical trial data}}{341}{figure.13.6}}
\newlabel{fig:interactionplot}{{13.6}{341}{JASP screen showing a `Descriptives Plot' in ANOVA using the clinical trial data}{figure.13.6}{}}
\zref@newlabel{mdf@pagelabel-53}{\default{13.2}\page{341}\abspage{356}\mdf@pagevalue{341}}
\zref@newlabel{mdf@pagelabel-54}{\default{13.2}\page{342}\abspage{357}\mdf@pagevalue{342}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}Calculating sums of squares for the interaction}{342}{subsection.13.2.2}}
\zref@newlabel{mdf@pagelabel-55}{\default{13.2}\page{343}\abspage{358}\mdf@pagevalue{343}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.3}Degrees of freedom for the interaction}{343}{subsection.13.2.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.4}Running the ANOVA in JASP}{344}{subsection.13.2.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.7}{\ignorespaces Results for the full factorial model, including the interaction component \texttt  {drug*therapy}}}{344}{figure.13.7}}
\newlabel{fig:factorialanova4}{{13.7}{344}{Results for the full factorial model, including the interaction component \texttt {drug*therapy}}{figure.13.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.5}Interpreting the results}{345}{subsection.13.2.5}}
\newlabel{sec:effectsizefactorialanova}{{13.3}{346}{Effect size~\label {sec:effectsizefactorialanova}}{section.13.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.3}Effect size~}{346}{section.13.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.1}Estimated group means}{348}{subsection.13.3.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.8}{\ignorespaces JASP screenshot showing the marginal means for the saturated model, i.e. including the interaction component, with the \texttt  {clinicaltrial} data set}}{348}{figure.13.8}}
\newlabel{fig:margmean1}{{13.8}{348}{JASP screenshot showing the marginal means for the saturated model, i.e. including the interaction component, with the \texttt {clinicaltrial} data set}{figure.13.8}{}}
\newlabel{sec:factorialanovaassumptions}{{13.4}{349}{Assumption checking~\label {sec:factorialanovaassumptions}}{section.13.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.4}Assumption checking~}{349}{section.13.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.1}Homogeneity of variance}{349}{subsection.13.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.2}Normality of residuals}{349}{subsection.13.4.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.9}{\ignorespaces Checking assumptions in an ANOVA model}}{350}{figure.13.9}}
\newlabel{fig:factorialanova5}{{13.9}{350}{Checking assumptions in an ANOVA model}{figure.13.9}{}}
\abx@aux@cite{Everitt1996}
\abx@aux@segm{0}{0}{Everitt1996}
\newlabel{sec:ancova}{{13.5}{351}{Analysis of Covariance (ANCOVA)\label {sec:ancova}}{section.13.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.5}Analysis of Covariance (ANCOVA)}{351}{section.13.5}}
\abx@aux@backref{64}{Everitt1996}{0}{351}{351}
\abx@aux@page{64}{351}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.1}Running ANCOVA in JASP}{351}{subsection.13.5.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.10}{\ignorespaces Plot of Statistics anxiety against age for two distinct groups}}{352}{figure.13.10}}
\newlabel{fig:ancova_groups}{{13.10}{352}{Plot of Statistics anxiety against age for two distinct groups}{figure.13.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.11}{\ignorespaces The JASP ANCOVA analysis window}}{353}{figure.13.11}}
\newlabel{fig:ancova1}{{13.11}{353}{The JASP ANCOVA analysis window}{figure.13.11}{}}
\newlabel{sec:anovalm}{{13.6}{353}{ANOVA as a linear model\label {sec:anovalm}}{section.13.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.6}ANOVA as a linear model}{353}{section.13.6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.12}{\ignorespaces Plot of mean happiness level as a function of stress and commuting method}}{354}{figure.13.12}}
\newlabel{fig:ancova4}{{13.12}{354}{Plot of mean happiness level as a function of stress and commuting method}{figure.13.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.6.1}Some data}{354}{subsection.13.6.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.6.2}ANOVA with binary factors as a regression model}{355}{subsection.13.6.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.13}{\ignorespaces ANOVA of the \texttt  {rtfm.csv} data set in JASP, without the interaction term}}{356}{figure.13.13}}
\newlabel{fig:factorialanova6}{{13.13}{356}{ANOVA of the \texttt {rtfm.csv} data set in JASP, without the interaction term}{figure.13.13}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.14}{\ignorespaces Regression analysis of the \texttt  {rtfm.csv} data set in JASP, without the interaction term}}{358}{figure.13.14}}
\newlabel{fig:factorialanova7}{{13.14}{358}{Regression analysis of the \texttt {rtfm.csv} data set in JASP, without the interaction term}{figure.13.14}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.6.3}Using contrasts to encode non binary factors}{358}{subsection.13.6.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.6.4}The equivalence between ANOVA and regression for non-binary factors}{359}{subsection.13.6.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.15}{\ignorespaces JASP ANOVA results, without interaction component}}{360}{figure.13.15}}
\newlabel{fig:factorialanova9}{{13.15}{360}{JASP ANOVA results, without interaction component}{figure.13.15}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.16}{\ignorespaces JASP regression results, with contrast variables \texttt  {druganxifree} and \texttt  {drugjoyzepam}}}{360}{figure.13.16}}
\newlabel{fig:factorialanova10}{{13.16}{360}{JASP regression results, with contrast variables \texttt {druganxifree} and \texttt {drugjoyzepam}}{figure.13.16}{}}
\newlabel{sec:contrasts}{{13.7}{361}{Different ways to specify contrasts\label {sec:contrasts}}{section.13.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.7}Different ways to specify contrasts}{361}{section.13.7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.7.1}Treatment contrasts}{361}{subsection.13.7.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.17}{\ignorespaces Model comparison in JASP regression, null model 0 vs. contrasts model 1}}{362}{figure.13.17}}
\newlabel{fig:factorialanova11}{{13.17}{362}{Model comparison in JASP regression, null model 0 vs. contrasts model 1}{figure.13.17}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.7.2}Helmert contrasts}{363}{subsection.13.7.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.7.3}Sum to zero contrasts}{364}{subsection.13.7.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.7.4}Optional contrasts in JASP}{364}{subsection.13.7.4}}
\abx@aux@segm{0}{0}{Hsu1996}
\newlabel{sec:posthoc2}{{13.8}{365}{Post hoc tests\label {sec:posthoc2}}{section.13.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.8}Post hoc tests}{365}{section.13.8}}
\abx@aux@backref{65}{Hsu1996}{0}{366}{366}
\abx@aux@page{65}{366}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.18}{\ignorespaces Tukey HSD post hoc test in JASP}}{367}{figure.13.18}}
\newlabel{fig:factorialanova13}{{13.18}{367}{Tukey HSD post hoc test in JASP}{figure.13.18}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.19}{\ignorespaces Tukey HSD post hoc test in JASP factorial ANOVA with an interaction term}}{368}{figure.13.19}}
\newlabel{fig:factorialanova14}{{13.19}{368}{Tukey HSD post hoc test in JASP factorial ANOVA with an interaction term}{figure.13.19}{}}
\newlabel{sec:plannedcomparisons}{{13.9}{368}{The method of planned comparisons\label {sec:plannedcomparisons}}{section.13.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.9}The method of planned comparisons}{368}{section.13.9}}
\newlabel{sec:unbalancedanova}{{13.10}{369}{Factorial ANOVA 3: unbalanced designs\label {sec:unbalancedanova}}{section.13.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.10}Factorial ANOVA 3: unbalanced designs}{369}{section.13.10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.10.1}The coffee data}{369}{subsection.13.10.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.10.2}``Standard ANOVA'' does not exist for unbalanced designs}{370}{subsection.13.10.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.20}{\ignorespaces Descriptives for the \texttt  {coffee.csv} data set, separately split by \texttt  {milk} and \texttt  {sugar}, respectively.}}{371}{figure.13.20}}
\newlabel{fig:factorialanova15}{{13.20}{371}{Descriptives for the \texttt {coffee.csv} data set, separately split by \texttt {milk} and \texttt {sugar}, respectively}{figure.13.20}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.10.3}Type I sum of squares}{372}{subsection.13.10.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.21}{\ignorespaces ANOVA results table using Type I sum of squares in JASP}}{373}{figure.13.21}}
\newlabel{fig:factorialanova16}{{13.21}{373}{ANOVA results table using Type I sum of squares in JASP}{figure.13.21}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.22}{\ignorespaces ANOVA results table using Type I sum of squares in JASP, but with factors entered in a different order (\texttt  {milk} first)}}{374}{figure.13.22}}
\newlabel{fig:factorialanova17}{{13.22}{374}{ANOVA results table using Type I sum of squares in JASP, but with factors entered in a different order (\texttt {milk} first)}{figure.13.22}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.10.4}Type III sum of squares}{374}{subsection.13.10.4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.23}{\ignorespaces ANOVA results table using Type III sum of squares in JASP}}{376}{figure.13.23}}
\newlabel{fig:factorialanova18}{{13.23}{376}{ANOVA results table using Type III sum of squares in JASP}{figure.13.23}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.10.5}Type II sum of squares}{376}{subsection.13.10.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13.24}{\ignorespaces ANOVA results table using Type II sum of squares in JASP}}{378}{figure.13.24}}
\newlabel{fig:factorialanova19}{{13.24}{378}{ANOVA results table using Type II sum of squares in JASP}{figure.13.24}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {13.10.6}Effect sizes (and non-additive sums of squares)}{379}{subsection.13.10.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {13.11}Summary}{380}{section.13.11}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {part}{V\hspace  {1em}Endings, alternatives and prospects}{381}{part.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {14}Bayesian statistics }{383}{chapter.14}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:bayes}{{14}{383}{Bayesian statistics}{chapter.14}{}}
\newlabel{sec:basicbayes}{{14.1}{384}{Probabilistic reasoning by rational agents\label {sec:basicbayes}}{section.14.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {14.1}Probabilistic reasoning by rational agents}{384}{section.14.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.1}Priors: what you believed before}{384}{subsection.14.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.2}Likelihoods: theories about the data}{385}{subsection.14.1.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.3}The joint probability of data and hypothesis}{385}{subsection.14.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.4}Updating beliefs using Bayes' rule}{387}{subsection.14.1.4}}
\newlabel{sec:bayesianhypothesistests}{{14.2}{389}{Bayesian hypothesis tests~\label {sec:bayesianhypothesistests}}{section.14.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {14.2}Bayesian hypothesis tests~}{389}{section.14.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.1}The Bayes factor}{389}{subsection.14.2.1}}
\abx@aux@cite{Jeffreys1961}
\abx@aux@segm{0}{0}{Jeffreys1961}
\abx@aux@cite{Kass1995}
\abx@aux@segm{0}{0}{Kass1995}
\abx@aux@segm{0}{0}{Kass1995}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.2}Interpreting Bayes factors}{390}{subsection.14.2.2}}
\abx@aux@backref{66}{Jeffreys1961}{0}{390}{390}
\abx@aux@page{66}{390}
\abx@aux@backref{67}{Kass1995}{0}{390}{390}
\abx@aux@page{67}{390}
\abx@aux@backref{68}{Kass1995}{0}{390}{390}
\abx@aux@page{68}{390}
\abx@aux@segm{0}{0}{Kass1995}
\abx@aux@backref{69}{Kass1995}{0}{391}{391}
\abx@aux@page{69}{391}
\newlabel{sec:whybayes}{{14.3}{391}{Why be a Bayesian?\label {sec:whybayes}}{section.14.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {14.3}Why be a Bayesian?}{391}{section.14.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.1}Statistics that mean what you think they mean}{392}{subsection.14.3.1}}
\abx@aux@cite{Fisher1925}
\abx@aux@segm{0}{0}{Fisher1925}
\abx@aux@cite{Johnson2013}
\abx@aux@segm{0}{0}{Johnson2013}
\abx@aux@segm{0}{0}{Johnson2013}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.2}Evidentiary standards you can believe}{393}{subsection.14.3.2}}
\abx@aux@backref{70}{Fisher1925}{0}{393}{393}
\abx@aux@page{70}{393}
\abx@aux@backref{71}{Johnson2013}{0}{393}{393}
\abx@aux@page{71}{393}
\abx@aux@backref{72}{Johnson2013}{0}{393}{393}
\abx@aux@page{72}{393}
\abx@aux@segm{0}{0}{Johnson2013}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.3}The $p$-value is a lie.}{394}{subsection.14.3.3}}
\abx@aux@backref{73}{Johnson2013}{0}{394}{394}
\abx@aux@page{73}{394}
\abx@aux@segm{0}{0}{Johnson2013}
\abx@aux@segm{0}{0}{Johnson2013}
\abx@aux@backref{74}{Johnson2013}{0}{396}{396}
\abx@aux@page{74}{396}
\abx@aux@backref{75}{Johnson2013}{0}{396}{396}
\abx@aux@fnpage{75}{396}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is ``very wrong''.}}{397}{figure.14.1}}
\newlabel{fig:type1}{{14.1}{397}{How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is ``very wrong''}{figure.14.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.4}Is it really this bad?}{398}{subsection.14.3.4}}
\newlabel{sec:ttestbf}{{14.4}{399}{Bayesian \texorpdfstring {\boldm {$t$}}{}-tests\label {sec:ttestbf}}{section.14.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {14.4}Bayesian \mathversion  {bold}$t$\mathversion  {normal}-tests}{399}{section.14.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.1}Independent samples $t$-test}{399}{subsection.14.4.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces Bayesian independent Samples $t$-test result in JASP}}{400}{figure.14.2}}
\newlabel{fig:bayes1}{{14.2}{400}{Bayesian independent Samples $t$-test result in JASP}{figure.14.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.2}Paired samples $t$-test}{400}{subsection.14.4.2}}
\abx@aux@cite{Kruschke2011}
\abx@aux@segm{0}{0}{Kruschke2011}
\abx@aux@cite{Lee2014}
\abx@aux@segm{0}{0}{Lee2014}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces Paired samples T-Test and Bayes Factor result in JASP}}{401}{figure.14.3}}
\newlabel{fig:bayes3}{{14.3}{401}{Paired samples T-Test and Bayes Factor result in JASP}{figure.14.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {14.5}Summary}{401}{section.14.5}}
\abx@aux@backref{76}{Kruschke2011}{0}{402}{402}
\abx@aux@page{76}{402}
\abx@aux@backref{77}{Lee2014}{0}{402}{402}
\abx@aux@page{77}{402}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {15}Epilogue}{403}{chapter.15}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {15.1}The undiscovered statistics}{403}{section.15.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {15.1.1}Omissions within the topics covered}{404}{subsection.15.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {15.1.2}Statistical models missing from the book}{405}{subsection.15.1.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {15.1.3}Other ways of doing inference}{407}{subsection.15.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {15.1.4}Miscellaneous topics}{410}{subsection.15.1.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {15.2}Learning the basics, and learning them in JASP}{411}{section.15.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {16}References}{415}{chapter.16}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\abx@aux@page{78}{415}
\abx@aux@page{79}{415}
\abx@aux@page{80}{415}
\abx@aux@page{81}{415}
\abx@aux@page{82}{415}
\abx@aux@page{83}{415}
\abx@aux@page{84}{415}
\abx@aux@page{85}{415}
\abx@aux@page{86}{415}
\abx@aux@page{87}{415}
\abx@aux@page{88}{415}
\abx@aux@page{89}{415}
\abx@aux@page{90}{415}
\abx@aux@page{91}{415}
\abx@aux@page{92}{415}
\abx@aux@page{93}{415}
\abx@aux@page{94}{416}
\abx@aux@page{95}{416}
\abx@aux@page{96}{416}
\abx@aux@page{97}{416}
\abx@aux@page{98}{416}
\abx@aux@page{99}{416}
\abx@aux@page{100}{416}
\abx@aux@page{101}{416}
\abx@aux@page{102}{416}
\abx@aux@page{103}{416}
\abx@aux@page{104}{416}
\abx@aux@page{105}{416}
\abx@aux@page{106}{416}
\abx@aux@page{107}{416}
\abx@aux@page{108}{416}
\abx@aux@page{109}{416}
\abx@aux@page{110}{416}
\abx@aux@page{111}{416}
\abx@aux@page{112}{416}
\abx@aux@page{113}{416}
\abx@aux@page{114}{416}
\abx@aux@page{115}{416}
\abx@aux@page{116}{417}
\abx@aux@page{117}{417}
\abx@aux@page{118}{417}
\abx@aux@page{119}{417}
\abx@aux@page{120}{417}
\abx@aux@page{121}{417}
\abx@aux@page{122}{417}
\abx@aux@page{123}{417}
\abx@aux@page{124}{417}
\abx@aux@page{125}{417}
\abx@aux@page{126}{417}
\abx@aux@page{127}{417}
\abx@aux@page{128}{417}
\abx@aux@page{129}{417}
\abx@aux@page{130}{417}
\abx@aux@page{131}{417}
\abx@aux@page{132}{417}
\abx@aux@page{133}{417}
\abx@aux@page{134}{417}
\abx@aux@page{135}{417}
\abx@aux@page{136}{417}
\abx@aux@page{137}{418}
\abx@aux@page{138}{418}
\abx@aux@page{139}{418}
\abx@aux@page{140}{418}
\abx@aux@page{141}{418}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{Adair1984}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Agresti1996}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Agresti2002}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Akaike1974}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Anscombe1973}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Bickel1975}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Box1953}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Box1976}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Box1987}{nyt/global/}
\abx@aux@defaultrefcontext{0}{BrownForsythe1974}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Campbell1963}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Cochran1954}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Cohen1988}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Cramer1946}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Dunn1961}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Ellis2010}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Ellman2002}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Evans1983}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Evans2000}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Everitt1996}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Fisher1922}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Fisher1922b}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Fisher1925}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Fox2011}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Gelman2006}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Gelman2014}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Geschwind1972}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hays1994}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hedges1981}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hedges1985}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hogg2005}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Holm1979}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hothersall2004}{nyt/global/}
\abx@aux@defaultrefcontext{0}{hrobjartsson2010}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Hsu1996}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Ioannidis2005}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Jeffreys1961}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Johnson2013}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Kahneman1973}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Kass1995}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Keynes1923}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Kruschke2011}{nyt/global/}
\abx@aux@defaultrefcontext{0}{KruskalWallis1952}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Kuhberger2014}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Larntz1978}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Lee2014}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Lehmann2011}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Levene1960}{nyt/global/}
\abx@aux@defaultrefcontext{0}{McGrath2006}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Meehl1967}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Pearson1900}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Pfungst1911}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Rosenthal1966}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Sahai2000}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Shaffer1995}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Shapiro1965}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Sokal1994}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Stevens1946}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Stigler1986}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Student1908}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Welch1947}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Welch1951}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Wilkinson2006}{nyt/global/}
\abx@aux@defaultrefcontext{0}{Yates1934}{nyt/global/}
